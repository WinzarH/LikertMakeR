[{"path":"https://winzarh.github.io/LikertMakeR/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to LikertMakeR","title":"Contributing to LikertMakeR","text":"outlines propose change LikertMakeR. detailed discussion contributing tidyverse packages, please see development contributing guide code review principles.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to LikertMakeR","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to LikertMakeR","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See guide create great issue advice.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to LikertMakeR","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"WinzarH/LikertMakeR\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to LikertMakeR","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to LikertMakeR","text":"Please note LikertMakeR project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"purpose","dir":"Articles","previous_headings":"","what":"Purpose","title":"LikertMakeR vignette","text":"package useful teaching Social Sciences, scholars wish “replicate” “reverse engineer” rating-scale data analysis visualisation summary statistics reported.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"LikertMakeR vignette","text":"prompted write core functions LikertMakeR reviewing many journal article submissions authors presented questionnaire results means standard deviations (often means), apparent understanding scale distributions, impact scale properties. Hopefully, tool help researchers, teachers & students, reviewers, better think rating-scale distributions, effects variance, scale boundaries, number items scale. Researchers can also use LikertMakeR create dummy data prepare analyses ahead formal survey.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"rating-scale-properties","dir":"Articles","previous_headings":"","what":"Rating scale properties","title":"LikertMakeR vignette","text":"Likert scale mean, sum, several ordinal rating scales. Typically, bipolar (usually “agree-disagree”) responses propositions determined moderately--highly correlated capture facet theoretical construct.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"rating-scales-have-bounds-and-discrete-measurement-intervals","dir":"Articles","previous_headings":"Rating scale properties","what":"Rating scales have bounds and discrete measurement intervals","title":"LikertMakeR vignette","text":"Rating scales, Likert scales, continuous unbounded. example, 5-point Likert scale constructed , say, five items (questions) summed range 5 (rated ‘1’) 25 (rated ‘5’) integers , mean range ‘1’ ‘5’ intervals 1/5=0.20. 7-point Likert scale constructed eight items summed range 8 (rated ‘1’) 56 (rated ‘7’) integers , mean range ‘1’ ‘7’ intervals 1/8=0.125. Technically, bounded continuous, parametric statistics, mean, standard deviation, correlation, applied summated rating scales. practice, however, parametric statistics commonly used social sciences : common usage easily understood, practice, measures bounded constraints measurement tool, meaning also upper lower boundaries discrete units measurement, means : results conclusions drawn technically-correct non-parametric statistics (almost) always parametric statistics data.  example, D’Alessandro et al. (2020) argue summated scale, made multiple items, “approaches” interval scale measure, implying parametric statistics quite acceptable.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"a-single-1-5-rating-scale-is-not-a-likert-scale---it-may-be-an-likert-scale-item-","dir":"Articles","previous_headings":"Rating scale properties","what":"A single 1-5 rating scale is NOT a Likert scale - it may be an Likert-scale item.","title":"LikertMakeR vignette","text":"Likert-scale items, responses single 1--5 agree-disagree question, analysed professional responsible researchers. much random error single item. Rensis Likert (1932) designed scale logic random overstatement one item likely compensated random understatement another item, , multiple items combined, get reasonably consistent, internally reliable, measure target construct.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"most-rating-scales-are-skewed","dir":"Articles","previous_headings":"Rating scale properties","what":"Most rating scales are skewed","title":"LikertMakeR vignette","text":"Rating-scale boundaries define minima maxima scale values. mean close one boundary data points gather closely boundary.  mean middle scale, data always skewed, shown following plots. -centre means always give skewed distribution bounded rating scales","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"likertmaker-functions","dir":"Articles","previous_headings":"","what":"LikertMakeR functions","title":"LikertMakeR vignette","text":"lfast() generate vector values predefined mean standard deviation. lcor() takes dataframe rating-scale values rearranges values column columns correlated match predefined correlation matrix. makeCorrAlpha constructs random correlation matrix given dimensions predefined Cronbach’s Alpha. makeCorrLoadings constructs random correlation matrix given factor loadings matrix, factor-correlations matrix. makeScales() wrapper function lfast() lcor() generate items summated scales predefined first second moments predefined correlation matrix. function replaces makeItems() now includes multi-item measures. makeItemsScale() generates random dataframe scale items based predefined summated scale desired Cronbach’s Alpha. makePaired() generates dataframe two correlated columns based summary data paired-sample t-test. makeRepeated() generates dataframe k correlated columns based summary data repeated-samples ANOVA. makeScalesRegression() generates dataframe based results output multiple-regression - R2, standardised betas, IV correlations (available). correlateScales() creates dataframe correlated summated scales one might find completed survey questionnaire possibly used Structural Equation model. Helper Functions alpha() calculates Cronbach’s Alpha given correlation matrix given dataframe. eigenvalues() calculates eigenvalues correlation matrix, reports positive-definite status matrix , optionally, displays scree plot visualise eigenvalues. reliability() Computes internal consistency reliability estimates single-factor scale, including Cronbach’s alpha, McDonald’s omega (total), optional ordinal (polychoric-based) variants Confidence intervals","code":""},{"path":[]},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"from-cran","dir":"Articles","previous_headings":"Using LikertMakeR > Download and Install LikertMakeR","what":"from CRAN","title":"LikertMakeR vignette","text":"","code":"> ``` > > install.packages(\"LikertMakeR\") > library(LikertMakeR) > > ```"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"development-version-from-github-","dir":"Articles","previous_headings":"Using LikertMakeR > Download and Install LikertMakeR","what":"development version from GitHub.","title":"LikertMakeR vignette","text":"","code":"> ``` >  > library(devtools) > install_github(\"WinzarH/LikertMakeR\") > library(LikertMakeR) > > ```"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"lfast","dir":"Articles","previous_headings":"Using LikertMakeR > Generate synthetic rating-scale data","what":"lfast()","title":"LikertMakeR vignette","text":"lfast() applies simple evolutionary algorithm draws repeated random samples scaled Beta distribution. produces vector values mean standard deviation typically correct two decimal places. synthesise rating scale lfast(), user must input following parameters: n: sample size mean: desired mean sd: desired standard deviation lowerbound: desired lower bound upperbound: desired upper bound items: number items making scale - default = 1 earlier version LikertMakeR function, lexact(), slow accurate latest version lfast(). , lexact() now deprecated.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"a-four-item-five-point-likert-scale","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"Example: 4-item, 1-5 Likert scale","code":"nItems <- 4 mean <- 2.5 sd <- 0.75  x1 <- lfast(   n = 512,   mean = mean,   sd = sd,   lowerbound = 1,   upperbound = 5,   items = nItems ) #> best solution in 256 iterations"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"lfast-1","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"Example: likelihood--purchase scale","code":"x2 <- lfast(256, 3, 2.5, 0, 10) #> best solution in 7723 iterations"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"correlating-rating-scales","dir":"Articles","previous_headings":"Using LikertMakeR","what":"Correlating rating scales","title":"LikertMakeR vignette","text":"function, lcor(), rearranges values columns data-set correlated specified level. change values - swaps positions within column univariate statistics change, correlations vectors .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"lcor","dir":"Articles","previous_headings":"Using LikertMakeR > Correlating rating scales","what":"lcor()","title":"LikertMakeR vignette","text":"lcor() systematically selects pairs values column swaps places, checks see swap improves correlation matrix. revised dataframe produces correlation matrix closer target correlation matrix, swap retained. Otherwise, values returned original places. process iterated across column. create desired correlated data, user must define following parameters: data: starter data set rating-scales. Number columns must match dimensions target correlation matrix. target: target correlation matrix.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"lcor-example","dir":"Articles","previous_headings":"Using LikertMakeR > Correlating rating scales","what":"lcor() example","title":"LikertMakeR vignette","text":"Let’s generate data: three 5-point Likert scales, five items. first six observations dataframe : first second moments (3 decimal places) : can see data first second moments close expected. expect, randomly-generated synthetic data low correlations: Now, let’s define target correlation matrix: now dataframe desired first second moments, target correlation matrix. Values column new dataframe change original; values rearranged. first ten observations dataframe : new dataframe correlated close desired correlation matrix; presented 3 decimal places:","code":"## generate uncorrelated synthetic data n <- 128 lowerbound <- 1 upperbound <- 5 items <- 5  mydat3 <- data.frame(   x1 = lfast(n, 2.5, 0.75, lowerbound, upperbound, items),   x2 = lfast(n, 3.0, 1.50, lowerbound, upperbound, items),   x3 = lfast(n, 3.5, 1.00, lowerbound, upperbound, items) ) #> best solution in 812 iterations #> best solution in 7553 iterations #> best solution in 385 iterations #>    x1  x2  x3 #> 1 1.4 1.0 5.0 #> 2 2.8 5.0 4.2 #> 3 3.4 1.8 2.0 #> 4 2.0 4.8 4.4 #> 5 3.6 1.0 3.4 #> 6 2.2 2.8 4.0 #>         x1    x2    x3 #> mean 2.500 3.002 3.498 #> sd   0.752 1.501 1.001 #>       x1    x2   x3 #> x1  1.00 -0.02 0.03 #> x2 -0.02  1.00 0.00 #> x3  0.03  0.00 1.00 ## describe a target correlation matrix tgt3 <- matrix(   c(     1.00, 0.85, 0.75,     0.85, 1.00, 0.65,     0.75, 0.65, 1.00   ),   nrow = 3 ) ## apply lcor() function new3 <- lcor(data = mydat3, target = tgt3) #>     X1  X2  X3 #> 1  3.2 4.8 4.8 #> 2  2.4 1.8 4.0 #> 3  1.8 1.8 2.0 #> 4  2.6 4.8 4.4 #> 5  2.4 3.6 3.4 #> 6  2.8 3.2 4.0 #> 7  3.2 2.2 2.8 #> 8  1.8 1.2 2.2 #> 9  3.8 4.8 4.4 #> 10 2.2 4.0 4.2 #>      X1   X2   X3 #> X1 1.00 0.85 0.75 #> X2 0.85 1.00 0.65 #> X3 0.75 0.65 1.00"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makecorralpha","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a correlation matrix from Cronbach’s Alpha","what":"makeCorrAlpha()","title":"LikertMakeR vignette","text":"makeCorrAlpha(), constructs random correlation matrix given dimensions predefined Cronbach’s Alpha. create desired correlation matrix, user must define following parameters: items: “k” - number rows columns desired correlation matrix. alpha: target value Cronbach’s Alpha variance: notional variance coefficient affect spread values correlation matrix. Default = ‘0.5’. value ‘0’ produces matrix -diagonal correlations equal. Setting ‘variance = 1.0’ gives wider range values. Setting ‘variance = 2.0’, , may feasible increases likelihood non-positive-definite matrix. precision: value ‘0’ ‘3’ add random variation around target Cronbach’s Alpha. Default = ‘0’. value ‘0’ produces desired Alpha, generally exact two decimal places. Higher values produce increasingly random values around desired Alpha. sort_cors: Logical. Default = FALSE. TRUE, runs quickly, produces less natural correlation matrix. diagnostics: Logical. TRUE, returns list containing correlation matrix diagnostics list (target/achieved alpha, average inter-item correlation, eigenvalues, PD flag, key arguments). FALSE (default), returns correlation matrix .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makecorralpha-is-volatile","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a correlation matrix from Cronbach’s Alpha","what":"makeCorrAlpha() is volatile","title":"LikertMakeR vignette","text":"Random values generated makeCorrAlpha() highly volatile. makeCorrAlpha() may generate feasible (positive-definite) correlation matrix, especially variance high relative desired Alpha, desired correlation dimensions makeCorrAlpha() inform user resulting correlation matrix positive definite, . returned correlation matrix positive-definite, feasible solution may still possible, often . user encouraged try , possibly several times, find one.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"four-variables-alpha-0-85-variance-default","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"makeCorrAlpha() produced following correlation matrix (three decimal places):","code":"## define parameters items <- 4 alpha <- 0.85 # variance <- 0.5 ## by default  ## apply makeCorrAlpha() function set.seed(42)  cor_matrix_4 <- makeCorrAlpha(items, alpha) #> correlation values consistent with desired alpha in 59 iterations #>        item01 item02 item03 item04 #> item01  1.000  0.766  0.693  0.433 #> item02  0.766  1.000  0.694  0.425 #> item03  0.693  0.694  1.000  0.507 #> item04  0.433  0.425  0.507  1.000"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"test-output-with-helper-functions","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## using helper function alpha()  alpha(cor_matrix_4) #> [1] 0.8500063 ## using helper function eigenvalues()  eigenvalues(cor_matrix_4, 1) #> cor_matrix_4  is positive-definite #> [1] 2.7831667 0.6670820 0.3157114 0.2340400"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"twelve-variables-alpha-0-90-variance-1-0","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a correlation matrix from Cronbach’s Alpha > makeCorrAlpha() is volatile","what":"twelve variables, alpha = 0.90, variance = 1.0","title":"LikertMakeR vignette","text":"High variance produce positive-definite correlation matrix.","code":"## define parameters items <- 12 alpha <- 0.90 variance <- 1.0  ## apply makeCorrAlpha() function set.seed(42)  cor_matrix_12 <- makeCorrAlpha(items = items, alpha = alpha, variance = variance) #> correlation values consistent with desired alpha in 4312 iterations #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 1 (min eigenvalue: -0.953206) #> improved at swap - 4 (min eigenvalue: -0.948513) #> improved at swap - 7 (min eigenvalue: -0.885627) #> improved at swap - 11 (min eigenvalue: -0.823701) #> improved at swap - 13 (min eigenvalue: -0.817121) #> improved at swap - 18 (min eigenvalue: -0.795548) #> improved at swap - 19 (min eigenvalue: -0.786689) #> improved at swap - 24 (min eigenvalue: -0.785326) #> improved at swap - 25 (min eigenvalue: -0.783145) #> improved at swap - 30 (min eigenvalue: -0.780524) #> improved at swap - 33 (min eigenvalue: -0.770617) #> improved at swap - 43 (min eigenvalue: -0.769144) #> improved at swap - 47 (min eigenvalue: -0.76871) #> improved at swap - 49 (min eigenvalue: -0.767956) #> improved at swap - 74 (min eigenvalue: -0.75443) #> improved at swap - 84 (min eigenvalue: -0.709321) #> improved at swap - 89 (min eigenvalue: -0.709319) #> improved at swap - 97 (min eigenvalue: -0.709101) #> improved at swap - 105 (min eigenvalue: -0.702808) #> improved at swap - 109 (min eigenvalue: -0.686501) #> improved at swap - 123 (min eigenvalue: -0.675567) #> improved at swap - 149 (min eigenvalue: -0.656166) #> improved at swap - 154 (min eigenvalue: -0.647445) #> improved at swap - 156 (min eigenvalue: -0.639865) #> improved at swap - 162 (min eigenvalue: -0.636826) #> improved at swap - 207 (min eigenvalue: -0.636473) #> improved at swap - 227 (min eigenvalue: -0.631569) #> improved at swap - 239 (min eigenvalue: -0.631529) #> improved at swap - 260 (min eigenvalue: -0.630315) #> improved at swap - 265 (min eigenvalue: -0.630018) #> improved at swap - 277 (min eigenvalue: -0.628351) #> improved at swap - 305 (min eigenvalue: -0.624799) #> improved at swap - 306 (min eigenvalue: -0.620084) #> improved at swap - 330 (min eigenvalue: -0.61767) #> improved at swap - 370 (min eigenvalue: -0.616984) #> improved at swap - 410 (min eigenvalue: -0.615695) #> improved at swap - 413 (min eigenvalue: -0.610777) #> improved at swap - 432 (min eigenvalue: -0.609952) #> improved at swap - 454 (min eigenvalue: -0.609934) #> improved at swap - 464 (min eigenvalue: -0.608802) #> improved at swap - 475 (min eigenvalue: -0.606048) #> improved at swap - 480 (min eigenvalue: -0.604196) #> improved at swap - 481 (min eigenvalue: -0.603605) #> improved at swap - 489 (min eigenvalue: -0.603575) #> improved at swap - 509 (min eigenvalue: -0.557319) #> improved at swap - 517 (min eigenvalue: -0.556993) #> improved at swap - 525 (min eigenvalue: -0.555728) #> improved at swap - 550 (min eigenvalue: -0.550725) #> improved at swap - 584 (min eigenvalue: -0.546519) #> improved at swap - 588 (min eigenvalue: -0.54343) #> improved at swap - 589 (min eigenvalue: -0.525605) #> improved at swap - 609 (min eigenvalue: -0.520762) #> improved at swap - 677 (min eigenvalue: -0.520203) #> improved at swap - 682 (min eigenvalue: -0.517845) #> improved at swap - 735 (min eigenvalue: -0.517013) #> improved at swap - 772 (min eigenvalue: -0.516949) #> improved at swap - 823 (min eigenvalue: -0.516743) #> improved at swap - 840 (min eigenvalue: -0.516391) #> improved at swap - 848 (min eigenvalue: -0.512467) #> improved at swap - 852 (min eigenvalue: -0.512226) #> improved at swap - 896 (min eigenvalue: -0.512095) #> improved at swap - 911 (min eigenvalue: -0.51205) #> improved at swap - 916 (min eigenvalue: -0.51194) #> improved at swap - 917 (min eigenvalue: -0.510744) #> improved at swap - 969 (min eigenvalue: -0.509389) #> improved at swap - 971 (min eigenvalue: -0.508507) #> improved at swap - 974 (min eigenvalue: -0.508265) #> improved at swap - 1000 (min eigenvalue: -0.501376) #> improved at swap - 1006 (min eigenvalue: -0.500307) #> improved at swap - 1024 (min eigenvalue: -0.49485) #> improved at swap - 1084 (min eigenvalue: -0.494849) #> improved at swap - 1114 (min eigenvalue: -0.49467) #> improved at swap - 1127 (min eigenvalue: -0.492862) #> improved at swap - 1138 (min eigenvalue: -0.491828) #> improved at swap - 1146 (min eigenvalue: -0.488759) #> improved at swap - 1220 (min eigenvalue: -0.487876) #> improved at swap - 1230 (min eigenvalue: -0.487867) #> improved at swap - 1235 (min eigenvalue: -0.48684) #> improved at swap - 1299 (min eigenvalue: -0.486493) #> improved at swap - 1413 (min eigenvalue: -0.482833) #> improved at swap - 1434 (min eigenvalue: -0.48144) #> improved at swap - 1534 (min eigenvalue: -0.481217) #> improved at swap - 1538 (min eigenvalue: -0.481094) #> improved at swap - 1637 (min eigenvalue: -0.479178) #> improved at swap - 1647 (min eigenvalue: -0.478561) #> improved at swap - 1704 (min eigenvalue: -0.476757) #> improved at swap - 1780 (min eigenvalue: -0.474676) #> improved at swap - 1806 (min eigenvalue: -0.474662) #> improved at swap - 1953 (min eigenvalue: -0.474644) #> improved at swap - 1956 (min eigenvalue: -0.474581) #> improved at swap - 1984 (min eigenvalue: -0.47448) #> improved at swap - 2006 (min eigenvalue: -0.474466) #> improved at swap - 2016 (min eigenvalue: -0.470122) #> improved at swap - 2034 (min eigenvalue: -0.468171) #> improved at swap - 2326 (min eigenvalue: -0.468064) #> improved at swap - 2338 (min eigenvalue: -0.467538) #> improved at swap - 2350 (min eigenvalue: -0.467353) #> improved at swap - 2435 (min eigenvalue: -0.466742) #> improved at swap - 2487 (min eigenvalue: -0.466487) #> improved at swap - 2516 (min eigenvalue: -0.465717) #> improved at swap - 2522 (min eigenvalue: -0.465611) #> improved at swap - 2535 (min eigenvalue: -0.460623) #> improved at swap - 2576 (min eigenvalue: -0.460212) #> improved at swap - 2678 (min eigenvalue: -0.460061) #> improved at swap - 2761 (min eigenvalue: -0.458568) #> improved at swap - 2795 (min eigenvalue: -0.458554) #> improved at swap - 2827 (min eigenvalue: -0.458226) #> improved at swap - 2832 (min eigenvalue: -0.458215) #> improved at swap - 2838 (min eigenvalue: -0.456188) #> improved at swap - 2934 (min eigenvalue: -0.455385) #> improved at swap - 2948 (min eigenvalue: -0.455052) #> improved at swap - 3114 (min eigenvalue: -0.454904) #> improved at swap - 3171 (min eigenvalue: -0.454781) #> improved at swap - 3175 (min eigenvalue: -0.454779) #> improved at swap - 3384 (min eigenvalue: -0.454773) #> improved at swap - 3425 (min eigenvalue: -0.454728) #> improved at swap - 3768 (min eigenvalue: -0.454533) #> improved at swap - 3994 (min eigenvalue: -0.454355) #> improved at swap - 4095 (min eigenvalue: -0.454326) #> improved at swap - 4123 (min eigenvalue: -0.45431) #> improved at swap - 4141 (min eigenvalue: -0.454307) #> improved at swap - 4202 (min eigenvalue: -0.453472) #> improved at swap - 4241 (min eigenvalue: -0.453251) #> improved at swap - 4396 (min eigenvalue: -0.452216) #> improved at swap - 4584 (min eigenvalue: -0.452049) #> improved at swap - 4598 (min eigenvalue: -0.451984) #> improved at swap - 4811 (min eigenvalue: -0.451936) #> improved at swap - 4887 (min eigenvalue: -0.451751) #> improved at swap - 4900 (min eigenvalue: -0.451602) #> improved at swap - 5158 (min eigenvalue: -0.451459) #> improved at swap - 5182 (min eigenvalue: -0.451271) #> improved at swap - 5225 (min eigenvalue: -0.45124) #> improved at swap - 5660 (min eigenvalue: -0.451226) #> improved at swap - 5947 (min eigenvalue: -0.450989) #> improved at swap - 5955 (min eigenvalue: -0.450651) #> improved at swap - 5981 (min eigenvalue: -0.450623) #> improved at swap - 6020 (min eigenvalue: -0.450603) #> improved at swap - 6486 (min eigenvalue: -0.450145) #> improved at swap - 7033 (min eigenvalue: -0.44951) #> improved at swap - 7251 (min eigenvalue: -0.44897) #> improved at swap - 7284 (min eigenvalue: -0.448929) #> improved at swap - 7422 (min eigenvalue: -0.448928) #> improved at swap - 7491 (min eigenvalue: -0.448854) #> improved at swap - 7804 (min eigenvalue: -0.448834) #> improved at swap - 7830 (min eigenvalue: -0.448833) #> improved at swap - 7996 (min eigenvalue: -0.448577) #> improved at swap - 8059 (min eigenvalue: -0.448564) #> improved at swap - 8082 (min eigenvalue: -0.448492) #> improved at swap - 8300 (min eigenvalue: -0.448205) #> improved at swap - 8360 (min eigenvalue: -0.448158) #> improved at swap - 8384 (min eigenvalue: -0.447955) #> improved at swap - 8538 (min eigenvalue: -0.447952) #> improved at swap - 8672 (min eigenvalue: -0.447944) #> improved at swap - 8803 (min eigenvalue: -0.447936) #> improved at swap - 9042 (min eigenvalue: -0.447897) #> improved at swap - 9100 (min eigenvalue: -0.447875) #> improved at swap - 9335 (min eigenvalue: -0.447862) #> improved at swap - 9436 (min eigenvalue: -0.447824) #> improved at swap - 9585 (min eigenvalue: -0.447823) #> improved at swap - 10072 (min eigenvalue: -0.447799) #> improved at swap - 10797 (min eigenvalue: -0.447797) #> improved at swap - 11139 (min eigenvalue: -0.447796) #> improved at swap - 11305 (min eigenvalue: -0.447777) #> improved at swap - 11878 (min eigenvalue: -0.447767) #> improved at swap - 12608 (min eigenvalue: -0.447766) #> improved at swap - 14464 (min eigenvalue: -0.447756) #> stopped after 158213 swaps (no improvement for 143748 attempts)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"section","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"makeCorrAlpha() produced following correlation matrix (two decimal places):","code":"#>        item01 item02 item03 item04 item05 item06 item07 item08 item09 item10 #> item01   1.00  -0.07   0.57  -0.51   0.81   0.56   0.61  -0.27   0.43   0.31 #> item02  -0.07   1.00   0.53   0.88   0.56   0.47  -0.04   0.62   0.70   0.34 #> item03   0.57   0.53   1.00   0.33   0.79   0.39   0.62   0.83   0.46   0.62 #> item04  -0.51   0.88   0.33   1.00   0.70   0.62  -0.32   0.70   0.63   0.06 #> item05   0.81   0.56   0.79   0.70   1.00   0.70   0.60   0.26   0.79   0.06 #> item06   0.56   0.47   0.39   0.62   0.70   1.00  -0.67  -0.03   0.57   0.20 #> item07   0.61  -0.04   0.62  -0.32   0.60  -0.67   1.00   0.47   0.00   0.45 #> item08  -0.27   0.62   0.83   0.70   0.26  -0.03   0.47   1.00   0.36   0.89 #> item09   0.43   0.70   0.46   0.63   0.79   0.57   0.00   0.36   1.00  -0.30 #> item10   0.31   0.34   0.62   0.06   0.06   0.20   0.45   0.89  -0.30   1.00 #> item11   0.66   0.36   0.97   0.26   0.84   0.25   0.78   0.80   0.72   0.48 #> item12   0.50  -0.29   0.71   0.28   0.73   0.42   0.59   0.64  -0.14   0.72 #>        item11 item12 #> item01   0.66   0.50 #> item02   0.36  -0.29 #> item03   0.97   0.71 #> item04   0.26   0.28 #> item05   0.84   0.73 #> item06   0.25   0.42 #> item07   0.78   0.59 #> item08   0.80   0.64 #> item09   0.72  -0.14 #> item10   0.48   0.72 #> item11   1.00   0.71 #> item12   0.71   1.00"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"test-output","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## calculate Cronbach's Alpha alpha(cor_matrix_12) #> [1] 0.9000045  ## calculate eigenvalues of the correlation matrix eigenvalues(cor_matrix_12, 1) |> round(3) #> cor_matrix_12  is NOT positive-definite #>  [1]  6.033  3.006  2.192  1.396  0.789  0.371  0.081 -0.169 -0.364 -0.441 #> [11] -0.447 -0.448"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makecorralpha-with-diagnostics-output","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a correlation matrix from Cronbach’s Alpha > makeCorrAlpha() is volatile","what":"makeCorrAlpha() with diagnostics output","title":"LikertMakeR vignette","text":"","code":"## apply makeCorrAlpha() with diagnostics set.seed(42)  cor_matrix_5 <- makeCorrAlpha(   items = 6,   alpha = 0.90,   diagnostics = TRUE ) #> reached max iterations (3600) - best mean difference: 3.2e-05 #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 2 (min eigenvalue: -0.059282) #> improved at swap - 3 (min eigenvalue: -0.047945) #> improved at swap - 4 (min eigenvalue: -0.010881) #> improved at swap - 6 (min eigenvalue: 0.023082) #> positive definite at swap - 6"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"diagnostics-output","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## output cor_matrix_5$R |> round(2) #>        item01 item02 item03 item04 item05 item06 #> item01   1.00   0.70   0.63   0.83   0.75   0.74 #> item02   0.70   1.00   0.31   0.53   0.51   0.44 #> item03   0.63   0.31   1.00   0.49   0.79   0.73 #> item04   0.83   0.53   0.49   1.00   0.64   0.36 #> item05   0.75   0.51   0.79   0.64   1.00   0.55 #> item06   0.74   0.44   0.73   0.36   0.55   1.00  cor_matrix_5$diagnostics #> $items #> [1] 6 #>  #> $alpha_target #> [1] 0.9 #>  #> $alpha_achieved #> [1] 0.90002 #>  #> $average_r #> [1] 0.6000534 #>  #> $eigenvalues #> [1] 4.03582074 0.86126219 0.58519829 0.35723432 0.13740248 0.02308198 #>  #> $is_positive_definite #> [1] TRUE #>  #> $variance #> [1] 0.5 #>  #> $precision #> [1] 0 #>  #> $sort_cors #> [1] FALSE"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makecorrloadings","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a correlation matrix from factor loadings","what":"makeCorrLoadings","title":"LikertMakeR vignette","text":"makeCorrLoadings() generates correlation matrix factor loadings factor correlations might seen Exploratory Factor Analysis (EFA) Structural Equation Model (SEM).","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makecorrloadings-usage","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a correlation matrix from factor loadings > makeCorrLoadings","what":"makeCorrLoadings() usage","title":"LikertMakeR vignette","text":"","code":"makeCorrLoadings(loadings, factorCor = NULL, uniquenesses = NULL, nearPD = FALSE)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makecorrloadings-arguments","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"loadings: k (items) f (factors) matrix standardised factor loadings. Item names Factor names can taken row_names (items) column_names (factors), present. factorCor: f x f factor correlation matrix. present, assume factors uncorrelated (orthogonal), rare practice, function applies identity matrix factor_cor. uniquenesses: length k vector uniquenesses. NULL, default, compute calculated communalities. nearPD: (logical) TRUE, function calls nearPD function Matrix package transform resulting correlation matrix onto nearest Positive Definite matrix. Obviously, applies resulting correlation matrix positive definite. (never needed.)","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"note","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"“Censored” loadings (example, loadings less small value (often ‘0.30’), removed ease--communication) tend severely reduce accuracy makeCorrLoadings() function. detailed demonstration, see vignette file, makeCorrLoadings_Validate.","code":""},{"path":[]},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"define-parameters","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## Example loadings  factorLoadings <- matrix(   c(     0.05, 0.20, 0.70,     0.10, 0.05, 0.80,     0.05, 0.15, 0.85,     0.20, 0.85, 0.15,     0.05, 0.85, 0.10,     0.10, 0.90, 0.05,     0.90, 0.15, 0.05,     0.80, 0.10, 0.10   ),   nrow = 8, ncol = 3, byrow = TRUE )  ## row and column names  rownames(factorLoadings) <- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\", \"Q6\", \"Q7\", \"Q8\") colnames(factorLoadings) <- c(\"Factor1\", \"Factor2\", \"Factor3\")  ## Factor correlation matrix**  factorCor <- matrix(   c(     1.0,  0.5, 0.4,     0.5,  1.0, 0.3,     0.4,  0.3, 1.0   ),   nrow = 3, byrow = TRUE )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"apply-the-function","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## apply makeCorrLoadings() function itemCorrelations <- makeCorrLoadings(factorLoadings, factorCor)  ## derived correlation matrix to two decimal places round(itemCorrelations, 2) #>      Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8 #> Q1 1.00 0.62 0.67 0.48 0.42 0.42 0.43 0.41 #> Q2 0.62 1.00 0.72 0.43 0.36 0.36 0.44 0.42 #> Q3 0.67 0.72 1.00 0.50 0.43 0.43 0.46 0.45 #> Q4 0.48 0.43 0.50 1.00 0.79 0.83 0.65 0.58 #> Q5 0.42 0.36 0.43 0.79 1.00 0.80 0.54 0.48 #> Q6 0.42 0.36 0.43 0.83 0.80 1.00 0.59 0.52 #> Q7 0.43 0.44 0.46 0.65 0.54 0.59 1.00 0.78 #> Q8 0.41 0.42 0.45 0.58 0.48 0.52 0.78 1.00"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"test-makecorrloadings-output","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## correlated factors mean that eigenvalues should suggest two or three factors eigenvalues(cormatrix = itemCorrelations, scree = TRUE) #> itemCorrelations  is positive-definite #> [1] 4.7679427 1.2254239 0.7641967 0.3799863 0.2668158 0.2237851 0.2073574 #> [8] 0.1644922"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"assuming-orthogonal-factors","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## orthogonal factors are assumed when factor correlation matrix is not included orthogonalItemCors <- makeCorrLoadings(factorLoadings)  ## derived correlation matrix to two decimal places round(orthogonalItemCors, 2) #>      Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8 #> Q1 1.00 0.58 0.63 0.28 0.24 0.22 0.11 0.13 #> Q2 0.58 1.00 0.69 0.18 0.13 0.10 0.14 0.17 #> Q3 0.63 0.69 1.00 0.26 0.22 0.18 0.11 0.14 #> Q4 0.28 0.18 0.26 1.00 0.75 0.79 0.32 0.26 #> Q5 0.24 0.13 0.22 0.75 1.00 0.78 0.18 0.14 #> Q6 0.22 0.10 0.18 0.79 0.78 1.00 0.23 0.18 #> Q7 0.11 0.14 0.11 0.32 0.18 0.23 1.00 0.74 #> Q8 0.13 0.17 0.14 0.26 0.14 0.18 0.74 1.00"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"test-orthogonal-output","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## eigenvalues should suggest exactly  three factors eigenvalues(cormatrix = orthogonalItemCors, scree = TRUE) #> orthogonalItemCors  is positive-definite #> [1] 3.2769426 1.8091128 1.4966064 0.4244753 0.2966222 0.2605233 0.2402622 #> [8] 0.1954553"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makescales","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe of rating scales from a correlation matrix and predefined moments","what":"makeScales()","title":"LikertMakeR vignette","text":"makeScales() generates dataframe random discrete values data replicate set scale items summated rating scales, correlated close predefined correlation matrix. Generally, means, standard deviations, correlations correct two decimal places. makeScales() wrapper function lfast(), takes repeated samples selecting vector best fits desired moments, lcor(), rearranges values column dataframe closely match desired correlation matrix. create desired dataframe, user must define following parameters: n: number observations dfMeans: vector length k desired means variable dfSds: vector length k desired standard deviations variable lowerbound: vector length k values lower bound variable. default = ‘1’ upperbound: vector length k values upper bound variable. Default = ‘5’ items: vector length k number items variable. Default = ‘1’. cormatrix: target correlation matrix k rows k columns.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"four-correlated-items","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe of rating scales from a correlation matrix and predefined moments > makeScales() examples","what":"four correlated items","title":"LikertMakeR vignette","text":"","code":"## define parameters n <- 128 dfMeans <- c(2.5, 3.0, 3.0, 3.5) dfSds <- c(1.0, 1.0, 1.5, 0.75) lowerbound <- rep(1, 4) upperbound <- rep(5, 4)  corMat <- matrix(   c(     1.00, 0.25, 0.35, 0.45,     0.25, 1.00, 0.70, 0.75,     0.35, 0.70, 1.00, 0.85,     0.45, 0.75, 0.85, 1.00   ),   nrow = 4, ncol = 4 )  var_names <- c(\"var1\", \"var2\", \"var3\", \"var4\") colnames(corMat) <- var_names rownames(corMat) <- var_names  ## apply makeScales() function df <- makeScales(   n = n,   means = dfMeans,   sds = dfSds,   lowerbound = lowerbound,   upperbound = upperbound,   cormatrix = corMat ) #> Variable  1 :  var1  - #> reached maximum of 16384 iterations #> Variable  2 :  var2  - #> reached maximum of 16384 iterations #> Variable  3 :  var3  - #> best solution in 323 iterations #> Variable  4 :  var4  - #> reached maximum of 16384 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"structure-of-new-dataframe","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## test the function str(df) #> 'data.frame':    128 obs. of  4 variables: #>  $ var1: num  2 2 2 1 3 3 3 3 1 4 ... #>  $ var2: num  3 4 2 3 4 5 3 1 4 5 ... #>  $ var3: num  4 5 2 3 5 5 4 1 2 5 ... #>  $ var4: num  4 4 3 3 4 4 4 2 3 5 ..."},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"means-should-be-correct-to-two-decimal-places","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"### means should be correct to two decimal places dfmoments <- data.frame(   mean = apply(df, 2, mean) |> round(3),   sd = apply(df, 2, sd) |> round(3) ) |> t()  dfmoments #>       var1  var2  var3  var4 #> mean 2.500 3.000 3.000 3.500 #> sd   1.004 1.004 1.501 0.753"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"correlations-should-be-correct-to-two-decimal-places","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"### correlations should be correct to two decimal places cor(df) |> round(3) #>       var1 var2  var3  var4 #> var1 1.000 0.25 0.350 0.448 #> var2 0.250 1.00 0.700 0.750 #> var3 0.350 0.70 1.000 0.836 #> var4 0.448 0.75 0.836 1.000"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"four-likert-scales","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe of rating scales from a correlation matrix and predefined moments > makeScales() examples","what":"four Likert scales","title":"LikertMakeR vignette","text":"Brand Trust (BT) - confidence consumer brand’s reliability honesty. Brand Satisfaction (BS) - Overall affective evaluation brand experience. Brand Love (BL) - Deep emotional attachment toward brand. Brand Loyalty (BLY) - Intention repurchase recommend brand.","code":"## define parameters n <- 256 dfMeans <- c(3.9, 4.1, 3.6, 4.0) dfSds <- c(0.6, 0.5, 0.8, 0.7) lowerbound <- rep(1, 4) upperbound <- rep(5, 4) items <- c(4, 3, 4, 3)  corMat <- matrix(   c(     1.00, 0.75, 0.60, 0.70,     0.75, 1.00, 0.65, 0.72,     0.60, 0.65, 1.00, 0.68,     0.70, 0.72, 0.68, 1.00   ),   nrow = 4, ncol = 4 )  scale_names <- c(\"BT\", \"BS\", \"BL\", \"BLY\") rownames(corMat) <- scale_names colnames(corMat) <- scale_names  ## apply makeScales() function df <- makeScales(   n = n,   means = dfMeans,   sds = dfSds,   lowerbound = lowerbound,   upperbound = upperbound,   items = items,   cormatrix = corMat ) #> Variable  1 :  BT  - #> best solution in 336 iterations #> Variable  2 :  BS  - #> best solution in 421 iterations #> Variable  3 :  BL  - #> best solution in 923 iterations #> Variable  4 :  BLY  - #> best solution in 27 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables  ## test the function head(df) #>     BT       BS   BL      BLY #> 1 3.25 3.666667 4.25 4.333333 #> 2 4.25 4.333333 3.00 4.000000 #> 3 4.75 4.333333 3.50 4.333333 #> 4 3.50 3.666667 3.25 4.666667 #> 5 4.50 4.666667 4.25 4.666667 #> 6 4.25 4.000000 2.50 4.333333 tail(df) #>       BT       BS   BL      BLY #> 251 3.25 3.333333 2.00 3.333333 #> 252 4.00 3.666667 3.50 3.333333 #> 253 3.75 4.000000 3.25 3.666667 #> 254 3.00 4.000000 2.00 4.000000 #> 255 3.50 3.666667 2.75 3.333333 #> 256 4.50 4.666667 4.00 4.666667  ### means should be correct to two decimal places dfmoments <- data.frame(   mean = apply(df, 2, mean) |> round(3),   sd = apply(df, 2, sd) |> round(3) ) |> t()  dfmoments #>         BT    BS    BL   BLY #> mean 3.899 4.102 3.599 4.001 #> sd   0.601 0.500 0.800 0.700  ### correlations should be correct to two decimal places cor(df) |> round(3) #>        BT    BS   BL  BLY #> BT  1.000 0.751 0.60 0.70 #> BS  0.751 1.000 0.65 0.72 #> BL  0.600 0.650 1.00 0.68 #> BLY 0.700 0.720 0.68 1.00"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"generate-a-dataframe-from-cronbachs-alpha-and-predefined-moments","dir":"Articles","previous_headings":"Using LikertMakeR","what":"Generate a dataframe from Cronbach’s Alpha and predefined moments","title":"LikertMakeR vignette","text":"two-step process: apply makeCorrAlpha() generate correlation matrix desired alpha, apply makeScales() generate rating-scale items correlation matrix desired moments Required parameters : k: number items/ columns alpha: target Cronbach’s Alpha. n: number observations lowerbound: vector length k values lower bound variable upperbound: vector length k values upper bound variable means: vector length k desired means variable sds: vector length k desired standard deviations variable","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"step-1-generate-a-correlation-matrix","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe from Cronbach’s Alpha and predefined moments","what":"Step 1: Generate a correlation matrix","title":"LikertMakeR vignette","text":"","code":"## define parameters k <- 6 myAlpha <- 0.85  ## generate correlation matrix set.seed(42) myCorr <- makeCorrAlpha(items = k, alpha = myAlpha) #> reached max iterations (3600) - best mean difference: 1.4e-05 #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 1 (min eigenvalue: 0.015206) #> positive definite at swap - 1  ## display correlation matrix myCorr |> round(3) #>        item01 item02 item03 item04 item05 item06 #> item01  1.000  0.761  0.633  0.553  0.421  0.458 #> item02  0.761  1.000  0.413  0.353  0.130  0.781 #> item03  0.633  0.413  1.000  0.433  0.595  0.585 #> item04  0.553  0.353  0.433  1.000  0.597  0.400 #> item05  0.421  0.130  0.595  0.597  1.000  0.173 #> item06  0.458  0.781  0.585  0.400  0.173  1.000  ### checking Cronbach's Alpha alpha(cormatrix = myCorr) #> [1] 0.8500034"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"step-2-generate-dataframe","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe from Cronbach’s Alpha and predefined moments","what":"Step 2: Generate dataframe","title":"LikertMakeR vignette","text":"","code":"## define parameters n <- 256 myMeans <- c(2.75, 3.00, 3.00, 3.25, 3.50, 3.5) mySds <- c(1.00, 0.75, 1.00, 1.00, 1.00, 1.5) lowerbound <- rep(1, k) upperbound <- rep(5, k)  ## Generate Items myItems <- makeScales(   n = n, means = myMeans, sds = mySds,   lowerbound = lowerbound, upperbound = upperbound,   items = 1,   cormatrix = myCorr ) #> Variable  1 :  item01  - #> best solution in 843 iterations #> Variable  2 :  item02  - #> best solution in 583 iterations #> Variable  3 :  item03  - #> best solution in 2263 iterations #> Variable  4 :  item04  - #> best solution in 169 iterations #> Variable  5 :  item05  - #> best solution in 2353 iterations #> Variable  6 :  item06  - #> best solution in 3633 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables  ## resulting dataframe head(myItems) #>   item01 item02 item03 item04 item05 item06 #> 1      2      3      2      3      3      3 #> 2      2      3      2      2      2      4 #> 3      4      3      3      3      4      2 #> 4      2      3      2      3      2      4 #> 5      1      2      2      2      2      3 #> 6      3      3      2      3      3      2 tail(myItems) #>     item01 item02 item03 item04 item05 item06 #> 251      2      3      3      4      4      5 #> 252      4      4      4      3      3      5 #> 253      5      4      4      5      5      5 #> 254      2      2      3      4      4      2 #> 255      4      4      4      5      4      5 #> 256      2      3      4      3      4      5  ## means and standard deviations myMoments <- data.frame(   means = apply(myItems, 2, mean) |> round(3),   sds = apply(myItems, 2, sd) |> round(3) ) |> t() myMoments #>       item01 item02 item03 item04 item05 item06 #> means  2.750  3.000  3.000  3.250  3.500    3.5 #> sds    0.998  0.751  0.998  1.002  1.002    1.5  ## Cronbach's Alpha of dataframe alpha(NULL, myItems) #> [1] 0.849134"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"summary-plots-of-new-dataframe","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe from Cronbach’s Alpha and predefined moments > Step 2: Generate dataframe","what":"Summary plots of new dataframe","title":"LikertMakeR vignette","text":"Summary dataframe makeScales() function","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makeitemsscale","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe of rating-scale items from a summated rating scale","what":"makeItemsScale()","title":"LikertMakeR vignette","text":"makeItemsScale() generates dataframe rating-scale items summated rating scale desired Cronbach’s Alpha. create desired dataframe, user must define following parameters: scale: vector dataframe summated rating scale. range (‘lowerbound’ * ‘items’) (‘upperbound’ * ‘items’) lowerbound: lower bound scale item (example: ‘1’ ‘1’ ‘5’ rating) upperbound: upper bound scale item (example: ‘5’ ‘1’ ‘5’ rating) items: k, number columns generate alpha: desired Cronbach’s Alpha. Default = ‘0.8’ variance: quantile selecting combination items give summated scores. Must lie ‘0’ (minimum variance) ‘1’ (maximum variance). Default = ‘0.5’.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"generate-a-summated-scale","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"Summated scale distribution","code":"## define parameters n <- 256 mean <- 3.00 sd <- 0.85 lowerbound <- 1 upperbound <- 5 items <- 4  ## apply lfast() function meanScale <- lfast(   n = n, mean = mean, sd = sd,   lowerbound = lowerbound, upperbound = upperbound,   items = items ) #> best solution in 313 iterations  ## sum over all items summatedScale <- meanScale * items"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"create-items-with-makeitemsscale","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe of rating-scale items from a summated rating scale > makeItemsScale()","what":"create items with makeItemsScale()","title":"LikertMakeR vignette","text":"","code":"## apply makeItemsScale() function  newItems_1 <- makeItemsScale(   scale = summatedScale,   lowerbound = lowerbound,   upperbound = upperbound,   items = items ) #> generate 256 rows #> rearrange 4 values within each of 256 rows #> Complete! #> desired Cronbach's alpha = 0.8 (achieved alpha = 0.799)  ### First 10 observations and summated scale head(cbind(newItems_1, summatedScale), 10) #>    V1 V2 V3 V4 summatedScale #> 1   1  2  2  5            10 #> 2   2  5  3  4            14 #> 3   1  4  4  2            11 #> 4   2  2  2  2             8 #> 5   4  4  4  5            17 #> 6   1  4  2  2             9 #> 7   1  2  1  1             5 #> 8   1  3  3  3            10 #> 9   2  5  4  4            15 #> 10  1  3  3  2             9  ### correlation matrix cor(newItems_1) |> round(2) #>      V1   V2   V3   V4 #> V1 1.00 0.53 0.62 0.52 #> V2 0.53 1.00 0.61 0.35 #> V3 0.62 0.61 1.00 0.37 #> V4 0.52 0.35 0.37 1.00  ### default Cronbach's alpha = 0.80 alpha(data = newItems_1) |> round(4) #> [1] 0.799  ### calculate eigenvalues and print scree plot eigenvalues(cor(newItems_1), 1) |> round(3) #> cor(newItems_1)  is positive-definite #> [1] 2.509 0.717 0.437 0.337"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makeitemsscale-with-same-summated-values-and-higher-alpha","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe of rating-scale items from a summated rating scale > makeItemsScale()","what":"makeItemsScale() with same summated values and higher alpha","title":"LikertMakeR vignette","text":"","code":"## apply makeItemsScale() function newItems_2 <- makeItemsScale(   scale = summatedScale,   lowerbound = lowerbound,   upperbound = upperbound,   items = items,   alpha = 0.9 ) #> generate 256 rows #> rearrange 4 values within each of 256 rows #> Complete! #> desired Cronbach's alpha = 0.9 (achieved alpha = 0.8775)  ### First 10 observations and summated scale head(cbind(newItems_2, summatedScale), 10) #>    V1 V2 V3 V4 summatedScale #> 1   3  1  3  3            10 #> 2   4  2  4  4            14 #> 3   4  1  2  4            11 #> 4   4  1  1  2             8 #> 5   5  4  4  4            17 #> 6   3  2  2  2             9 #> 7   2  1  1  1             5 #> 8   4  2  2  2            10 #> 9   5  3  3  4            15 #> 10  3  1  2  3             9  ### correlation matrix cor(newItems_2) |> round(2) #>      V1   V2   V3   V4 #> V1 1.00 0.59 0.61 0.68 #> V2 0.59 1.00 0.66 0.60 #> V3 0.61 0.66 1.00 0.70 #> V4 0.68 0.60 0.70 1.00  ### requested Cronbach's alpha = 0.90 alpha(data = newItems_2) |> round(4) #> [1] 0.8775  ### calculate eigenvalues and print scree plot eigenvalues(cor(newItems_2), 1) |> round(3) #> cor(newItems_2)  is positive-definite #> [1] 2.926 0.436 0.370 0.268"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"same-summated-values-with-lower-alpha-may-require-higher-variance","dir":"Articles","previous_headings":"Using LikertMakeR > Generate a dataframe of rating-scale items from a summated rating scale > makeItemsScale()","what":"same summated values with lower alpha may require higher variance","title":"LikertMakeR vignette","text":"","code":"## apply makeItemsScale() function newItems_3 <- makeItemsScale(   scale = summatedScale,   lowerbound = lowerbound,   upperbound = upperbound,   items = items,   alpha = 0.6,   variance = 0.7 ) #> generate 256 rows #> rearrange 4 values within each of 256 rows #> Complete! #> desired Cronbach's alpha = 0.6 (achieved alpha = 0.6)  ### First 10 observations and summated scale head(cbind(newItems_3, summatedScale), 10) #>    V1 V2 V3 V4 summatedScale #> 1   3  1  3  3            10 #> 2   3  5  4  2            14 #> 3   2  5  2  2            11 #> 4   1  1  1  5             8 #> 5   3  4  5  5            17 #> 6   2  1  4  2             9 #> 7   1  1  2  1             5 #> 8   1  4  3  2            10 #> 9   2  4  4  5            15 #> 10  1  3  3  2             9  ### correlation matrix cor(newItems_3) |> round(2) #>      V1   V2   V3   V4 #> V1 1.00 0.44 0.43 0.19 #> V2 0.44 1.00 0.28 0.13 #> V3 0.43 0.28 1.00 0.16 #> V4 0.19 0.13 0.16 1.00  ### requested Cronbach's alpha = 0.70 alpha(data = newItems_3) |> round(4) #> [1] 0.6  ### calculate eigenvalues and print scree plot eigenvalues(cor(newItems_3), 1) |> round(3) #> cor(newItems_3)  is positive-definite #> [1] 1.866 0.915 0.715 0.504"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"create-a-dataframe-for-a-t-test","dir":"Articles","previous_headings":"Using LikertMakeR","what":"Create a dataframe for a t-test","title":"LikertMakeR vignette","text":"Generating data independent-samples t-test trivial LikertMakeR. dataframe paired-sample t-test tricky observations related . , must generate dataframe correlated observations.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"independent-samples-t-test","dir":"Articles","previous_headings":"Using LikertMakeR > Create a dataframe for a t-test","what":"Independent-samples t-test","title":"LikertMakeR vignette","text":"Note tests don’t even require sample-size.","code":"## define parameters lower <- 1 upper <- 5 items <- 6  ## generate two independent samples x1 <- lfast(   n = 20, mean = 2.5, sd = 0.75,   lowerbound = lower, upperbound = upper, items = items ) #> reached maximum of 1024 iterations x2 <- lfast(   n = 30, mean = 3.0, sd = 0.85,   lowerbound = lower, upperbound = upper, items = items ) #> reached maximum of 1024 iterations  ## run independent-samples t-test t.test(x1, x2) #>  #>  Welch Two Sample t-test #>  #> data:  x1 and x2 #> t = -2.186, df = 44.464, p-value = 0.03412 #> alternative hypothesis: true difference in means is not equal to 0 #> 95 percent confidence interval: #>  -0.96083888 -0.03916112 #> sample estimates: #> mean of x mean of y  #>       2.5       3.0"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makepaired-paired-sample-t-test","dir":"Articles","previous_headings":"Using LikertMakeR > Create a dataframe for a t-test","what":"makePaired() paired-sample t-test","title":"LikertMakeR vignette","text":"makePaired() generates correlated values data replicate rating scales taken, example, experimental design. function effectively wrapper function lfast() lcor() addition t-statistic -column correlation inferred. Paired t-tests apply observations associated . example: people rating object treatment, people rating two different objects, ratings husband & wife, etc. makePaired() similar parameters lfast() function addition value desired t-statistic. n sample size means [1:2] vector target means two /measures sds [1:2] vector target standard deviations t_value desired paired t-statistic lowerbound lower bound (e.g. ‘1’ 1-5 rating scale) upperbound upper bound (e.g. ‘5’ 1-5 rating scale) items number items rating scale. precision can relax level accuracy required, lfast().","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makepaired-examples","dir":"Articles","previous_headings":"Using LikertMakeR > Create a dataframe for a t-test","what":"makePaired() examples","title":"LikertMakeR vignette","text":"","code":"## define parameters n <- 20 means <- c(2.5, 3.0) sds <- c(0.75, 0.85) lower <- 1 upper <- 5 items <- 6 t <- -2.5  ## run the function pairedDat <- makePaired(   n = n, means = means, sds = sds,   t_value = t,   lowerbound = lower, upperbound = upper, items = items ) #> Initial data vectors #> best solution in 6 iterations #> reached maximum of 1024 iterations #> Arranging values to conform with desired t-value #> Complete!"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"check-properties-of-new-data","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## test function output str(pairedDat) #> 'data.frame':    20 obs. of  2 variables: #>  $ X1: num  4.33 2.67 3.17 2.83 2 ... #>  $ X2: num  3 3.5 4 2.17 1.83 ...  cor(pairedDat) |> round(2) #>      X1   X2 #> X1 1.00 0.38 #> X2 0.38 1.00  pairedMoments <- data.frame(   mean = apply(pairedDat, MARGIN = 2, FUN = mean) |> round(3),   sd = apply(pairedDat, MARGIN = 2, FUN = sd) |> round(3) ) |> t()  pairedMoments #>         X1    X2 #> mean 2.500 3.000 #> sd   0.749 0.845"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"run-a-paired-sample-t-test-with-the-new-data","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## run a paired-sample t-test  paired_t <- t.test(x = pairedDat$X1, y = pairedDat$X2, paired = TRUE)  # paired_t <- t.test(pairedDat$X1, pairedDat$X2, paired = TRUE)   paired_t #>  #>  Paired t-test #>  #> data:  pairedDat$X1 and pairedDat$X2 #> t = -2.512, df = 19, p-value = 0.0212 #> alternative hypothesis: true mean difference is not equal to 0 #> 95 percent confidence interval: #>  -0.91661108 -0.08338892 #> sample estimates: #> mean difference  #>            -0.5"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makerepeated","dir":"Articles","previous_headings":"Using LikertMakeR > Create a dataframe for Repeated-Measures ANOVA","what":"makeRepeated()","title":"LikertMakeR vignette","text":"makeRepeated() Reconstructs synthetic dataset inter-timepoint correlation matrix repeated-measures ANOVA result, based reported means, standard deviations, F-statistic. function estimates average correlation repeated measures matching reported F-statistic, one three assumed correlation structures: \"cs\" (Compound Symmetry): Compound Symmetry assumes repeated measures equally correlated . , correlation time 1 time 2 time 1 time 3, . structure commonly used repeated-measures ANOVA default. ’s mathematically simple reflects idea timepoints equally related. However, may realistic data correlations decrease time intervals increase (e.g., memory decay learning effects). \"ar1\" (First-Order Autoregressive): first-order autoregressive, assumes measurements closer together time highly correlated apart. example, correlation time 1 time 2 stronger time 1 time 3. pattern often realistic longitudinal time-series studies change gradual. correlation drops exponentially time step. \"toeplitz\" (Linearly Decreasing): Toeplitz structure flexible option allows correlation measurements decrease linearly time gap increases. Unlike AR(1), decline exponential, Toeplitz structure assumes straight-line drop correlation.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makerepeated-usage","dir":"Articles","previous_headings":"Using LikertMakeR > Create a dataframe for Repeated-Measures ANOVA > makeRepeated()","what":"makeRepeated() usage","title":"LikertMakeR vignette","text":"","code":"makeRepeated(   n,    k,    means,    sds,   f_stat,   df_between = k - 1,   df_within = (n - 1) * (k - 1),   structure = c(\"cs\", \"ar1\", \"toeplitz\"),   names = paste0(\"time_\", 1:k),   items = 1,   lowerbound = 1, upperbound = 5,   return_corr_only = FALSE,   diagnostics = FALSE,   ... )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makerepeated-arguments","dir":"Articles","previous_headings":"Using LikertMakeR > Create a dataframe for Repeated-Measures ANOVA > makeRepeated()","what":"makeRepeated() arguments","title":"LikertMakeR vignette","text":"n Integer. Sample size used original study. k Integer. Number repeated measures (timepoints). means Numeric vector length k. Mean values reported timepoint. sds Numeric vector length k. Standard deviations reported timepoint. f_stat Numeric. reported repeated-measures ANOVA F-statistic within-subjects factor. df_between, Degrees freedom conditions (default: k - 1). df_within, Degrees freedom within-subjects (default: (n - 1) * (k - 1)). structure Character. Correlation structure assume: \"cs\", \"ar1\", \"toeplitz\" (default). names Character vector length k. Variable names timepoint (default: \"time_1\" \"time_k\"). items Integer. Number items used generate scale score (passed link{lfast}). (default: 1). lowerbound, Integer. Lower bounds Likert-type response scales (default: 1). upperbound, Integer. upper bounds Likert-type response scales (default: 5). return_corr_only Logical. TRUE, return estimated correlation matrix. diagnostics Logical. TRUE, include diagnostic summaries feasible F-statistic range effect sizes.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makerepeated-examples","dir":"Articles","previous_headings":"Using LikertMakeR > Create a dataframe for Repeated-Measures ANOVA > makeRepeated()","what":"makeRepeated() examples","title":"LikertMakeR vignette","text":"","code":"out1 <- makeRepeated(   n = 128,   k = 3,   means = c(3.1, 3.5, 3.9),   sds = c(1.0, 1.1, 1.0),   items = 4,   f_stat = 4.87,   structure = \"cs\",   diagnostics = FALSE ) #> Warning in makeRepeated(n = 128, k = 3, means = c(3.1, 3.5, 3.9), sds = c(1, : #> Optimization may not have converged. Check results carefully. #> best solution in 413 iterations #> best solution in 1112 iterations #> best solution in 758 iterations  head(out1$data) #>   time_1 time_2 time_3 #> 1   3.25   4.00   3.25 #> 2   2.25   4.75   4.00 #> 3   2.50   4.50   3.75 #> 4   2.25   2.75   5.00 #> 5   1.50   5.00   4.25 #> 6   4.50   4.75   1.50 out1$correlation_matrix #>            time_1     time_2     time_3 #> time_1  1.0000000 -0.4899454 -0.4899454 #> time_2 -0.4899454  1.0000000 -0.4899454 #> time_3 -0.4899454 -0.4899454  1.0000000   out2 <- makeRepeated(   n = 32, k = 4,   means = c(2.75, 3.5, 4.0, 4.4),   sds = c(0.8, 1.0, 1.2, 1.0),   f_stat = 16,   structure = \"ar1\",   items = 5,   lowerbound = 1, upperbound = 7,   return_corr_only = FALSE,   diagnostics = TRUE ) #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations  print(out2) #> $data #>    time_1 time_2 time_3 time_4 #> 1     2.8    3.0    4.4    5.0 #> 2     4.6    5.0    3.4    3.6 #> 3     4.2    4.6    4.6    4.4 #> 4     3.4    3.4    4.0    6.0 #> 5     2.4    2.6    3.2    4.4 #> 6     3.0    2.8    2.2    4.4 #> 7     3.4    5.0    6.4    4.4 #> 8     3.0    3.0    5.6    5.8 #> 9     3.0    3.4    4.6    5.6 #> 10    3.0    3.0    6.0    4.4 #> 11    4.0    4.0    4.4    5.0 #> 12    2.2    3.6    4.2    4.0 #> 13    1.8    2.4    4.0    5.0 #> 14    2.0    4.4    2.8    3.8 #> 15    3.6    3.6    3.2    4.0 #> 16    2.8    4.6    3.6    4.0 #> 17    2.2    2.6    2.2    4.2 #> 18    3.0    4.6    3.0    3.0 #> 19    1.8    1.6    2.8    1.8 #> 20    2.8    3.8    4.8    4.0 #> 21    2.2    3.6    5.8    5.2 #> 22    2.8    2.2    3.8    6.0 #> 23    3.6    3.4    3.6    5.0 #> 24    1.4    2.0    4.4    3.4 #> 25    2.0    4.2    3.2    4.4 #> 26    3.6    4.0    5.4    3.4 #> 27    2.4    2.6    2.6    3.6 #> 28    2.2    3.6    4.6    4.6 #> 29    2.8    2.8    2.0    3.8 #> 30    1.6    2.2    2.6    4.0 #> 31    1.6    5.8    5.6    6.8 #> 32    2.8    4.6    5.2    3.6 #>  #> $correlation_matrix #>            time_1    time_2    time_3     time_4 #> time_1 1.00000000 0.3910032 0.1528835 0.05977794 #> time_2 0.39100319 1.0000000 0.3910032 0.15288350 #> time_3 0.15288350 0.3910032 1.0000000 0.39100319 #> time_4 0.05977794 0.1528835 0.3910032 1.00000000 #>  #> $structure #> [1] \"ar1\" #>  #> $feasible_f_range #>       min       max  #>  9.353034 39.481390  #>  #> $recommended_f #> $recommended_f$conservative #> [1] 10.21 #>  #> $recommended_f$moderate #> [1] 11.91 #>  #> $recommended_f$strong #> [1] 30.29 #>  #>  #> $achieved_f #> [1] 15.99983 #>  #> $effect_size_raw #> [1] 0.3792188 #>  #> $effect_size_standardised #> [1] 0.3717831   out3 <- makeRepeated(   n = 32, k = 4,   means = c(2.0, 2.5, 3.0, 2.8),   sds = c(0.8, 0.9, 1.0, 0.9),   items = 4,   f_stat = 24,   structure = \"toeplitz\",   diagnostics = TRUE ) #> Warning in makeRepeated(n = 32, k = 4, means = c(2, 2.5, 3, 2.8), sds = c(0.8, #> : Optimization may not have converged. Check results carefully. #> best solution in 683 iterations #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations  str(out3) #> List of 8 #>  $ data                    :'data.frame':    32 obs. of  4 variables: #>   ..$ time_1: num [1:32] 1 1.5 3 2 4 1.75 1.75 3 1.25 2.25 ... #>   ..$ time_2: num [1:32] 1.25 1.75 3 2.75 2.5 2.25 2.25 3.75 1.25 3 ... #>   ..$ time_3: num [1:32] 3.5 1.25 2.5 3.25 3.5 1.75 1.75 3.75 2.5 3.25 ... #>   ..$ time_4: num [1:32] 3.25 1.5 2 2.75 2.5 1.25 2.25 3 2.5 2.25 ... #>  $ correlation_matrix      : num [1:4, 1:4] 1 0.66 0.33 0 0.66 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:4] \"time_1\" \"time_2\" \"time_3\" \"time_4\" #>   .. ..$ : chr [1:4] \"time_1\" \"time_2\" \"time_3\" \"time_4\" #>  $ structure               : chr \"toeplitz\" #>  $ feasible_f_range        : Named num [1:2] 5.57 8.64 #>   ..- attr(*, \"names\")= chr [1:2] \"min\" \"max\" #>  $ recommended_f           :List of 3 #>   ..$ conservative: num 5.59 #>   ..$ moderate    : num 5.62 #>   ..$ strong      : num 7.64 #>  $ achieved_f              : num 9.95 #>  $ effect_size_raw         : num 0.142 #>  $ effect_size_standardised: num 0.174"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makescalesregression","dir":"Articles","previous_headings":"Using LikertMakeR > Generate rating-scale data from multiple regression results","what":"makeScalesRegression()","title":"LikertMakeR vignette","text":"Generates synthetic rating-scale data replicates reported regression results: standardised betas, R^2, correlation matrix independent variables (available).","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makescalesregression-usage","dir":"Articles","previous_headings":"Using LikertMakeR > Generate rating-scale data from multiple regression results > makeScalesRegression()","what":"makeScalesRegression() usage","title":"LikertMakeR vignette","text":"","code":"makeScalesRegression <- (    n,     beta_std,     r_squared,     iv_cormatrix = NULL,     iv_cor_mean = 0.3,      iv_cor_variance = 0.01,     iv_cor_range = c(-0.7, 0.7),     iv_means,     iv_sds,      dv_mean,      dv_sd,     lowerbound_iv,      upperbound_iv,      lowerbound_dv,     upperbound_dv,      items_iv = 1,      items_dv = 1,      var_names = NULL,      tolerance = 0.005  )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"makescalesregression-arguments","dir":"Articles","previous_headings":"Using LikertMakeR > Generate rating-scale data from multiple regression results > makeScalesRegression()","what":"makeScalesRegression() arguments","title":"LikertMakeR vignette","text":"n sample size. beta_std vector length k (number independent variables) standardised betas. r_squared model R^2 iv_cormatrix independent variables correlation matrix. Default= NULL iv_cor_mean iv_cormatrix, average IV correlations. Default = 0.3 iv_cor_variance iv_cormatrix, variation iv_cormatrix. Default = 0.01 iv_cor_range iv_cormatrix, range iv_cormatrix. Default = c(-0.7, 0.7) iv_means vector length k IV mean values iv_sds vector length k IV standard deviations dv_mean mean Dependent Variable (DV) dv_sd standard deviation DV lowerbound_iv vector length k lowerbounds IV’s upperbound_iv vector length k upperbounds IV’s lowerbound_dv lowerbound DV upperbound_dv upperbound DV items_iv vector length k number items IV’s. Default = 1. items_dv number items DV. Default = 1. var_names vector variable names (Independent Variables first Dependent Variable). Default = NULL tolerance close target R-squared. Default = 0.005","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"example-1-with-provided-iv-correlation-matrix","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"set.seed(123) iv_corr <- matrix(c(1.0, 0.3, 0.3, 1.0), nrow = 2)  result1 <- makeScalesRegression(   n = 64,   beta_std = c(0.4, 0.3),   r_squared = 0.35,   iv_cormatrix = iv_corr,   iv_means = c(3.0, 3.5),   iv_sds = c(1.0, 0.9),   dv_mean = 3.8,   dv_sd = 1.1,   lowerbound_iv = 1,   upperbound_iv = 5,   lowerbound_dv = 1,   upperbound_dv = 5,   items_iv = 4,   items_dv = 4,   var_names = c(\"Attitude\", \"Intention\", \"Behaviour\") )  print(result1) head(result1$data)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"example-2-with-optimisation-no-iv-correlation-matrix","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"set.seed(456) result2 <- makeScalesRegression(   n = 64,   beta_std = c(0.3, 0.25, 0.2),   r_squared = 0.40,   iv_cormatrix = NULL, # Will be optimised   iv_cor_mean = 0.3,   iv_cor_variance = 0.02,   iv_means = c(3.0, 3.2, 2.8),   iv_sds = c(1.0, 0.9, 1.1),   dv_mean = 3.5,   dv_sd = 1.0,   lowerbound_iv = 1,   upperbound_iv = 5,   lowerbound_dv = 1,   upperbound_dv = 5,   items_iv = 4,   items_dv = 5 )  # View optimised correlation matrix print(result2$target_stats$iv_cormatrix) print(result2$optimisation_info)"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"correlatescales","dir":"Articles","previous_headings":"Using LikertMakeR > Create a multidimensional dataframe of correlated scale items","what":"correlateScales()","title":"LikertMakeR vignette","text":"Correlated rating-scale items generally summed averaged create measure “unobservable”, “latent”, construct. correlateScales() takes several dataframes rating-scale items rearranges rows scales correlated according predefined correlation matrix. Univariate statistics dataframe rating-scale items change, correlations rating-scale items dataframes . run correlateScales(), parameters : dataframes: list k dataframes rearranged combined scalecors: target correlation matrix - symmetric k*k positive-semi-definite matrix, k number dataframes functions LikertMakeR, correlateScales() focuses item scale moments (mean standard deviation) rather covariance structure. wish simulate data teaching experimenting Structural Equation modelling, recommend sim.item() sim.congeneric() functions psych package","code":""},{"path":[]},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"create-dataframes-of-likert-scale-items","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"n <- 128 lower <- 1 upper <- 5  ### attitude #1  #### generate a correlation matrix cor_1 <- makeCorrAlpha(items = 4, alpha = 0.80) #> reached max iterations (1600) - best mean difference: 4.6e-05  #### specify moments as vectors means_1 <- c(2.5, 2.5, 3.0, 3.5) sds_1 <- c(0.75, 0.85, 0.85, 0.75)  #### apply makeScales() function Att_1 <- makeScales(   n = n, means = means_1, sds = sds_1,   lowerbound = rep(lower, 4), upperbound = rep(upper, 4),   items = 1,   cormatrix = cor_1 ) #> Variable  1 :  item01  - #> reached maximum of 16384 iterations #> Variable  2 :  item02  - #> best solution in 1475 iterations #> Variable  3 :  item03  - #> best solution in 171 iterations #> Variable  4 :  item04  - #> reached maximum of 16384 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables  ### attitude #2  #### generate a correlation matrix cor_2 <- makeCorrAlpha(items = 5, alpha = 0.85) #> reached max iterations (2500) - best mean difference: 4e-05  #### specify moments as vectors means_2 <- c(2.5, 2.5, 3.0, 3.0, 3.5) sds_2 <- c(0.75, 0.85, 0.75, 0.85, 0.75)  #### apply makeScales() function Att_2 <- makeScales(   n, means_2, sds_2,   rep(lower, 5), rep(upper, 5),   items = 1,   cor_2 ) #> Variable  1 :  item01  - #> reached maximum of 16384 iterations #> Variable  2 :  item02  - #> best solution in 34 iterations #> Variable  3 :  item03  - #> reached maximum of 16384 iterations #> Variable  4 :  item04  - #> best solution in 234 iterations #> Variable  5 :  item05  - #> reached maximum of 16384 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables  ### attitude #3  #### generate a correlation matrix cor_3 <- makeCorrAlpha(items = 6, alpha = 0.90) #> reached max iterations (3600) - best mean difference: 4e-05 #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 2 (min eigenvalue: -0.031507) #> improved at swap - 6 (min eigenvalue: -0.018702) #> improved at swap - 7 (min eigenvalue: -0.010675) #> improved at swap - 10 (min eigenvalue: 0.052317) #> positive definite at swap - 10  #### specify moments as vectors means_3 <- c(2.5, 2.5, 3.0, 3.0, 3.5, 3.5) sds_3 <- c(0.75, 0.85, 0.85, 1.0, 0.75, 0.85)  #### apply makeScales() function Att_3 <- makeScales(   n, means_3, sds_3,   rep(lower, 6), rep(upper, 6),   items = 1,   cor_3 ) #> Variable  1 :  item01  - #> reached maximum of 16384 iterations #> Variable  2 :  item02  - #> best solution in 1140 iterations #> Variable  3 :  item03  - #> best solution in 41 iterations #> Variable  4 :  item04  - #> reached maximum of 16384 iterations #> Variable  5 :  item05  - #> reached maximum of 16384 iterations #> Variable  6 :  item06  - #> best solution in 561 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables  ### behavioural intention intent <- lfast(n, mean = 4.0, sd = 3, lowerbound = 0, upperbound = 10) |>   data.frame() #> best solution in 1305 iterations names(intent) <- \"int\""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"check-properties-of-item-dataframes","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## Attitude #1 A1_moments <- data.frame(   means = apply(Att_1, 2, mean) |> round(2),   sds = apply(Att_1, 2, sd) |> round(2) ) |> t()  ### Attitude #1 moments A1_moments #>       item01 item02 item03 item04 #> means   2.50   2.50   3.00   3.50 #> sds     0.75   0.85   0.85   0.75  ### Attitude #1 correlations cor(Att_1) |> round(2) #>        item01 item02 item03 item04 #> item01   1.00   0.49   0.37   0.51 #> item02   0.49   1.00   0.65   0.68 #> item03   0.37   0.65   1.00   0.28 #> item04   0.51   0.68   0.28   1.00  ### Attitude #1 cronbach's alpha alpha(cor(Att_1)) |> round(3) #> [1] 0.798  ## Attitude #2 A2_moments <- data.frame(   means = apply(Att_2, 2, mean) |> round(2),   sds = apply(Att_2, 2, sd) |> round(2) ) |> t()  ### Attitude #2 moments A2_moments #>       item01 item02 item03 item04 item05 #> means   2.50   2.50   3.00   3.00   3.50 #> sds     0.75   0.85   0.75   0.85   0.75  ### Attitude #2 correlations cor(Att_2) |> round(2) #>        item01 item02 item03 item04 item05 #> item01   1.00   0.61   0.54   0.74   0.61 #> item02   0.61   1.00   0.41   0.47   0.44 #> item03   0.54   0.41   1.00   0.60   0.35 #> item04   0.74   0.47   0.60   1.00   0.53 #> item05   0.61   0.44   0.35   0.53   1.00  ### Attitude #2 cronbach's alpha alpha(cor(Att_2)) |> round(3) #> [1] 0.849  ## Attitude #3 A3_moments <- data.frame(   means = apply(Att_3, 2, mean) |> round(2),   sds = apply(Att_3, 2, sd) |> round(2) ) |> t()  ### Attitude #3 moments A3_moments #>       item01 item02 item03 item04 item05 item06 #> means   2.50   2.50   3.00      3   3.50   3.50 #> sds     0.75   0.85   0.85      1   0.75   0.85  ### Attitude #3 correlations cor(Att_3) |> round(2) #>        item01 item02 item03 item04 item05 item06 #> item01   1.00   0.53   0.79   0.43   0.71   0.63 #> item02   0.53   1.00   0.72   0.86   0.47   0.41 #> item03   0.79   0.72   1.00   0.71   0.71   0.66 #> item04   0.43   0.86   0.71   1.00   0.43   0.46 #> item05   0.71   0.47   0.71   0.43   1.00   0.42 #> item06   0.63   0.41   0.66   0.46   0.42   1.00  ### Attitude #2 cronbach's alpha alpha(cor(Att_3)) |> round(3) #> [1] 0.898   ## Behavioural Intention  intent_moments <- data.frame(   mean = apply(intent, 2, mean) |> round(3),   sd = apply(intent, 2, sd) |> round(3) ) |> t()  ### Intention moments intent_moments #>        int #> mean 4.000 #> sd   2.999"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"correlatescales-parameters","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"### target scale correlation matrix scale_cors <- matrix(   c(     1.0, 0.7, 0.6, 0.5,     0.7, 1.0, 0.4, 0.3,     0.6, 0.4, 1.0, 0.2,     0.5, 0.3, 0.2, 1.0   ),   nrow = 4 )  ### bring dataframes into a list data_frames <- list(\"A1\" = Att_1, \"A2\" = Att_2, \"A3\" = Att_3, \"Int\" = intent)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"apply-the-correlatescales-function","dir":"Articles","previous_headings":"Using LikertMakeR > Create a multidimensional dataframe of correlated scale items > correlateScales() examples","what":"apply the correlateScales() function","title":"LikertMakeR vignette","text":"","code":"### apply correlateScales() function my_correlated_scales <- correlateScales(   dataframes = data_frames,   scalecors = scale_cors ) #> scalecors  is positive-definite #> New dataframe successfully created"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"check-the-properties-of-our-derived-dataframe","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"## data structure str(my_correlated_scales) #> 'data.frame':    128 obs. of  16 variables: #>  $ A1_1 : num  4 2 2 2 3 2 2 2 1 2 ... #>  $ A1_2 : num  3 2 1 3 2 2 2 2 1 2 ... #>  $ A1_3 : num  2 3 2 2 4 3 3 3 2 3 ... #>  $ A1_4 : num  4 4 3 4 3 4 3 3 2 3 ... #>  $ A2_1 : num  3 2 2 1 3 2 3 2 2 3 ... #>  $ A2_2 : num  1 2 2 1 3 3 3 2 2 2 ... #>  $ A2_3 : num  3 4 3 2 3 3 3 3 2 3 ... #>  $ A2_4 : num  3 3 2 1 4 3 3 2 2 4 ... #>  $ A2_5 : num  4 3 3 2 4 4 4 3 3 4 ... #>  $ A3_1 : num  2 3 1 3 2 2 2 2 2 2 ... #>  $ A3_2 : num  3 3 1 3 3 3 1 2 1 2 ... #>  $ A3_3 : num  3 3 2 4 3 3 2 3 2 2 ... #>  $ A3_4 : num  4 3 2 4 3 3 2 3 2 2 ... #>  $ A3_5 : num  4 4 2 4 4 4 3 3 3 3 ... #>  $ A3_6 : num  2 4 3 4 3 3 2 4 3 2 ... #>  $ Int_1: num  4 4 1 4 2 1 6 7 1 2 ... ## eigenvalues of dataframe correlations Cor_Correlated_Scales <- cor(my_correlated_scales) eigenvalues(cormatrix = Cor_Correlated_Scales, scree = TRUE) |> round(2) #> Cor_Correlated_Scales  is positive-definite #>  [1] 6.94 2.25 1.14 1.00 0.76 0.69 0.65 0.54 0.47 0.37 0.35 0.23 0.22 0.18 0.12 #> [16] 0.09 #### Eigenvalues of predictor variable items only Cor_Attitude_items <- cor(my_correlated_scales[, -16]) eigenvalues(cormatrix = Cor_Attitude_items, scree = TRUE) |> round(2) #> Cor_Attitude_items  is positive-definite #>  [1] 6.78 2.22 1.00 0.93 0.73 0.65 0.57 0.54 0.38 0.35 0.25 0.22 0.18 0.12 0.10"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"helper-functions","dir":"Articles","previous_headings":"Using LikertMakeR","what":"Helper functions","title":"LikertMakeR vignette","text":"likertMakeR() includes two additional functions may help examining parameters output. alpha() calculates Cronbach’s Alpha given correlation matrix given dataframe eigenvalues() calculates eigenvalues correlation matrix, report whether correlation matrix positive definite, produces optional scree plot. reliability presents table internal consistency statistics","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"alpha","dir":"Articles","previous_headings":"Using LikertMakeR > Helper functions","what":"alpha()","title":"LikertMakeR vignette","text":"alpha() accepts, input, either correlation matrix dataframe. submitted, correlation matrix used default, message effect.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"alpha-examples","dir":"Articles","previous_headings":"Using LikertMakeR > Helper functions","what":"alpha() examples","title":"LikertMakeR vignette","text":"","code":"## define parameters df <- data.frame(   V1 = c(4, 2, 4, 3, 2, 2, 2, 1),   V2 = c(3, 1, 3, 4, 4, 3, 2, 3),   V3 = c(4, 1, 3, 5, 4, 1, 4, 2),   V4 = c(4, 3, 4, 5, 3, 3, 3, 3) )  corMat <- matrix(   c(     1.00, 0.35, 0.45, 0.75,     0.35, 1.00, 0.65, 0.55,     0.45, 0.65, 1.00, 0.65,     0.75, 0.55, 0.65, 1.00   ),   nrow = 4, ncol = 4 )  ## apply function examples alpha(cormatrix = corMat) #> [1] 0.8395062 alpha(data = df) #> [1] 0.8026658 alpha(NULL, df) #> [1] 0.8026658 alpha(corMat, df) #> Alert:  #> Both cormatrix and data present. #>                  #> Using cormatrix by default. #> [1] 0.8395062"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"eigenvalues","dir":"Articles","previous_headings":"Using LikertMakeR > Helper functions","what":"eigenvalues()","title":"LikertMakeR vignette","text":"eigenvalues() calculates eigenvalues correlation matrix, reports whether matrix positive-definite, optionally produces scree plot.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"eigenvalues-examples","dir":"Articles","previous_headings":"Using LikertMakeR > Helper functions","what":"eigenvalues() examples","title":"LikertMakeR vignette","text":"","code":"## define parameters correlationMatrix <- matrix(   c(     1.00, 0.25, 0.35, 0.45,     0.25, 1.00, 0.70, 0.75,     0.35, 0.70, 1.00, 0.85,     0.45, 0.75, 0.85, 1.00   ),   nrow = 4, ncol = 4 )  ## apply function evals <- eigenvalues(cormatrix = correlationMatrix) #> correlationMatrix  is positive-definite  print(evals) #> [1] 2.7484991 0.8122627 0.3048151 0.1344231"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"eigenvalues-function-with-optional-scree-plot","dir":"Articles","previous_headings":"","what":"LikertMakeR vignette","title":"LikertMakeR vignette","text":"","code":"evals <- eigenvalues(correlationMatrix, 1) #> correlationMatrix  is positive-definite print(evals) #> [1] 2.7484991 0.8122627 0.3048151 0.1344231"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"reliability","dir":"Articles","previous_headings":"Using LikertMakeR > Helper functions","what":"reliability()","title":"LikertMakeR vignette","text":"reliabiity() Computes internal consistency reliability estimates single-factor scale, including Cronbach’s alpha, McDonald’s omega (total), optional ordinal (polychoric-based) variants Confidence intervals.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"reliability-examples","dir":"Articles","previous_headings":"Using LikertMakeR > Helper functions","what":"reliability() examples","title":"LikertMakeR vignette","text":"","code":"## create dataset my_cor <- LikertMakeR::makeCorrAlpha(   items = 4,   alpha = 0.80 ) #> reached max iterations (1600) - best mean difference: 8.8e-05  my_data <- LikertMakeR::makeScales(   n = 64,   means = c(2.75, 3.00, 3.25, 3.50),   sds = c(1.25, 1.50, 1.30, 1.25),   lowerbound = rep(1, 4),   upperbound = rep(5, 4),   cormatrix = my_cor ) #> Variable  1 :  item01  - #> Variable  2 :  item02  - #> Variable  3 :  item03  - #> Variable  4 :  item04  - #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables  ## run function reliability(my_data) #>    coef_name estimate n_items n_obs                notes #>        alpha    0.800       4    64 Pearson correlations #>  omega_total    0.871       4    64 1-factor eigen omega  reliability(   my_data,   include = c(\"lambda6\", \"polychoric\") ) #>            coef_name estimate n_items n_obs #>                alpha    0.800       4    64 #>          omega_total    0.871       4    64 #>              lambda6    0.797       4    64 #>        ordinal_alpha    0.767       4    64 #>  ordinal_omega_total    0.852       4    64 #>                                                notes #>                                 Pearson correlations #>                                 1-factor eigen omega #>                                       psych::alpha() #>                              Polychoric correlations #>  Polychoric correlations | Ordinal CIs not requested   ## bootstrapped Confidence intervals can be slow! reliability(   my_data,   include = \"polychoric\",   ci = TRUE,   n_boot = 64 ) #>            coef_name estimate ci_lower ci_upper n_items n_obs #>                alpha    0.800    0.698    0.868       4    64 #>          omega_total    0.871    0.818    0.910       4    64 #>        ordinal_alpha    0.767    0.635    0.824       4    64 #>  ordinal_omega_total    0.852    0.786    0.884       4    64 #>                                                notes #>                                 Pearson correlations #>                                 1-factor eigen omega #>                              Polychoric correlations #>  Polychoric correlations | Ordinal CIs via bootstrap"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"alternative-methods-packages","dir":"Articles","previous_headings":"","what":"Alternative methods & packages","title":"LikertMakeR vignette","text":"LikertMakeR intended synthesising & correlating rating-scale data means, standard deviations, correlations close possible predefined parameters. don’t need data close exact, options may faster flexible. Different approaches include: sampling truncated normal distribution sampling predetermined probability distribution marginal model specification","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"sampling-from-a-truncated-normal-distribution","dir":"Articles","previous_headings":"Alternative methods & packages","what":"sampling from a truncated normal distribution","title":"LikertMakeR vignette","text":"Data sampled normal distribution, truncated suit rating-scale boundaries, rounded set discrete values see rating scales. See Heinz (2021) excellent short example using following packages: truncnorm faux See also rLikert() function excellent latent2likert package, Lalovic (2024), approach using optimal discretization skew-normal distribution. latent2likert() converts continuous latent variables ordinal categories generate Likert scale item responses.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"sampling-with-a-predetermined-probability-distribution","dir":"Articles","previous_headings":"Alternative methods & packages","what":"sampling with a predetermined probability distribution","title":"LikertMakeR vignette","text":"following code generate vector values approximately given probabilities. Good simulating single item.","code":"n <- 128 sample(1:5, n,   replace = TRUE,   prob = c(0.1, 0.2, 0.4, 0.2, 0.1) )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"marginal-model-specification","dir":"Articles","previous_headings":"Alternative methods & packages","what":"marginal model specification","title":"LikertMakeR vignette","text":"Marginal model specification extends idea predefined probability distribution multivariate correlated dataframes. SimCorrMix: Simulation Correlated Data Multiple Variable Types Including Continuous Count Mixture Distributions CRAN. SimMultiCorrData: Simulation Correlated Data Multiple Variable Types CRAN. lsasim: Functions Facilitate Simulation Large Scale Assessment Data CRAN. See Matta et al. (2018) GenOrd:Simulation Discrete Random Variables Given Correlation Matrix Marginal Distributions CRAN. SimCorMultRes: Simulates Correlated Multinomial Responses CRAN. See Touloumis (2016) covsim: VITA, IG PLSIM Simulation Given Covariance Marginals CRAN. See Grønneberg et al. (2022)","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"factor-models-classical-test-theory-ctt","dir":"Articles","previous_headings":"Alternative methods & packages","what":"Factor Models: Classical Test Theory (CTT)","title":"LikertMakeR vignette","text":"psych package several excellent functions simulating rating-scale data based factor loadings.  focus factor item correlations rather item moments.  Highly recommended. psych::sim.item Generate simulated data structures circumplex, spherical, simple structure psych::sim.congeneric Simulate congeneric data set without minor factors See Revelle (prep) Also: simsem many functions simulating testing data application Structural Equation modelling. See examples https://simsem.org/","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"general-data-simulation","dir":"Articles","previous_headings":"Alternative methods & packages","what":"General data simulation","title":"LikertMakeR vignette","text":"simpr provides general, simple, tidyverse-friendly framework generating simulated data, fitting models simulations, tidying model results.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/LikertMakeR_vignette.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"LikertMakeR vignette","text":"D’Alessandro, S., H. Winzar, B. Lowe, B.J. Babin, W. Zikmund (2020). Marketing Research 5ed, Cengage Australia. https://cengage.com.au/sem121/marketing-research-5th-edition-dalessandro-babin-zikmund Grønneberg, S., Foldnes, N., & Marcoulides, K. M. (2022). covsim: R Package Simulating Non-Normal Data Structural Equation Models Using Copulas. Journal Statistical Software, 102(1), 1–45. doi:10.18637/jss.v102.i03 Heinz, . (2021), Simulating Correlated Likert-Scale Data R: 3 Simple Steps (blog post) https://glaswasser.github.io/simulating-correlated-likert-scale-data/ Lalovic M (2024). latent2likert: Converting Latent Variables Likert Scale Responses. R package version 1.2.2, https://latent2likert.lalovic.io/. Matta, T.H., Rutkowski, L., Rutkowski, D. & Liaw, Y.L. (2018), lsasim: R package simulating large-scale assessment data. Large-scale Assessments Education 6, 15. doi:10.1186/s40536-018-0068-8 Pornprasertmanit, S., Miller, P., & Schoemann, . (2021). simsem: R package simulated structural equation modeling https://simsem.org/ Revelle, W. (prep) introduction psychometric theory applications R. Springer. (working draft available https://personality-project.org/r/book/ ) Touloumis, . (2016), Simulating Correlated Binary Multinomial Responses Marginal Model Specification: SimCorMultRes Package, R Journal 8:2, 79-91. https://doi.org/10.32614/RJ-2016-034 Winzar, H. (2025). LikertMakeR (V 1.4.0): Synthesise correlate Likert scale related rating-scale data predefined first second moments. CRAN: https://CRAN.R-project.org/package=LikertMakeR","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"sec-likertmaker","dir":"Articles","previous_headings":"","what":"LikertMakeR Validation","title":"LikertMakeR Scale Reproduction Validation","text":"paper reports study compares data produced using LikertMakeR original data published publicly-available source.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"abstract","dir":"Articles","previous_headings":"","what":"Abstract","title":"LikertMakeR Scale Reproduction Validation","text":"LikertMakeR::lfast() function generally produces surprisingly good replications existing data. Data distributions usually unimodal, multimodal (wiggly) data poorly represented. Highly leptokurtic data (pointy wide tails) also may poorly represented. exceptions likely occur one utility function included original sample data. , different groups respondents joined together.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"sec-validation-against-real-data","dir":"Articles","previous_headings":"","what":"Validation against real data","title":"LikertMakeR Scale Reproduction Validation","text":"One objective LikertMakeR package (winzar2022?) “reproduce” “reverse engineer” rating-scale data analysis visualization summary statistics available. role, synthetic data accurately represent original data, meaning plausibly originate population. validate synthetic data, choose data set readily available, can filtered represent rating-scale data may commonly seen published reports. compare data variations : sample sizes, number items scale length scale item (1 5; 0 10; etc.) modality: extent distribution shows bumps something smooth hill.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"sec-spi","dir":"Articles","previous_headings":"Validation against real data","what":"SPI (SAPA Personality Inventory)","title":"LikertMakeR Scale Reproduction Validation","text":"convenience reproducibility, chose subsample SAPA Personality Inventory (Condon 2023) data available psych (Revelle 2024) psychtools (William Revelle 2024) packages R. data set holds 4000 observations 145 variables.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"variable_selection","dir":"Articles","previous_headings":"Validation against real data > SPI (SAPA Personality Inventory)","what":"Variable/ scale selection","title":"LikertMakeR Scale Reproduction Validation","text":"SPI based hierarchical framework assessing personality two levels. higher level familiar “Big Five” factors studied personality research since 1980s. SPI, five dimensions represented average fourteen 6-point agree-disagree items. , scale values scale 14 * 6 = 84 possible values.  lower level 27 factors, made averaging five 6-point items, sub-scales Big Five. give scales 5 * 6 = 30 possible values. List SPI Facets Finally, dimensions facets made averaging subsets 135 items (individual questions). item scale 1 * 6 = 6 possible values. Scale properties consideration","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"sec-measures-of-difference","dir":"Articles","previous_headings":"Validation against real data","what":"Measures of Difference","title":"LikertMakeR Scale Reproduction Validation","text":"function LikertMakeR::lfast() produces vector values predefined first second moments usually correct two decimal places. Vectors also exact minima & maxima, scale intervals. determine whether synthetic data different data produced original summary statistics, need something just equal mean standard deviation. need measures can accommodate third fourth moments (skewness kurtosis) well. , comparison accommodate occasional bimodal distributions may occur.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"sec-choice_of_test","dir":"Articles","previous_headings":"Validation against real data > Measures of Difference","what":"Choice of Test for Equal Distributions","title":"LikertMakeR Scale Reproduction Validation","text":"assess similarity synthetic data generated LikertMakeR real survey data, evaluate agreement empirical distributions using nonparametric two-sample tests. Several tests available comparing continuous distributions: Kolmogorov–Smirnov (KS) test focuses maximum vertical distance empirical cumulative distribution functions (ECDFs) two samples. KS test reduced sensitivity near centre distribution excessive sensitivity extreme values (Lilliefors 1967). Baumgartner–Weiß–Schindler (BWS) test (Baumgartner, Weiß, Schindler 1998) improves upon KS test incorporating differences across entire distribution, using rank-based test statistic derived integrated spacing differences. BWS test powerful either Kolmogorov-Smirnov test Wilcoxon test (Pav 2023), shown Baumgartner, Weiß, Schindler (1998). sensitive location shape differences generally greater power across variety alternatives (Neuhäuser 2001; Neuhäuser Ruxton 2009). Neuhäuser modification BWS test introduces weighting function emphasizes differences central region distribution reducing influence tails (Neuhäuser 2001). makes robust small discrepancies extremes — desirable property large samples minor tail mismatches can lead false positives (Neuhäuser 2005). Overall, researcher might want use BWS test see LikertMakeR::lfast() gives exact replication scale, use Neuhäuser test option see function produces “pretty good” dataframe. , present summary results tests study. Comparison BWS Neuhäuser Methods","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"research_design","dir":"Articles","previous_headings":"","what":"Research Design","title":"LikertMakeR Scale Reproduction Validation","text":"suspect accuracy data created LikertMakeR::lfast() affected : shape true distribution sample-size number discrete intervals scale , highly skewed multimodal distributions, smaller sample sizes, likely less well replicated synthetic data generated LikertMakeR::lfast().","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"sec-data_selection","dir":"Articles","previous_headings":"Research Design","what":"Data selection","title":"LikertMakeR Scale Reproduction Validation","text":"SPI dataset includes demographic information can filter 4000 observations sample-sizes likely find normal social research. Somewhat arbitrarily, decided use Age, Gender, Education filters.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"small_sample","dir":"Articles","previous_headings":"Research Design > Data selection","what":"Small sample","title":"LikertMakeR Scale Reproduction Validation","text":"Young highly-educated men filtering produced sample 19 observations.","code":"young_highly_educated_men <- spi |>   filter(age < 24 & sex == 1 & education == 7)  ## where, sex==1 = 'male' ##        education == 7 = 'postgraduate degree'"},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"medium_sample","dir":"Articles","previous_headings":"Research Design > Data selection","what":"Medium sample","title":"LikertMakeR Scale Reproduction Validation","text":"Young educated women filtering produced sample 99 observations.","code":"young_educated_women <- spi |>   filter(age < 24 & sex == 2 & education >= 5)  ## where, \"sex==2\" = 'female' ##        \"education >= 5\" = 'undergraduate degree or higher'"},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"large_sample","dir":"Articles","previous_headings":"Research Design > Data selection","what":"Large sample","title":"LikertMakeR Scale Reproduction Validation","text":"Young school-leavers filtering produced sample 314 observations.","code":"under_18_highschool <- spi |>   filter(age < 18 & education == 1)  ## where, \"education == 1\" = 'Less than 12 years schooling'"},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"procedure","dir":"Articles","previous_headings":"Research Design","what":"Procedure","title":"LikertMakeR Scale Reproduction Validation","text":"three samples small, medium large sample-sizes. small, 19 observations medium, 99 observations large, 314 observations three levels data aggregation: 5 dimensions, 14 items 27 factors, 5 items 135 individual items gives us (3 * (5 + 27 + 135) = 501) data subsets. combination sample data-level find mean standard deviation data subset, apply LikertMakeR::lfast function produce 2^10 = 1024 simulated dataframes compare true original dataframes SPI data. compare Empirical Cumulative Density Function (ECDF) simulated dataframe ECDF original dataframe using BWS Neuhäuser methods. 1000 tests 501 original dataframes able see accurate simulations .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"original-data","dir":"Articles","previous_headings":"Research Design","what":"Original Data","title":"LikertMakeR Scale Reproduction Validation","text":"present charts summary information three levels data consideration. bar-charts measurement level combined kernel density estimates.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"spi-big-five-dimensions","dir":"Articles","previous_headings":"Research Design > Original Data","what":"SPI Big Five Dimensions","title":"LikertMakeR Scale Reproduction Validation","text":"Big Five measures average 14 six-point items. 14 * 6 = 84 potential values scale. Note , small sample size much sparse values larger sample. Otherwise, distributions tend unimodal, fairly smooth kernel density curves. Big 5 Dimensions three samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"spi-27-facets","dir":"Articles","previous_headings":"Research Design > Original Data","what":"SPI 27 Facets","title":"LikertMakeR Scale Reproduction Validation","text":"SPI facets average five six-point items, giving 5 * 6 = 30 potential values facet measure. , distributions tend unimodal, smooth kernel density curves. SPI facets three samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"selected-spi-items","dir":"Articles","previous_headings":"Research Design > Original Data","what":"Selected SPI items","title":"LikertMakeR Scale Reproduction Validation","text":"simulated data 135 individual items - many show meaningfully. present sample showed unusual results simulations. cases data either highly skewed, bimodal (least flat parts). Selected SPI items three samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"LikertMakeR Scale Reproduction Validation","text":"following tables list dimension consideration proportion cases three samples ere ‘statistically significant’ (ρ\\rho < 0.05). Tables show BWS test / Neuhäuser test.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"big-five-dimensions-validity","dir":"Articles","previous_headings":"Results","what":"Big Five Dimensions validity","title":"LikertMakeR Scale Reproduction Validation","text":"Fourteen six-point items (84 levels scale) Proportion statistically-significant simulations (BWS/Neuhäuser) smooth kernel density estimates, saw , cases non-significant, suggesting data well-reproduced LikertMakeR::lfast() function.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"spi-facets-subscale-validity","dir":"Articles","previous_headings":"Results","what":"SPI Facets (Subscale) validity","title":"LikertMakeR Scale Reproduction Validation","text":"Five six-point items (30 potential values scale) Proportion statistically-significant simulations (BWS/Neuhäuser) BWS test applied larger sample, simulations significantly different original data. probably due smaller standard error produced larger sample. Interestingly, Neuhäuser test, less sensitive outliers, suggested simulations good representations original. facet, Introspection, stands one rarely accurately reproduced LikertMakeR::lfast() function, using BWS test, regardless sample size. facets worth exploring detail : Compassion, Humor, Intellect, SelfControl, EasyGoingness, Perfectionism. facets high rates significance mid-sample-size condition.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"focus-on-introspection-facet","dir":"Articles","previous_headings":"Results > SPI Facets (Subscale) validity","what":"Focus on “Introspection” facet","title":"LikertMakeR Scale Reproduction Validation","text":"following chart shows kernel density plots facet Introspection three samples. 1024 synthetic dataframes represented grey/black line, original “true” dataframe represented blue/cyan line. Introspection facet: Density plot small, medium large samples see synthetic data never match true data, especially middle large sample sizes. original, true, data highly left-skewed, nicely captured synthetic data. Note, however, true dataframe unimodal. kernel density estimate appears rough slightly multimodal. ’s wobbly.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"focus-on-compassion-facet","dir":"Articles","previous_headings":"Results > SPI Facets (Subscale) validity","what":"focus on “Compassion” facet","title":"LikertMakeR Scale Reproduction Validation","text":"following chart shows kernel density plots facet Compassion three samples. , 1024 synthetic dataframes represented grey/black line, original “true” dataframe represented blue/cyan line. Compassion facet: Kernel Density plots small, medium large samples factor BWS test showed cases synthetic data significantly different original data smaller sample. medium large samples, original data unimodal, BWS test suggests synthetic replications different original.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"humor","dir":"Articles","previous_headings":"","what":"LikertMakeR Scale Reproduction Validation","title":"LikertMakeR Scale Reproduction Validation","text":"Humor facet: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"intellect","dir":"Articles","previous_headings":"","what":"LikertMakeR Scale Reproduction Validation","title":"LikertMakeR Scale Reproduction Validation","text":"Intellect facet: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"selfcontrol","dir":"Articles","previous_headings":"","what":"LikertMakeR Scale Reproduction Validation","title":"LikertMakeR Scale Reproduction Validation","text":"SelfControl facet: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"easygoingness","dir":"Articles","previous_headings":"","what":"LikertMakeR Scale Reproduction Validation","title":"LikertMakeR Scale Reproduction Validation","text":"EasyGoingness facet: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"perfectionism","dir":"Articles","previous_headings":"","what":"LikertMakeR Scale Reproduction Validation","title":"LikertMakeR Scale Reproduction Validation","text":"Perfectionism facet: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"spi-item-validity","dir":"Articles","previous_headings":"Results","what":"SPI Item validity","title":"LikertMakeR Scale Reproduction Validation","text":"scale single six-point item. almost cases, Baumgartner–Weiß–Schindler (BWS) test showed statistically significant difference actual data synthetic data. cases, however, Neuhäuser test, robust outliers, statistically significant. appendix table shows summary results three data sets 135 items, indicating proportion cases distribution comparison tests ‘statistically significant’ (ρ\\rho < 0.05). 37 135 items (27%) BWS test show proportion significant simulations less 80%. following sample item results three samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"original-data-histograms","dir":"Articles","previous_headings":"Results","what":"Original data histograms","title":"LikertMakeR Scale Reproduction Validation","text":"Selected items Selected items Selected items","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"q_1685-seldom-joke-around","dir":"Articles","previous_headings":"Results > Synthetic data density plots","what":"q_1685 ‘Seldom joke around’","title":"LikertMakeR Scale Reproduction Validation","text":"q_1685 ‘Seldom joke around’: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"q_1896-use-others-for-my-own-ends","dir":"Articles","previous_headings":"Results > Synthetic data density plots","what":"q_1896 ‘Use others for my own ends’","title":"LikertMakeR Scale Reproduction Validation","text":"q_1058 ‘Use others ends’: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"q_1989-worry-about-things","dir":"Articles","previous_headings":"Results > Synthetic data density plots","what":"q_1989 ‘Worry about things’","title":"LikertMakeR Scale Reproduction Validation","text":"q_1989 ‘Worry things’: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"q_755-enjoy-examining-myself-and-my-life","dir":"Articles","previous_headings":"Results > Synthetic data density plots","what":"q_755 ‘Enjoy examining myself and my life’","title":"LikertMakeR Scale Reproduction Validation","text":"q_755 ‘Enjoy examining life’: Kernel Density plots small, medium large samples","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"summary_results","dir":"Articles","previous_headings":"","what":"Summary Results","title":"LikertMakeR Scale Reproduction Validation","text":"Results much might expect:","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"unimodal-data-are-good-multimodal-less-so","dir":"Articles","previous_headings":"Summary Results","what":"Unimodal data are good, multimodal less so","title":"LikertMakeR Scale Reproduction Validation","text":"Multimodal data well-represented LikertMakeR::lfast() function, consistently generates unimodal data.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"larger-sample-sizes-produce-smoother-unimodal-data-distributions","dir":"Articles","previous_headings":"Summary Results","what":"Larger sample sizes produce smoother unimodal data distributions","title":"LikertMakeR Scale Reproduction Validation","text":"Original data small sample size 19-subjects group frequently multimodal, whereas original data medium-sized large-sized groups often unimodal.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"sample-size-alone-does-not-affect-accuracy-of-data-synthisis-","dir":"Articles","previous_headings":"Summary Results","what":"Sample size alone does not affect accuracy of data synthisis.","title":"LikertMakeR Scale Reproduction Validation","text":"seem much difference results different sample sizes.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"third-and-fourth-moments-can-affect-accurcy","dir":"Articles","previous_headings":"Summary Results","what":"Third and Fourth moments can affect accurcy","title":"LikertMakeR Scale Reproduction Validation","text":"Leptokurtic (pointy) distributions platykurtic (flatter) distributions seem affect results. shouldn’t surprised since generating algorithm focuses first (mean) second (standard deviation) moments.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/lfast_validation.html","id":"summary-results-for-individual-items","dir":"Articles","previous_headings":"Appendix","what":"Summary results for individual items","title":"LikertMakeR Scale Reproduction Validation","text":"Rating scale items ranging ‘1’ ‘6’ SPI data set, three samples different sizes. Proportion statistically-significant simulations (BWS/Neuhauser)","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"abstract","dir":"Articles","previous_headings":"","what":"Abstract","title":"makeCorrLoadings() validation","text":"makeCorrLoadings() function accurately reconstructs item correlation matrix item-factor loadings factor correlations, similar produced EFA SEM. function robust even without explicit uniqueness factor correlation inputs, although loadings greater 0.05 needed best results.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"the-makecorrloadings-function","dir":"Articles","previous_headings":"","what":"The makeCorrLoadings() function","title":"makeCorrLoadings() validation","text":"makeCorrLoadings() function generates correlation matrix factor loadings factor correlations might published results Exploratory Factor Analysis (EFA) Structural Equation Model (SEM). resulting correlation matrix can applied makeItems() function generate synthetic data set rating-scale items closely resemble original data created factor loadings summary table. paper tests well makeCorrLoadings() function achieves goal.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"study-design","dir":"Articles","previous_headings":"","what":"Study design","title":"makeCorrLoadings() validation","text":"valid makeCorrLoadings() function able produce correlation matrix identical original correlation matrix. , subsequent treatment makeItems() produce dataframe appears come population original sample. need original, True, dataframe test function. Preferably, several different original dataframes ensure results generalisable.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"original-data","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Original data","title":"makeCorrLoadings() validation","text":"use pp15 dataset - subset Party Panel 2015 dataset. dataset available rosetta package (Peters Verboon 2023) longer available CRAN, time writing, still accessible Rosetta Stats book site Party Panel annual semi-panel study among Dutch nightlife patrons, every year, determinants another nightlife-related risk behaviour mapped. 2015, determinants measured behaviours related using highly dosed ecstasy pills. Nine items relevant study. 7-point likert-style question scored range -3 +3. questions item labels presented follows:","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"extract-and-clean-the-original-data-file","dir":"Articles","previous_headings":"Study #1: Party Panel 2015 > Original data","what":"Extract and clean the original data file","title":"makeCorrLoadings() validation","text":"","code":"## variable names item_list <- c(   \"highDose_AttBeliefs_long\",   \"highDose_AttBeliefs_intensity\",   \"highDose_AttBeliefs_intoxicated\",   \"highDose_AttBeliefs_energy\",   \"highDose_AttBeliefs_euphoria\",   \"highDose_AttBeliefs_insight\",   \"highDose_AttBeliefs_connection\",   \"highDose_AttBeliefs_contact\",   \"highDose_AttBeliefs_sex\" )  ## read the data/ select desired variables/ remove obs with missing values dat <- read.csv2(file = \"data/pp15.csv\") |>   select(all_of(item_list)) |>   na.omit()  ## give variables shorter names names(dat) <- itemLabels  sampleSize <- nrow(dat)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"target-correlation-matrix","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Target correlation matrix","title":"makeCorrLoadings() validation","text":"correlations among nine items reproducible makeCorrLoadings() function.","code":"## correlation matrix pp15_cor <- cor(dat)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-cases","dir":"Articles","previous_headings":"Study #1: Party Panel 2015 > Target correlation matrix","what":"Test cases","title":"makeCorrLoadings() validation","text":"shall produce following options factor correlation reporting: Full information: factor loadings factor correlations five decimal places, plus uniquenesses. Full information - uniquenesses: factor loadings factor correlations 5 decimal places without uniquenesses. Rounded loadings: factor loadings factor correlations two decimal places, plus uniquenesses. Rounded loadings - uniquenesses: factor loadings factor correlations two decimal places without uniquenesses. Censored loadings: factor loadings two decimal places, loadings less arbitrary value removed clarity presentation, uniquenesses. Censored loadings - uniqueness: factor loadings two decimal places, loadings less arbitrary value removed clarity, without uniquenesses. Censored loadings, uniqueness, factor correlations: factor loadings two decimal places, loadings less arbitrary value removed clarity, uniquenesses factor correlations. Functionally, equivalent claiming orthogonal factors even factor loadings non-orthogonal rotation.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"evaluation","dir":"Articles","previous_headings":"Study #1: Party Panel 2015 > Target correlation matrix","what":"Evaluation","title":"makeCorrLoadings() validation","text":"compare True correlation matrix Synthetic matrix, employ cortest.jennrich() function psych package (Revelle 2024). Chi-square test whether pair matrices equal (Jennrich 1970). report raw χ2\\chi^2 statistic corresponding p-value test.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"exploratory-factor-analysis","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Exploratory Factor Analysis","title":"makeCorrLoadings() validation","text":"pretesting suggests two factors appropriate sample. ’re confident factors correlated, use promax rotation.","code":"## factor analysis from `psych` package rfaDose <- psych::fa(   r = dat,   nfactors = 2,   rotate = \"promax\" ) #> Loading required namespace: GPArotation  factorLoadings <- rfaDose$loadings[1:nrow(rfaDose$loadings), 1:ncol(rfaDose$loadings)] factorNames <- paste(\"Factor\", \"_\", seq_along(factorLoadings[1, ]), sep = \"\") colnames(factorLoadings) <- factorNames udf <- rfaDose$uniquenesses |> as.data.frame() colnames(udf) <- \"Uniqueness\" factorLoadings <- cbind(factorLoadings, udf)  factorCorrs <- rfaDose$Phi colnames(factorCorrs) <- rownames(factorCorrs) <- factorNames"},{"path":[]},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-1-full-information","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Test Case #1: Full information","title":"makeCorrLoadings() validation","text":"","code":"## round input values to 5 decimal places # factor loadings fl1 <- factorLoadings[, 1:2] |>   round(5) |>   as.matrix() # item uniquenesses un1 <- factorLoadings[, 3] |> round(5) # factor correlations fc1 <- round(factorCorrs, 5) |> as.matrix() # run makeCorrLoadings() function itemCors_1 <- makeCorrLoadings(   loadings = fl1,   factorCor = fc1,   uniquenesses = un1 ) ## Compare the two matrices chiSq_1 <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_1,   n1 = sampleSize, n2 = sampleSize )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-2-full-information---no-uniquenesses","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Test case #2: Full information - No uniquenesses","title":"makeCorrLoadings() validation","text":"factor loadings factor correlations 5 decimal places without uniquenesses","code":"## round input values to 2 decimal places # factor loadings fl2 <- factorLoadings[, 1:2] |>   round(5) |>   as.matrix() # factor correlations fc2 <- factorCorrs |>   round(5) |>   as.matrix() itemCors_2 <- makeCorrLoadings(   loadings = fl2,   factorCor = fc2,   uniquenesses = NULL ) ## Compare the two matrices chiSq_2 <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_2,   n1 = sampleSize, n2 = sampleSize )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-3-rounded-loadings","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Test case #3: Rounded loadings","title":"makeCorrLoadings() validation","text":"factor loadings factor correlations two decimal places.","code":"## round input values to 2 decimal places # factor loadings fl3 <- factorLoadings[, 1:2] |>   round(2) |>   as.matrix() # item uniquenesses un3 <- factorLoadings[, 3] |>   round(2) ## factor correlations fc3 <- factorCorrs |>   round(2) |>   as.matrix() ## Compare the two matrices itemCors_3 <- makeCorrLoadings(   loadings = fl3,   factorCor = fc3,   uniquenesses = un3 ) ## Compare the two matrices chiSq_3 <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_3,   n1 = sampleSize, n2 = sampleSize )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-4-rounded-loadings---no-uniquenesses","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Test case #4: Rounded loadings - No uniquenesses","title":"makeCorrLoadings() validation","text":"Factor loadings factor correlations two decimal places, uniquenesses","code":"## round input values to 2 decimal places # factor loadings fl4 <- factorLoadings[, 1:2] |>   round(2) |>   as.matrix() ## factor correlations fc4 <- factorCorrs |>   round(2) |>   as.matrix() # apply the function itemCors_4 <- makeCorrLoadings(   loadings = fl4,   factorCor = fc4,   uniquenesses = NULL ) ## Compare the two matrices chiSq_4 <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_4,   n1 = sampleSize, n2 = sampleSize )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"censored-loadings","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Censored loadings","title":"makeCorrLoadings() validation","text":"Often, item-factor loadings presented lower values removed ease reading. ’m calling “Censored” loadings. usually acceptable, purpose show reader larger loadings . missing information may affect results reverse-engineering ’re employing makeCorrLoadings() function. study, set level hidden loadings values less ‘0.1’, ‘0.2’ ‘0.3’.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-5-censored-loadings","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Test case #5: Censored loadings","title":"makeCorrLoadings() validation","text":"Factor loadings less ‘0.1’, ‘0.2’, ‘0.3’ removed clarity, presented two decimal places.","code":"## round input values to 2 decimal places # factor loadings fl5a <- factorLoadings[, 1:2] |>   round(2) # convert factor loadings < '0.1' to '0' fl5a[abs(fl5a) < 0.1] <- 0 fl5a <- as.matrix(fl5a) # item uniquenesses un5 <- factorLoadings[, 3] |>   round(2) # factor correlations fc5 <- factorCorrs |>   round(2) |>   as.matrix() # apply the function itemCors_5a <- makeCorrLoadings(   loadings = fl5a,   factorCor = fc5,   uniquenesses = un5 ) ## Compare the two matrices chiSq_5a <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_5a,   n1 = sampleSize, n2 = sampleSize )  # factor loadings fl5b <- factorLoadings[, 1:2] |>   round(2) # convert factor loadings < '0.2' to '0' fl5b[abs(fl5b) < 0.2] <- 0 fl5b <- as.matrix(fl5b) # apply the function itemCors_5b <- makeCorrLoadings(   loadings = fl5b,   factorCor = fc5,   uniquenesses = un5 ) ## Compare the two matrices chiSq_5b <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_5b,   n1 = sampleSize, n2 = sampleSize )  # factor loadings fl5c <- factorLoadings[, 1:2] |>   round(2) # convert factor loadings < '0.2' to '0' fl5c[abs(fl5c) < 0.3] <- 0 fl5c <- as.matrix(fl5c) # apply the function itemCors_5c <- makeCorrLoadings(   loadings = fl5c,   factorCor = fc5,   uniquenesses = un5 ) ## Compare the two matrices chiSq_5c <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_5c,   n1 = sampleSize, n2 = sampleSize ) # kable(itemCors_5, digits = 2)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-6-censored-loadings---no-uniquenesses","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Test case #6: Censored loadings - no uniquenesses","title":"makeCorrLoadings() validation","text":"declared uniquenesses, inferred estimated communalities.","code":"itemCors_6a <- makeCorrLoadings(   loadings = fl5a,   factorCor = fc5,   uniquenesses = NULL ) ## Compare the two matrices chiSq_6a <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_6a,   n1 = sampleSize, n2 = sampleSize ) # apply the function itemCors_6b <- makeCorrLoadings(   loadings = fl5b,   factorCor = fc5,   uniquenesses = NULL ) ## Compare the two matrices chiSq_6b <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_6b,   n1 = sampleSize, n2 = sampleSize ) # apply the function itemCors_6c <- makeCorrLoadings(   loadings = fl5c,   factorCor = fc5,   uniquenesses = NULL ) ## Compare the two matrices chiSq_6c <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_6c,   n1 = sampleSize, n2 = sampleSize )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-7-censored-loadings-no-uniqueness-no-factor-correlations","dir":"Articles","previous_headings":"Study #1: Party Panel 2015","what":"Test case #7 Censored loadings, no uniqueness, no factor correlations","title":"makeCorrLoadings() validation","text":"uniquenesses. , Uniquenesses estimated 1-communalities, communalities = sum(factor-loadings^2). factor correlations. , assume orthogonal factors.","code":"itemCors_7a <- makeCorrLoadings(   loadings = fl5a,   factorCor = NULL,   uniquenesses = NULL ) ## Compare the two matrices chiSq_7a <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_7a,   n1 = sampleSize, n2 = sampleSize ) # apply the function itemCors_7b <- makeCorrLoadings(   loadings = fl5b,   factorCor = NULL,   uniquenesses = NULL ) ## Compare the two matrices chiSq_7b <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_7b,   n1 = sampleSize, n2 = sampleSize ) # apply the function itemCors_7c <- makeCorrLoadings(   loadings = fl5c,   factorCor = NULL,   uniquenesses = NULL ) ## Compare the two matrices chiSq_7c <- cortest.jennrich(   R1 = pp15_cor, R2 = itemCors_7c,   n1 = sampleSize, n2 = sampleSize )"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"makeCorrLoadings() validation","text":"makeCorrLoadings function works quite well information factor loadings, less well “summary” (censored) factor loadings given.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"original-data-bfi-25-personality-items-representing-5-factors","dir":"Articles","previous_headings":"Study #2: Big Five (bfi personality items)","what":"Original data: bfi 25 Personality items representing 5 factors","title":"makeCorrLoadings() validation","text":"25 personality self report items taken International Personality Item Pool (ipip.ori.org) included part Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project. data available psychTools package (William Revelle 2024). 2800 subjects included demonstration set scale construction, factor analysis, Item Response Theory analysis. Three additional demographic variables (sex, education, age) also included. purposes study, confine women (sex==2) postgraduate qualifications (education==5), giving us sample size 229 subjects. dataset contains following 28 variables. (q numbers SAPA item numbers). A1 indifferent feelings others. (q_146) A2 Inquire others’ well-. (q_1162) A3 Know comfort others. (q_1206) A4 Love children. (q_1364) A5 Make people feel ease. (q_1419) C1 exacting work. (q_124) C2 Continue everything perfect. (q_530) C3 things according plan. (q_619) C4 things half-way manner. (q_626) C5 Waste time. (q_1949) E1 Don’t talk lot. (q_712) E2 Find difficult approach others. (q_901) E3 Know captivate people. (q_1205) E4 Make friends easily. (q_1410) E5 Take charge. (q_1768) N1 Get angry easily. (q_952) N2 Get irritated easily. (q_974) N3 frequent mood swings. (q_1099 N4 Often feel blue. (q_1479) N5 Panic easily. (q_1505) O1 full ideas. (q_128) O2 Avoid difficult reading material.(q_316) O3 Carry conversation higher level. (q_492) O4 Spend time reflecting things. (q_1738) O5 probe deeply subject. (q_1964) gender (Male=1, Female=2) education (1=HS, 2=finished_HS, 3=some_college, 4=college_graduate 5=graduate_degree) age age years","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"second-target-correlation-matrix","dir":"Articles","previous_headings":"Study #2: Big Five (bfi personality items)","what":"Second target correlation matrix","title":"makeCorrLoadings() validation","text":"correlations among 25 items reproducable makeCorrLoadings() function.","code":"## download data data(bfi) ## filter for highly-educated women bfi_short <- bfi |>   filter(education == 5 & gender == 2) |>   na.omit() ## keep just the 25 items bfi_short <- bfi_short[, 1:25] sampleSize <- nrow(bfi_short) ## derive correlation matrix bfi_cor <- cor(bfi_short)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-cases-and-evaluation","dir":"Articles","previous_headings":"Study #2: Big Five (bfi personality items) > Second target correlation matrix","what":"Test cases and Evaluation","title":"makeCorrLoadings() validation","text":"first study, shall produce following options factor correlation reporting: Full information Full information - uniquenesses Rounded loadings Rounded loadings - uniquenesses Censored loadings Censored loadings - uniqueness Censored loadings - uniqueness factor cors , first study, compare True correlation matrix Synthetic matrix, employing Jennrich test.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"exploratory-factor-analysis-1","dir":"Articles","previous_headings":"Study #2: Big Five (bfi personality items)","what":"Exploratory Factor Analysis","title":"makeCorrLoadings() validation","text":"Five correlated factors appropriate sample, use promax rotation.","code":"## factor analysis from `psych::fa()` function  fa_bfi <- psych::fa(   r = bfi_short,   nfactors = 5,   rotate = \"promax\" )  bfiLoadings <- fa_bfi$loadings[1:nrow(fa_bfi$loadings), 1:ncol(fa_bfi$loadings)] bfiFactorNames <- paste(\"Factor\", \"_\", seq_along(bfiLoadings[1, ]), sep = \"\") colnames(bfiLoadings) <- bfiFactorNames bfiUdf <- fa_bfi$uniquenesses |> as.data.frame() colnames(bfiUdf) <- \"Uniqueness\" bfiLoadings <- cbind(bfiLoadings, bfiUdf)  bfiCorrs <- fa_bfi$Phi colnames(bfiCorrs) <- rownames(bfiCorrs) <- bfiFactorNames"},{"path":[]},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"test-case-1-full-information-1","dir":"Articles","previous_headings":"Study #2: Big Five (bfi personality items)","what":"Test Case #1: Full information","title":"makeCorrLoadings() validation","text":"","code":"## round input values to 5 decimal places # factor loadings fl1 <- bfiLoadings[, 1:5] |>   round(5) |>   as.matrix() # item uniquenesses un1 <- bfiLoadings[, 6] |> round(5) # factor correlations fc1 <- round(bfiCorrs, 5) |> as.matrix() # run makeCorrLoadings() function itemCors_1 <- makeCorrLoadings(   loadings = fl1,   factorCor = fc1,   uniquenesses = un1 ) ## Compare the two matrices chiSq_1 <- cortest.jennrich(   R1 = bfi_cor, R2 = itemCors_1,   n1 = sampleSize, n2 = sampleSize )"},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"other-test-cases-as-for-the-first-study","dir":"Articles","previous_headings":"Study #2: Big Five (bfi personality items)","what":"Other test cases as for the first study","title":"makeCorrLoadings() validation","text":"remaining test cases first study last test case. Changes made number decimal places considered, presence uniquenesses, presence factor correlation matrix, level item-factor loading included.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"overall-results","dir":"Articles","previous_headings":"","what":"Overall Results","title":"makeCorrLoadings() validation","text":"studies show consistent results.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/makeCorrLoadings_validate.html","id":"conclusions","dir":"Articles","previous_headings":"Overall Results","what":"Conclusions","title":"makeCorrLoadings() validation","text":"makeCorrLoadings() function designed produce item correlation matrix based item-factor loadings factor correlations one might see results Exploratory Factor Analysis (EFA) Structural Equation Modelling (SEM). Results study suggest makeCorrLoadings() function surprisingly good job reproducing target correlation matrix item-factor loadings present. correlation matrix created makeCorrLoadings() seems robust even absence specified uniquenesses, even without factor correlations. valid reproduction correlation matrix complete item-factor loadings , worst, item-factor loadings greater 0.10.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"reliability-estimation-with-likertmakerreliability","dir":"Articles","previous_headings":"","what":"Reliability estimation with LikertMakeR::reliability()","title":"likertMakeR::reliability()","text":"reliability() function estimates range internal consistency reliability coefficients single-factor Likert rating-scale measures. designed work naturally synthetic data generated LikertMakeR, applies equally real survey data. Unlike many reliability functions, reliability(): presents multiple coefficients tidy table, provides bootstrap confidence intervals requested, supports ordinal (polychoric-based) reliability, includes explicit diagnostics explaining ordinal estimates feasible.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"when-should-you-use-reliability","dir":"Articles","previous_headings":"Reliability estimation with LikertMakeR::reliability()","what":"When should you use reliability()?","title":"likertMakeR::reliability()","text":"Use reliability() : scale intended measure one underlying construct, items Likert-type bounded rating scales, want transparent, reproducible reliability estimates, especially teaching, simulation, methods work. function intended multidimensional scales SEM models; excellent alternatives already exist purposes (e.g. lavaan, semTools).","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"function-usage","dir":"Articles","previous_headings":"","what":"Function usage","title":"likertMakeR::reliability()","text":"","code":"reliability(   data,   include = \"none\",   ci = FALSE,   ci_level = 0.95,   n_boot = 1000,   na_method = c(\"pairwise\", \"listwise\"),   min_count = 2,   digits = 3,   verbose = TRUE )"},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"data","dir":"Articles","previous_headings":"Arguments","what":"data","title":"likertMakeR::reliability()","text":"n × k data frame matrix containing item responses, rows correspond respondents columns correspond items.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"include","dir":"Articles","previous_headings":"Arguments","what":"include","title":"likertMakeR::reliability()","text":"character vector specifying additional reliability coefficients compute. Possible values : \"none\" (default) Computes Cronbach’s alpha McDonald’s omega (total) using Pearson correlations. \"lambda6\" Adds Guttman’s lambda-6, computed via psych::alpha() (requires optional package psych). \"omega_h\" Adds McDonald’s omega hierarchical (ωh\\omega_h), also known Coefficient H. coefficient estimates maximum reliability general factor single-factor model, assuming optimal weighting items. \"polychoric\" Adds ordinal reliability estimates, computed polychoric correlations: ordinal alpha (Zumbo’s alpha), ordinal omega (total). Multiple options may supplied, example:","code":"include = c(\"lambda6\", \"polychoric\")"},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"ci","dir":"Articles","previous_headings":"Arguments","what":"ci","title":"likertMakeR::reliability()","text":"Logical. TRUE, confidence intervals computed using nonparametric bootstrap. Default FALSE.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"ci_level","dir":"Articles","previous_headings":"Arguments","what":"ci_level","title":"likertMakeR::reliability()","text":"Confidence level bootstrap intervals. Default 0.95.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"n_boot","dir":"Articles","previous_headings":"Arguments","what":"n_boot","title":"likertMakeR::reliability()","text":"Number bootstrap resamples used ci = TRUE. Default 1000. Larger values reduce Monte Carlo error increase computation time, especially ordinal (polychoric-based) reliability estimates.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"na_method","dir":"Articles","previous_headings":"Arguments","what":"na_method","title":"likertMakeR::reliability()","text":"missing values handled: \"pairwise\" (default): correlations use available pairs, \"listwise\": rows missing values removed analysis.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"min_count","dir":"Articles","previous_headings":"Arguments","what":"min_count","title":"likertMakeR::reliability()","text":"Minimum observed frequency per response category required attempt polychoric correlations. Default 2. Ordinal reliability estimates skipped item contains categories fewer min_count observations. occurs, diagnostics stored returned object may inspected using ordinal_diagnostics().","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"digits","dir":"Articles","previous_headings":"Arguments","what":"digits","title":"likertMakeR::reliability()","text":"Number decimal places used printing estimates. Default 3.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"verbose","dir":"Articles","previous_headings":"Arguments","what":"verbose","title":"likertMakeR::reliability()","text":"Logical. TRUE, warnings progress indicators displayed. Default TRUE.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"pearson-based-coefficients-always-available","dir":"Articles","previous_headings":"Reliability coefficients returned","what":"Pearson-based coefficients (always available)","title":"likertMakeR::reliability()","text":"Cronbach’s alpha Computed Pearson correlation matrix. McDonald’s omega (total) Computed leading eigenvalue correlation matrix, assuming single common factor. estimates appropriate Likert-scale responses treated approximately interval-scaled.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"ordinal-polychoric-based-coefficients","dir":"Articles","previous_headings":"Reliability coefficients returned","what":"Ordinal (polychoric-based) coefficients","title":"likertMakeR::reliability()","text":"include = \"polychoric\": Ordinal alpha (Zumbo’s alpha) Cronbach’s alpha computed polychoric correlation matrix. Ordinal omega (total) McDonald’s omega computed polychoric correlation matrix. estimates often preferred items clearly ordinal, response distributions skewed, floor/ceiling effects present.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"ordinal-diagnostics-and-safeguards","dir":"Articles","previous_headings":"","what":"Ordinal diagnostics and safeguards","title":"likertMakeR::reliability()","text":"Ordinal reliability estimation can fail response categories sparse (e.g., observations extreme categories). occurs: ordinal estimates skipped rather forced, warning issued, diagnostics stored returned object. Diagnostics may inspected using:","code":"ordinal_diagnostics(result)"},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"hierarchical-reliability-omega_h-coefficient-h","dir":"Articles","previous_headings":"Ordinal diagnostics and safeguards","what":"Hierarchical reliability: ωh\\omega_h (Coefficient H)","title":"likertMakeR::reliability()","text":"include = \"omega_h\", reliability() reports McDonald’s omega hierarchical (ωh\\omega_h), also known Coefficient H. ωh\\omega_h answers different question α\\alpha ω\\omega (total): well underlying latent factor measured best possible linear combination items used? Key characteristics ωh\\omega_h: model-based upper bound reliability reflects factor determinacy, observed-score reliability assumes single dominant common factor insensitive scale length sensitive factor structure ωh\\omega_h therefore best interpreted diagnostic index, rather direct estimate reliability observed summed scores.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"why-no-confidence-intervals-for-omega_h","dir":"Articles","previous_headings":"Ordinal diagnostics and safeguards > Hierarchical reliability: ωh\\omega_h (Coefficient H)","what":"Why no confidence intervals for ωh\\omega_h?","title":"likertMakeR::reliability()","text":"Confidence intervals reported ωh\\omega_h. intentional: ωh\\omega_h maximal reliability bound, descriptive statistic sampling distribution highly non-normal Bootstrap confidence intervals often unstable misleading agreed inferential framework ωh\\omega_h literature Accordingly, ωh\\omega_h reported point estimate , explanatory notes output table.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"create-a-synthetic-dataset","dir":"Articles","previous_headings":"Examples","what":"Create a synthetic dataset","title":"likertMakeR::reliability()","text":"example generates four-item single-factor scale target Cronbach’s alpha 0.80, using functions LikertMakeR.","code":"# example correlation matrix my_cor <- LikertMakeR::makeCorrAlpha(   items = 4,   alpha = 0.80 ) #> reached max iterations (1600) - best mean difference: 5.3e-05  # example correlated dataframe my_data <- LikertMakeR::makeScales(   n = 64,   means = c(2.75, 3.00, 3.25, 3.50),   sds = c(1.25, 1.50, 1.30, 1.25),   lowerbound = rep(1, 4),   upperbound = rep(5, 4),   cormatrix = my_cor ) #> Variable  1 :  item01  - #> reached maximum of 4096 iterations #> Variable  2 :  item02  - #> best solution in 919 iterations #> Variable  3 :  item03  - #> reached maximum of 4096 iterations #> Variable  4 :  item04  - #> reached maximum of 4096 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables"},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"basic-reliability-estimates","dir":"Articles","previous_headings":"Examples","what":"Basic reliability estimates","title":"likertMakeR::reliability()","text":"default, reliability() returns Pearson-based Cronbach’s alpha McDonald’s omega (total), assuming single common factor.","code":"# $\\alpha$ and $\\omega$  reliability(my_data) #>    coef_name estimate n_items n_obs                notes #>        alpha    0.800       4    64 Pearson correlations #>  omega_total    0.871       4    64 1-factor eigen omega"},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"including-additional-coefficients","dir":"Articles","previous_headings":"Examples","what":"Including additional coefficients","title":"likertMakeR::reliability()","text":"Additional reliability coefficients may requested using include argument. available options : \"lambda6\" Adds Guttman’s lambda-6, computed using psych::alpha(). option requires suggested package psych. \"omega_h\" Adds omega hierarchical (Coefficient H), model-based upper bound reliability reflects well general factor measured. ωh\\omega_h reported point estimate best used diagnostic indicator factor strength rather observed-score reliability. \"polychoric\" Adds ordinal (polychoric-based) reliability estimates, including ordinal alpha (Zumbo’s alpha) ordinal omega (total). Multiple options may supplied simultaneously. \"none\" included alongside options, ignored. ordinal reliability estimates computed — commonly due sparse response categories — skipped automatically. cases, returned object contains diagnostic information explaining estimates omitted.","code":"# $\\alpha$, $\\omega$ (total), $\\lambda 6$, $\\omega_h$, and ordinal variants  reliability(   my_data,   include = c(\"lambda6\", \"omega_h\", \"polychoric\") ) #>            coef_name estimate n_items n_obs #>                alpha    0.800       4    64 #>          omega_total    0.871       4    64 #>              lambda6    0.794       4    64 #>              omega_h    0.812       4    64 #>        ordinal_alpha    0.754       4    64 #>  ordinal_omega_total    0.845       4    64 #>                                                notes #>                                 Pearson correlations #>                                 1-factor eigen omega #>                                       psych::alpha() #>     Coefficient H (1-factor FA, maximal reliability) #>                              Polychoric correlations #>  Polychoric correlations | Ordinal CIs not requested"},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"when-should-i-use-each-option","dir":"Articles","previous_headings":"Examples","what":"When should I use each option?","title":"likertMakeR::reliability()","text":"default, reliability() reports Cronbach’s alpha McDonald’s omega computed Pearson correlations. appropriate teaching, exploratory, applied settings, especially Likert items five categories reasonably symmetric distributions. Use include = \"lambda6\" want additional lower-bound reliability estimate less sensitive tau-equivalence assumptions. Guttman’s lambda-6 often reported alongside alpha omega methodological comparisons requires psych package. Use include = \"omega_h\" want assess strength clarity general factor underlying scale. ωh\\omega_h particularly useful evaluating whether set items meaningfully reflects single latent construct, interpreted reliability summed averaged scores. Use include = \"polychoric\" item responses clearly ordinal category distributions well populated. case, function computes ordinal alpha (Zumbo’s alpha) ordinal omega based polychoric correlations. Ordinal methods appropriate response categories (e.g., 4–5 points) treating items continuous may questionable. response categories sparse, ordinal estimates skipped diagnostics provided explain .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"notes-on-computation","dir":"Articles","previous_headings":"Examples","what":"Notes on computation","title":"likertMakeR::reliability()","text":"reliability coefficients reliability() computed assumption single common factor. function intended unidimensional scales perform factor extraction dimensionality testing. Cronbach’s alpha McDonald’s omega computed Pearson correlations default. include = \"polychoric\" specified, ordinal reliability estimates computed using polychoric correlations, corresponding Zumbo’s ordinal alpha ordinal omega total. Ordinal reliability estimates may skipped automatically : item fewer two observed response categories, one response categories occur fewer min_count times. cases, function returns NA ordinal estimates stores diagnostic information explaining decision. diagnostics can inspected using ordinal_diagnostics(). ci = TRUE, confidence intervals obtained using nonparametric bootstrap. ordinal reliability estimates, bootstrap resamples may fail polychoric correlations estimated resampled datasets. failures tracked internally reported output notes. Increasing n_boot can improve stability ordinal confidence intervals proportion successful bootstrap draws high complete. transparency, methodological details estimation methods bootstrap performance reported alongside point estimates returned table.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"choosing-a-reliability-coefficient-a-practical-decision-guide","dir":"Articles","previous_headings":"","what":"Choosing a Reliability Coefficient: A Practical Decision Guide","title":"likertMakeR::reliability()","text":"Researchers students often faced multiple reliability coefficients little guidance used. section provides practical, defensible guide choosing among Cronbach’s alpha, McDonald’s omega, ordinal counterparts working Likert-type rating-scale data. guidance assumes single-factor scale, design focus LikertMakeR.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"continuous-or-approximately-continuous-items","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide > Step 1: What kind of data do you have?","what":"Continuous or approximately continuous items","title":"likertMakeR::reliability()","text":"Examples: Scale scores many response options Visual analogue scales Aggregated averaged ratings → Pearson correlations usually appropriate.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"ordinal-likert-type-items","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide > Step 1: What kind of data do you have?","what":"Ordinal (Likert-type) items","title":"likertMakeR::reliability()","text":"Examples: Single 5-point 7-point agreement scales Frequency scales clear category boundaries → Ordinal (polychoric-based) methods often appropriate, especially responses skewed unevenly distributed.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"cronbachs-alpha-alpha","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide > Step 2: Choosing between α\\alpha and ω\\omega","what":"Cronbach’s alpha (α\\alpha)","title":"likertMakeR::reliability()","text":"Cronbach’s alpha widely reported reliability coefficient based average inter-item correlations. Use alpha : need comparability legacy literature Items roughly tau-equivalent (items make equal contributions underlying factor) want simple baseline estimate Limitations: Assumes equal factor loadings Can underestimate reliability loadings differ Sensitive number items Alpha viewed descriptive lower bound, definitive measure internal consistency.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"mcdonalds-omega-omega","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide > Step 2: Choosing between α\\alpha and ω\\omega","what":"McDonald’s omega (ω\\omega)","title":"likertMakeR::reliability()","text":"McDonald’s omega estimates proportion variance attributable single common factor, allowing items different loadings. Use omega : Items vary strength discrimination want model-based reliability estimate single factor theoretically justified Advantages: Fewer restrictive assumptions alpha Better behaved simulations Increasingly recommended methodological literature general rule, omega preferred alpha single-factor scales factor loadings unequal.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"where-does-guttmans-lambda_6-fit","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide > Step 2: Choosing between α\\alpha and ω\\omega","what":"Where does Guttman’s λ6\\lambda_6 fit?","title":"likertMakeR::reliability()","text":"Guttman’s lambda-6 (λ6\\lambda_6) lower-bound estimate reliability relaxes Cronbach’s assumption equal error variances across items. Use λ6\\lambda_6 : defensible α\\alpha, rely factor model comparing multiple lower-bound estimates want conservative benchmark alongside ω\\omega Key points: λ6\\lambda_6 always ≥\\geqslantα\\alpha data Like α\\alpha, lower bound — estimate true reliability Unlike ω\\omega, assume latent factor structure practice, λ6\\lambda_6 useful reported alongside α\\alpha ω\\omega show sensitive conclusions different reliability assumptions.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"step-3-when-should-i-use-ordinal-reliability","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide","what":"Step 3: When should I use ordinal reliability?","title":"likertMakeR::reliability()","text":"Ordinal reliability coefficients computed polychoric correlations, estimate associations latent continuous variables underlying ordinal responses. reliability(), correspond : Ordinal alpha (often called Zumbo’s alpha) Ordinal omega Use ordinal reliability : Items ordinal (e.g., 5- 7-point Likert scales) Response distributions skewed uneven wish respect ordinal measurement scale Important caveats: Polychoric correlations require sufficient observations per category Sparse categories can cause estimation failure Diagnostics always inspected ordinal estimation feasible, reliability() reports transparently falls back Pearson-based estimates.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"step-4-alpha-vs-omega-vs-ordinal-omega-a-practical-summary","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide","what":"Step 4: α\\alpha vs ω\\omega vs ordinal ω\\omega — a practical summary","title":"likertMakeR::reliability()","text":"doubt: data clearly ordinal diagnostics permit:","code":"Report omega, and optionally alpha for comparison. Ordinal omega is the most defensible choice.  “ordinal $\\omega$” refers to omega total computed from the  polychoric correlation matrix."},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"step-5-confidence-intervals","dir":"Articles","previous_headings":"Choosing a Reliability Coefficient: A Practical Decision Guide","what":"Step 5: Confidence intervals","title":"likertMakeR::reliability()","text":"ci = TRUE, LikertMakeR computes nonparametric bootstrap confidence intervals. bootstrap? closed-form CI exists omega Ordinal reliability reliable analytic CI Bootstrap intervals flexible robust Practical advice: Use least 1,000 resamples stable intervals Expect longer runtimes ordinal bootstraps Always report method used compute CIs Confidence intervals intentionally provided ωh\\omega_h, represents model-based upper bound reliability rather inferential estimate.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"recommended-reading","dir":"Articles","previous_headings":"","what":"Recommended reading","title":"likertMakeR::reliability()","text":"readers want go little deeper use reliability() teaching applied research, following sources provide accessible explanations ideas behind coefficients reported .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"understanding-cronbachs-alpha-and-its-limitations","dir":"Articles","previous_headings":"Recommended reading","what":"Understanding Cronbach’s alpha and its limitations","title":"likertMakeR::reliability()","text":"Cronbach, L. J. (1951). Coefficient alpha internal structure tests. (Cronbach (1951)) original source alpha; still worth reading understand alpha ——measure. Revelle, W., & Zinbarg, R. E. (2009). Coefficients alpha, beta, omega, glb. (Revelle Zinbarg (2009)) clear discussion alpha can misleading omega preferable.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"omega-and-factor-based-reliability","dir":"Articles","previous_headings":"Recommended reading","what":"Omega and factor-based reliability","title":"likertMakeR::reliability()","text":"McDonald, R. P. (1999). Test theory: unified treatment. (McDonald (2013)) definitive reference omega; recommended readers comfortable factor analysis concepts. Hancock, G. R., & Mueller, R. O. (2001). Rethinking construct reliability within latent variable systems. (Hancock Mueller (2001)) Introduces Coefficient H discusses interpretation factor determinacy rather observed-score reliability.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"comparative-studies","dir":"Articles","previous_headings":"Recommended reading","what":"Comparative studies","title":"likertMakeR::reliability()","text":"Xiao L, Hau KT. (2022). Performance Coefficient Alpha Alternatives: Effects Different Types Non-Normality. (Xiao Hau (2023))","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"ordinal-reliability-for-likert-type-data","dir":"Articles","previous_headings":"Recommended reading","what":"Ordinal reliability for Likert-type data","title":"likertMakeR::reliability()","text":"Zumbo, B. D., Gadermann, . M., & Zeisser, C. (2007). Ordinal versions coefficients alpha theta Likert rating scales. (Zumbo, Gadermann, Zeisser (2007)) Introduces ordinal (polychoric-based) alpha—often called Zumbo’s alpha. Gadermann, . M., Guhn, M., & Zumbo, B. D. (2012). Estimating ordinal reliability Likert-type ordinal item response data. (Gadermann, Guhn, Zumbo (2012)) practical, non-technical guide especially suitable teaching.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"polychoric-correlations-in-practice","dir":"Articles","previous_headings":"Recommended reading","what":"Polychoric correlations in practice","title":"likertMakeR::reliability()","text":"Holgado–Tello, F. P., et al. (2010). Polychoric versus Pearson correlations factor analysis ordinal variables. (Holgado–Tello et al. (2010)) helpful applied comparison explaining Pearson correlations can distort analyses Likert-type data.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/articles/reliability_measures.html","id":"teaching-tip","dir":"Articles","previous_headings":"","what":"Teaching tip","title":"likertMakeR::reliability()","text":"classroom examples, start Pearson-based alpha omega. Introduce ordinal reliability students understand: factor models, Likert responses truly continuous. mirrors progressive structure used reliability() helps students see additional assumptions required ordinal methods.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hume Winzar. Maintainer, author.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Winzar H (2025). LikertMakeR: Synthesise Correlate Likert Scale Rating-Scale Data Based Summary Statistics. R package version 1.4.0, https://github.com/WinzarH/LikertMakeR/.","code":"@Manual{,   title = {LikertMakeR: Synthesise and Correlate Likert Scale and Rating-Scale Data Based on Summary Statistics},   author = {Hume Winzar},   year = {2025},   note = {R package version 1.4.0},   url = {https://github.com/WinzarH/LikertMakeR/}, }"},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"likertmaker-","dir":"","previous_headings":"","what":"LikertMakeR","title":"LikertMakeR","text":"LikertMakeR synthesises Likert-scale related bounded rating-scale data predefined means, standard deviations, (optionally) correlations, Cronbach’s alpha, factor-loading-based structure.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"purpose","dir":"","previous_headings":"","what":"Purpose","title":"LikertMakeR","text":"Reverse-engineer published results summary statistics reported (re-analysis, visualisation, teaching). Teaching & demos: generate data known properties without collecting real data. Methods work / simulation: explore reliability, items, bounds, sample size interact. full introduction worked examples, see package website: https://winzarh.github.io/LikertMakeR/","code":""},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"LikertMakeR","text":"CRAN: latest development version available author’s GitHub repository.","code":"install.packages(\"LikertMakeR\") library(devtools)    install_github(\"WinzarH/LikertMakeR\")"},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"LikertMakeR","text":"Make target correlation matrix desired Cronbach’s alpha Generate synthetic rating-scale data predefined moments","code":"library(LikertMakeR)  R <- makeCorrAlpha(items = 4, alpha = 0.80)      R dat <- makeScales(   n = 64,   means = c(2.75, 3.00, 3.25, 3.50),   sds   = c(1.25, 1.50, 1.30, 1.25),   lowerbound = rep(1, 4),   upperbound = rep(5, 4),   items = 4,   cormatrix = R )  head(dat) cor(dat) |> round(2)"},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"key-functions","dir":"","previous_headings":"","what":"Key functions","title":"LikertMakeR","text":"lfast(): generate bounded/discrete data target mean & SD lcor(): rearrange columns approximate target correlation matrix makeCorrAlpha(): generate item correlation matrix target Cronbach’s alpha makeScales(): wrapper lfast() + lcor() generate dataframe correlated columns makeCorrLoadings(): build item correlation matrix factor loadings factor correlations makeItemsScale(): generate items summated scale target alpha makePaired() / makeRepeated(): reconstruct data paired t-test / repeated-measures summaries makeScalesRegression(): generate data summary multiple-regression analysis correlateScales(): combine multiple item sets summated scales match target correlation matrix Helpers: alpha(), eigenvalues(), reliability()","code":""},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"rating-scale-properties","dir":"","previous_headings":"","what":"Rating scale properties","title":"LikertMakeR","text":"Likert scale mean, sum, several ordinal rating scales. bipolar (usually “agree-disagree”) responses propositions determined moderately--highly correlated among , capturing various facets theoretical construct. single 1-5 rating scale Likert scale - may Likert-scale item. Summated rating scales continuous unbounded. example, 5-point Likert scale constructed , say, five items (questions) summed range 5 (rated ‘1’) 25 (rated ‘5’) integers , mean range ‘1’ ‘5’ intervals 1/5=0.20. 7-point Likert scale constructed eight items summed range 8 (rated ‘1’) 56 (rated ‘7’) integers , mean range ‘1’ ‘7’ intervals 1/8=0.125.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn more","title":"LikertMakeR","text":"Package website (recommended): https://winzarh.github.io/LikertMakeR/ Vignettes cover: generating scales summary statistics, correlation matrices alpha loadings, repeated-measures paired designs, reliability estimation diagnostics, validation studies demonstrating function accuracy.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"apa","dir":"","previous_headings":"Learn more > To cite LikertMakeR","what":"APA:","title":"LikertMakeR","text":"","code":"Winzar, H. (2025). LikertMakeR (version 1.4.0) [R package].   The Comprehensive R Archive Network (CRAN), <https://CRAN.R-project.org/package=LikertMakeR>"},{"path":"https://winzarh.github.io/LikertMakeR/index.html","id":"bib","dir":"","previous_headings":"Learn more > To cite LikertMakeR","what":"BIB:","title":"LikertMakeR","text":"","code":"@software{winzar2025},  title = {LikertMakeR},  author = {Winzar, Hume},  abstract = {LikertMakeR synthesises and correlates rating-scale data with predefined means and standard deviations.},  publisher = {The Comprehensive R Archive Network (CRAN)},  month = dec,  year = {2025},  version = {1.4.0},  origdate = {2022},  url = {https://CRAN.R-project.org/package=LikertMakeR},  note = {R package} }"},{"path":"https://winzarh.github.io/LikertMakeR/reference/alpha.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Cronbach's Alpha from a correlation matrix or dataframe — alpha","title":"Calculate Cronbach's Alpha from a correlation matrix or dataframe — alpha","text":"alpha() calculates Cronbach's Alpha given correlation matrix given dataframe.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/alpha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Cronbach's Alpha from a correlation matrix or dataframe — alpha","text":"","code":"alpha(cormatrix = NULL, data = NULL)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/alpha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Cronbach's Alpha from a correlation matrix or dataframe — alpha","text":"cormatrix (real) square symmetrical matrix values ranging -1 +1 '1' diagonal data (real) dataframe matrix","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/alpha.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Cronbach's Alpha from a correlation matrix or dataframe — alpha","text":"single value","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/alpha.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Cronbach's Alpha from a correlation matrix or dataframe — alpha","text":"","code":"## Sample data frame df <- data.frame(   V1  =  c(4, 2, 4, 3, 2, 2, 2, 1),   V2  =  c(4, 1, 3, 4, 4, 3, 2, 3),   V3  =  c(4, 1, 3, 5, 4, 1, 4, 2),   V4  =  c(4, 3, 4, 5, 3, 3, 3, 3) )  ## example correlation matrix corMat <- matrix(   c(     1.00, 0.35, 0.45, 0.70,     0.35, 1.00, 0.60, 0.55,     0.45, 0.60, 1.00, 0.65,     0.70, 0.55, 0.65, 1.00   ),   nrow = 4, ncol = 4 )  ## apply function examples  alpha(cormatrix = corMat) #> [1] 0.8301887  alpha(, df) #> [1] 0.830008  alpha(corMat, df) #> Alert:  #> Both cormatrix and data present. #>                  #> Using cormatrix by default. #> [1] 0.8301887"},{"path":"https://winzarh.github.io/LikertMakeR/reference/correlateScales.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataframe of correlated scales from different dataframes of scale items — correlateScales","title":"Dataframe of correlated scales from different dataframes of scale items — correlateScales","text":"correlateScales() creates dataframe scale items representing correlated constructs, one might find completed questionnaire.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/correlateScales.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataframe of correlated scales from different dataframes of scale items — correlateScales","text":"","code":"correlateScales(dataframes, scalecors)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/correlateScales.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataframe of correlated scales from different dataframes of scale items — correlateScales","text":"dataframes list 'k' dataframes rearranged combined scalecors target correlation matrix - symmetric \\(k \\times k\\) positive-semi-definite matrix, 'k' number dataframes","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/correlateScales.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dataframe of correlated scales from different dataframes of scale items — correlateScales","text":"Returns dataframe whose columns taken starter dataframes whose summated values correlated according user-specified correlation matrix","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/correlateScales.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dataframe of correlated scales from different dataframes of scale items — correlateScales","text":"Correlated rating-scale items generally summed averaged create measure \"unobservable\", \"latent\", construct. correlateScales() takes several dataframes rating-scale items rearranges rows scales correlated according predefined correlation matrix. Univariate statistics dataframe rating-scale items change, correlations rating-scale items dataframes .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/correlateScales.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dataframe of correlated scales from different dataframes of scale items — correlateScales","text":"","code":"## three attitudes and a behavioural intention n <- 32 lower <- 1 upper <- 5  ### attitude #1 cor_1 <- makeCorrAlpha(items = 4, alpha = 0.90) #> reached max iterations (1600) - best mean difference: 2.2e-05 means_1 <- c(2.5, 2.5, 3.0, 3.5) sds_1 <- c(0.9, 1.0, 0.9, 1.0)  Att_1 <- makeScales(   n = n, means = means_1, sds = sds_1,   lowerbound = rep(lower, 4), upperbound = rep(upper, 4),   items = 4,   cormatrix = cor_1 ) #> Variable  1 :  item01  -  #> reached maximum of 1024 iterations #> Variable  2 :  item02  -  #> best solution in 667 iterations #> Variable  3 :  item03  -  #> reached maximum of 1024 iterations #> Variable  4 :  item04  -  #> reached maximum of 1024 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables #>    ### attitude #2 cor_2 <- makeCorrAlpha(items = 5, alpha = 0.85) #> reached max iterations (2500) - best mean difference: 8.1e-05 #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 1 (min eigenvalue: -0.017242) #> improved at swap - 3 (min eigenvalue: -0.008469) #> improved at swap - 4 (min eigenvalue: 0.025501) #> positive definite at swap - 4 means_2 <- c(2.5, 2.5, 3.0, 3.0, 3.5) sds_2 <- c(1.0, 1.0, 0.9, 1.0, 1.5)  Att_2 <- makeScales(   n = n, means = means_2, sds = sds_2,   lowerbound = rep(lower, 5), upperbound = rep(upper, 5),   items = 5,   cormatrix = cor_2 ) #> Variable  1 :  item01  -  #> reached maximum of 1024 iterations #> Variable  2 :  item02  -  #> reached maximum of 1024 iterations #> Variable  3 :  item03  -  #> best solution in 808 iterations #> Variable  4 :  item04  -  #> reached maximum of 1024 iterations #> Variable  5 :  item05  -  #> reached maximum of 1024 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables #>    ### attitude #3 cor_3 <- makeCorrAlpha(items = 6, alpha = 0.75) #> correlation values consistent with desired alpha in 72 iterations means_3 <- c(2.5, 2.5, 3.0, 3.0, 3.5, 3.5) sds_3 <- c(1.0, 1.5, 1.0, 1.5, 1.0, 1.5)  Att_3 <- makeScales(   n = n, means = means_3, sds = sds_3,   lowerbound = rep(lower, 6), upperbound = rep(upper, 6),   items = 6,   cormatrix = cor_3 ) #> Variable  1 :  item01  -  #> reached maximum of 1024 iterations #> Variable  2 :  item02  -  #> reached maximum of 1024 iterations #> Variable  3 :  item03  -  #> reached maximum of 1024 iterations #> Variable  4 :  item04  -  #> reached maximum of 1024 iterations #> Variable  5 :  item05  -  #> reached maximum of 1024 iterations #> Variable  6 :  item06  -  #> reached maximum of 1024 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables #>    ### behavioural intention intent <- lfast(n, mean = 3.0, sd = 3, lowerbound = 0, upperbound = 10) |>   data.frame() #> reached maximum of 1024 iterations names(intent) <- \"int\"   ### target scale correlation matrix scale_cors <- matrix(   c(     1.0, 0.6, 0.5, 0.3,     0.6, 1.0, 0.4, 0.2,     0.5, 0.4, 1.0, 0.1,     0.3, 0.2, 0.1, 1.0   ),   nrow = 4 )  data_frames <- list(\"A1\" = Att_1, \"A2\" = Att_2, \"A3\" = Att_3, \"Int\" = intent)   ### apply the function my_correlated_scales <- correlateScales(   dataframes = data_frames,   scalecors = scale_cors ) #> scalecors  is positive-definite #>  #> New dataframe successfully created head(my_correlated_scales) #>   A1_1 A1_2 A1_3 A1_4 A2_1 A2_2 A2_3 A2_4 A2_5     A3_1     A3_2     A3_3 #> 1 4.25 4.00 3.75 3.75  2.4  1.2  3.2  4.8  5.0 3.666667 1.166667 1.166667 #> 2 2.50 3.00 3.25 4.25  3.8  2.8  2.8  4.2  4.8 1.833333 3.500000 4.833333 #> 3 4.25 4.25 4.25 4.75  3.4  3.2  3.6  3.4  3.8 2.833333 1.333333 2.666667 #> 4 2.00 1.25 1.25 2.50  1.4  1.8  2.0  1.2  2.4 3.000000 1.000000 2.000000 #> 5 2.00 1.50 2.00 3.00  2.8  2.2  3.6  4.0  3.2 3.833333 1.166667 3.000000 #> 6 2.25 3.50 3.25 4.25  3.2  2.2  4.0  4.4  4.8 3.833333 4.833333 4.500000 #>       A3_4     A3_5     A3_6 Int_1 #> 1 4.500000 2.333333 3.666667     1 #> 2 5.000000 5.000000 5.000000     4 #> 3 3.000000 3.833333 5.000000     8 #> 4 1.833333 1.833333 4.166667     1 #> 5 3.666667 1.833333 5.000000     1 #> 6 4.833333 3.666667 5.000000     9"},{"path":"https://winzarh.github.io/LikertMakeR/reference/eigenvalues.html","id":null,"dir":"Reference","previous_headings":"","what":"calculate eigenvalues of a correlation matrix with optional scree plot — eigenvalues","title":"calculate eigenvalues of a correlation matrix with optional scree plot — eigenvalues","text":"eigenvalues() calculates eigenvalues correlation matrix optionally produces scree plot.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/eigenvalues.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"calculate eigenvalues of a correlation matrix with optional scree plot — eigenvalues","text":"","code":"eigenvalues(cormatrix, scree = FALSE)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/eigenvalues.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"calculate eigenvalues of a correlation matrix with optional scree plot — eigenvalues","text":"cormatrix (real, matrix) correlation matrix scree (logical) default = FALSE. TRUE (1), eigenvalues() produces scree plot illustrate eigenvalues","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/eigenvalues.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"calculate eigenvalues of a correlation matrix with optional scree plot — eigenvalues","text":"vector eigenvalues report positive-definite status cormatrix","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/eigenvalues.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"calculate eigenvalues of a correlation matrix with optional scree plot — eigenvalues","text":"","code":"## define parameters  correlationMatrix <- matrix(   c(     1.00, 0.25, 0.35, 0.40,     0.25, 1.00, 0.70, 0.75,     0.35, 0.70, 1.00, 0.80,     0.40, 0.75, 0.80, 1.00   ),   nrow = 4, ncol = 4 )  ## apply function  evals <- eigenvalues(cormatrix = correlationMatrix) #> correlationMatrix  is positive-definite #>  evals <- eigenvalues(correlationMatrix, 1)  #> correlationMatrix  is positive-definite #>"},{"path":"https://winzarh.github.io/LikertMakeR/reference/lcor.html","id":null,"dir":"Reference","previous_headings":"","what":"Rearrange elements in each column of a data-frame to fit a predefined correlation matrix — lcor","title":"Rearrange elements in each column of a data-frame to fit a predefined correlation matrix — lcor","text":"lcor() rearranges values column data-frame columns correlated match predefined correlation matrix.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lcor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rearrange elements in each column of a data-frame to fit a predefined correlation matrix — lcor","text":"","code":"lcor(data, target, passes = 10)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/lcor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rearrange elements in each column of a data-frame to fit a predefined correlation matrix — lcor","text":"data dataframe rearranged target target correlation matrix. Must dimensions number columns data-frame. passes Number optimization passes (default = 10). Increasing value MAY improve results n-columns (target correlation matrix dimensions) many. Decreasing value 'passes' faster may decrease accuracy.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lcor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rearrange elements in each column of a data-frame to fit a predefined correlation matrix — lcor","text":"Returns dataframe whose column-wise correlations approximate user-specified correlation matrix","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lcor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rearrange elements in each column of a data-frame to fit a predefined correlation matrix — lcor","text":"Values column change, univariate statistics remain .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lcor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rearrange elements in each column of a data-frame to fit a predefined correlation matrix — lcor","text":"","code":"## parameters n <- 32 lowerbound <- 1 upperbound <- 5 items <- 5  mydat3 <- data.frame(   x1 = lfast(n, 2.5, 0.75, lowerbound, upperbound, items),   x2 = lfast(n, 3.0, 1.50, lowerbound, upperbound, items),   x3 = lfast(n, 3.5, 1.00, lowerbound, upperbound, items) ) #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations  cor(mydat3) |> round(3) #>        x1     x2    x3 #> x1  1.000 -0.121 0.125 #> x2 -0.121  1.000 0.171 #> x3  0.125  0.171 1.000  tgt3 <- matrix(   c(     1.00, 0.50, 0.75,     0.50, 1.00, 0.25,     0.75, 0.25, 1.00   ),   nrow = 3, ncol = 3 )  ## apply function new3 <- lcor(mydat3, tgt3)  ## test output cor(new3) |> round(3) #>       X1   X2    X3 #> X1 1.000 0.50 0.751 #> X2 0.500 1.00 0.250 #> X3 0.751 0.25 1.000"},{"path":"https://winzarh.github.io/LikertMakeR/reference/lexact.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated. Use lfast() instead — lexact","title":"Deprecated. Use lfast() instead — lexact","text":"lexact DEPRECATED. Replaced LikertMakeR Version 0.4.0 new version lfast. lexact remains legacy earlier package users. now just wrapper lfast Previously, lexact used Differential Evolution (DE) algorithm find optimum solution desired mean standard deviation, found updated lfast function much faster just accurate. Also package much less bulky.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lexact.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated. Use lfast() instead — lexact","text":"","code":"lexact(n, mean, sd, lowerbound, upperbound, items = 1)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/lexact.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated. Use lfast() instead — lexact","text":"n (positive, int) number observations generate mean (real) target mean sd (real) target standard deviation lowerbound (positive, int) lower bound upperbound (positive, int) upper bound items (positive, int) number items rating scale.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lexact.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deprecated. Use lfast() instead — lexact","text":"vector simulated data approximating user-specified conditions.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lexact.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Deprecated. Use lfast() instead — lexact","text":"","code":"x <- lexact(   n = 256,   mean = 4.0,   sd = 1.0,   lowerbound = 1,   upperbound = 7,   items = 6 ) #> lexact() function is deprecated. #>            #> Using the more efficient lfast() function instead #> best solution in 3630 iterations  x <- lexact(256, 2, 1.8, 0, 10) #> lexact() function is deprecated. #>            #> Using the more efficient lfast() function instead #> best solution in 989 iterations"},{"path":"https://winzarh.github.io/LikertMakeR/reference/lfast.html","id":null,"dir":"Reference","previous_headings":"","what":"Synthesise rating-scale data with predefined mean and standard deviation — lfast","title":"Synthesise rating-scale data with predefined mean and standard deviation — lfast","text":"lfast() applies simple Evolutionary Algorithm find vector best fits desired moments. lfast() generates random discrete values scaled Beta distribution data replicate ordinal rating scale - example, Likert scale made multiple items (questions) 0-10 likelihood--purchase scale. Data generated generally consistent real data, shown lfast() validation article.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lfast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synthesise rating-scale data with predefined mean and standard deviation — lfast","text":"","code":"lfast(n, mean, sd, lowerbound, upperbound, items = 1, precision = 0)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/lfast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Synthesise rating-scale data with predefined mean and standard deviation — lfast","text":"n (positive, int) number observations generate mean (real) target mean, upper lower bounds sd (positive, real) target standard deviation lowerbound (int) lower bound (e.g. '1' 1-5 rating scale) upperbound (int) upper bound (e.g. '5' 1-5 rating scale) items (positive, int) number items rating scale. Default = 1 precision (positive, real) can relax level accuracy required. (e.g. '1' generally generates vector moments correct within '0.025', '2' generally within '0.05') Default = 0","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/lfast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Synthesise rating-scale data with predefined mean and standard deviation — lfast","text":"vector approximating user-specified conditions.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/reference/lfast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Synthesise rating-scale data with predefined mean and standard deviation — lfast","text":"","code":"## six-item 1-7 rating scale x <- lfast(   n = 256,   mean = 4.0,   sd = 1.25,   lowerbound = 1,   upperbound = 7,   items = 6 ) #> best solution in 3162 iterations  ## five-item -3 to +3 rating scale x <- lfast(   n = 64,   mean = 0.025,   sd = 1.25,   lowerbound = -3,   upperbound = 3,   items = 5 ) #> best solution in 3726 iterations  ## four-item 1-5 rating scale with medium variation x <- lfast(   n = 128,   mean = 3.0,   sd = 1.00,   lowerbound = 1,   upperbound = 5,   items = 4,   precision = 5 ) #> best solution in 3 iterations  ## eleven-point 'likelihood of purchase' scale x <- lfast(256, 3, 3.0, 0, 10) #> best solution in 4924 iterations"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrAlpha.html","id":null,"dir":"Reference","previous_headings":"","what":"Correlation matrix from Cronbach's Alpha — makeCorrAlpha","title":"Correlation matrix from Cronbach's Alpha — makeCorrAlpha","text":"makeCorrAlpha() generates random correlation matrix given dimensions predefined Cronbach's Alpha. correlation matrix can applied makeScales() function generate synthetic data predefined alpha.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrAlpha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Correlation matrix from Cronbach's Alpha — makeCorrAlpha","text":"","code":"makeCorrAlpha(   items,   alpha,   variance = 0.5,   precision = 0,   sort_cors = FALSE,   diagnostics = FALSE )"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrAlpha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Correlation matrix from Cronbach's Alpha — makeCorrAlpha","text":"items (positive, int) matrix dimensions: number rows & columns generate alpha (real) target Cronbach's Alpha (usually positive, must -0.3 +1) variance (positive, real) Default = 0.5. User-provided standard deviation values sampled normally-distributed log transformation. Caution: Larger values increase chance non-positive-definite matrix. 'TRUE' faster, produces less natural output. Default = FALSE precision (positive, real) Default = 0. User-defined value ranging '0' '3' add random variation around target Cronbach's Alpha. '0' gives exact alpha (two decimal places) sort_cors (logical) 'TRUE', sorts correlation coefficients final correlation matrix. Similar earlier version function. diagnostics (logical) 'TRUE', returns list containing correlation matrix diagnostics list (target/achieved alpha, average inter-item correlation, eigenvalues, PD flag, key arguments). 'FALSE' (default), returns correlation matrix .","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrAlpha.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Correlation matrix from Cronbach's Alpha — makeCorrAlpha","text":"'diagnostics = FALSE', k x k correlation matrix. 'diagnostics = TRUE', list components: R k x k correlation matrix diagnostics list summary statistics","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrAlpha.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Correlation matrix from Cronbach's Alpha — makeCorrAlpha","text":"Random values generated makeCorrAlpha() highly volatile. makeCorrAlpha() may generate feasible (positive-definite) correlation matrix, especially variance high relative desired Alpha, desired correlation dimensions makeCorrAlpha() inform user resulting correlation matrix positive definite, . returned correlation matrix positive-definite, feasible solution may still possible. user encouraged try , possibly several times, find one.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrAlpha.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Correlation matrix from Cronbach's Alpha — makeCorrAlpha","text":"","code":"# define parameters items <- 4 alpha <- 0.85 variance <- 0.5  # apply function set.seed(42) cor_matrix <- makeCorrAlpha(   items = items,   alpha = alpha,   variance = variance ) #> correlation values consistent with desired alpha in 59 iterations  # test function output print(cor_matrix) #>           item01    item02    item03    item04 #> item01 1.0000000 0.7658611 0.6926037 0.4331446 #> item02 0.7658611 1.0000000 0.6936888 0.4251139 #> item03 0.6926037 0.6936888 1.0000000 0.5069007 #> item04 0.4331446 0.4251139 0.5069007 1.0000000 alpha(cor_matrix) #> [1] 0.8500063 eigenvalues(cor_matrix, 1)  #> cor_matrix  is positive-definite #>  #> [1] 2.7831667 0.6670820 0.3157114 0.2340400  # higher alpha, more items cor_matrix2 <- makeCorrAlpha(   items = 8,   alpha = 0.95 ) #> correlation values consistent with desired alpha in 731 iterations #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 1 (min eigenvalue: -0.124095) #> improved at swap - 2 (min eigenvalue: -0.122669) #> improved at swap - 19 (min eigenvalue: -0.102987) #> improved at swap - 20 (min eigenvalue: -0.073935) #> improved at swap - 22 (min eigenvalue: -0.064356) #> improved at swap - 26 (min eigenvalue: -0.057259) #> improved at swap - 27 (min eigenvalue: -0.041386) #> improved at swap - 31 (min eigenvalue: -0.031194) #> improved at swap - 32 (min eigenvalue: -0.030663) #> improved at swap - 33 (min eigenvalue: -0.010804) #> improved at swap - 67 (min eigenvalue: -0.007359) #> improved at swap - 71 (min eigenvalue: -0.005866) #> improved at swap - 77 (min eigenvalue: -0.000102) #> improved at swap - 102 (min eigenvalue: 0.002371) #> positive definite at swap - 102  # test output cor_matrix2 |> round(2) #>        item01 item02 item03 item04 item05 item06 item07 item08 #> item01   1.00   0.69   0.83   0.76   0.76   0.69   0.72   0.51 #> item02   0.69   1.00   0.86   0.79   0.75   0.67   0.45   0.89 #> item03   0.83   0.86   1.00   0.73   0.89   0.70   0.58   0.69 #> item04   0.76   0.79   0.73   1.00   0.78   0.73   0.87   0.62 #> item05   0.76   0.75   0.89   0.78   1.00   0.81   0.73   0.58 #> item06   0.69   0.67   0.70   0.73   0.81   1.00   0.68   0.71 #> item07   0.72   0.45   0.58   0.87   0.73   0.68   1.00   0.25 #> item08   0.51   0.89   0.69   0.62   0.58   0.71   0.25   1.00 alpha(cor_matrix2) |> round(3) #> [1] 0.95 eigenvalues(cor_matrix2, 1) |> round(3)  #> cor_matrix2  is positive-definite #>  #> [1] 5.954 0.983 0.418 0.340 0.223 0.047 0.032 0.002   # large random variation around alpha set.seed(42) cor_matrix3 <- makeCorrAlpha(   items = 6,   alpha = 0.85,   precision = 2 ) #> correlation values consistent with desired alpha in 2484 iterations #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 3 (min eigenvalue: -0.034581) #> improved at swap - 6 (min eigenvalue: -0.013659) #> improved at swap - 7 (min eigenvalue: -0.003994) #> improved at swap - 9 (min eigenvalue: 0.006886) #> positive definite at swap - 9  # test output cor_matrix3 |> round(2) #>        item01 item02 item03 item04 item05 item06 #> item01   1.00   0.74   0.47   0.68   0.77   0.78 #> item02   0.74   1.00   0.77   0.85   0.71   0.78 #> item03   0.47   0.77   1.00   0.76   0.71   0.48 #> item04   0.68   0.85   0.76   1.00   0.74   0.90 #> item05   0.77   0.71   0.71   0.74   1.00   0.72 #> item06   0.78   0.78   0.48   0.90   0.72   1.00 alpha(cor_matrix3) |> round(3) #> [1] 0.94 eigenvalues(cor_matrix3, 1) |> round(3)  #> cor_matrix3  is positive-definite #>  #> [1] 4.638 0.641 0.390 0.235 0.089 0.007   # with diagnostics cor_matrix4 <- makeCorrAlpha(   items = 4,   alpha = 0.80,   diagnostics = TRUE ) #> correlation values consistent with desired alpha in 1400 iterations #> Correlation matrix is not yet positive definite #> Working on it #>  #> improved at swap - 1 (min eigenvalue: 0.060505) #> positive definite at swap - 1  # test output cor_matrix4 #> $R #>           item01    item02    item03    item04 #> item01 1.0000000 0.6754423 0.1099580 0.3831963 #> item02 0.6754423 1.0000000 0.5653570 0.4757467 #> item03 0.1099580 0.5653570 1.0000000 0.7902754 #> item04 0.3831963 0.4757467 0.7902754 1.0000000 #>  #> $diagnostics #> $diagnostics$items #> [1] 4 #>  #> $diagnostics$alpha_target #> [1] 0.8 #>  #> $diagnostics$alpha_achieved #> [1] 0.7999974 #>  #> $diagnostics$average_r #> [1] 0.499996 #>  #> $diagnostics$eigenvalues #> [1] 2.52156119 1.02842128 0.38951300 0.06050454 #>  #> $diagnostics$is_positive_definite #> [1] TRUE #>  #> $diagnostics$variance #> [1] 0.5 #>  #> $diagnostics$precision #> [1] 0 #>  #> $diagnostics$sort_cors #> [1] FALSE #>  #>"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrLoadings.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Inter-Item Correlation Matrix from Factor Loadings — makeCorrLoadings","title":"Generate Inter-Item Correlation Matrix from Factor Loadings — makeCorrLoadings","text":"Constructs inter-item correlation matrix based user-supplied matrix standardised factor loadings (optionally) factor correlation matrix. makeCorrLoadings() function surprisingly good job reproducing target correlation matrix item-factor loadings present, shown makeCorrLoadings() validation article.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrLoadings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Inter-Item Correlation Matrix from Factor Loadings — makeCorrLoadings","text":"","code":"makeCorrLoadings(   loadings,   factorCor = NULL,   uniquenesses = NULL,   nearPD = FALSE,   diagnostics = FALSE )"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrLoadings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Inter-Item Correlation Matrix from Factor Loadings — makeCorrLoadings","text":"loadings Numeric matrix. \\(k \\times f\\) matrix standardized factor loadings \\(items \\times factors\\). Row names column names used diagnostics present. factorCor Optional \\(f \\times f\\) matrix factor correlations (\\(\\Phi\\)). NULL, assumes orthogonal factors. uniquenesses Optional vector length k. NULL, calculated \\(1 - rowSums(loadings^2)\\). nearPD Logical. TRUE, attempts coerce non–positive-definite matrices using Matrix::nearPD(). diagnostics Logical. TRUE, returns diagnostics including McDonald's Omega item-level summaries.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrLoadings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Inter-Item Correlation Matrix from Factor Loadings — makeCorrLoadings","text":"diagnostics = FALSE, returns correlation matrix (class: matrix). diagnostics = TRUE, returns list : - R: correlation matrix - Omega: per-factor Omega adjusted Omega - OmegaTotal: total Omega across factors - Diagnostics: dataframe communalities, uniquenesses, primary factor","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeCorrLoadings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Inter-Item Correlation Matrix from Factor Loadings — makeCorrLoadings","text":"","code":"# -------------------------------------------------------- # Example 1: Basic use without diagnostics # --------------------------------------------------------  factorLoadings <- matrix(   c(     0.05, 0.20, 0.70,     0.10, 0.05, 0.80,     0.05, 0.15, 0.85,     0.20, 0.85, 0.15,     0.05, 0.85, 0.10,     0.10, 0.90, 0.05,     0.90, 0.15, 0.05,     0.80, 0.10, 0.10   ),   nrow = 8, ncol = 3, byrow = TRUE )  rownames(factorLoadings) <- paste0(\"Q\", 1:8) colnames(factorLoadings) <- c(\"Factor1\", \"Factor2\", \"Factor3\")  factorCor <- matrix(   c(     1.0,  0.7, 0.6,     0.7,  1.0, 0.4,     0.6,  0.4, 1.0   ),   nrow = 3, byrow = TRUE )  itemCor <- makeCorrLoadings(factorLoadings, factorCor) round(itemCor, 3) #>       Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8 #> Q1 1.000 0.638 0.683 0.537 0.477 0.484 0.552 0.521 #> Q2 0.638 1.000 0.735 0.503 0.434 0.436 0.557 0.531 #> Q3 0.683 0.735 1.000 0.569 0.499 0.503 0.601 0.570 #> Q4 0.537 0.503 0.569 1.000 0.799 0.839 0.751 0.676 #> Q5 0.477 0.434 0.499 0.799 1.000 0.805 0.670 0.599 #> Q6 0.484 0.436 0.503 0.839 0.805 1.000 0.709 0.633 #> Q7 0.552 0.557 0.601 0.751 0.670 0.709 1.000 0.790 #> Q8 0.521 0.531 0.570 0.676 0.599 0.633 0.790 1.000  # -------------------------------------------------------- # Example 2: Diagnostics with factor correlations (Adjusted Omega) # --------------------------------------------------------  result_adj <- makeCorrLoadings(   loadings = factorLoadings,   factorCor = factorCor,   diagnostics = TRUE ) #> Diagnostics returned with Adjusted Omega (accounting for factor correlations).  # View outputs round(result_adj$R, 3) # correlation matrix #>       Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8 #> Q1 1.000 0.638 0.683 0.537 0.477 0.484 0.552 0.521 #> Q2 0.638 1.000 0.735 0.503 0.434 0.436 0.557 0.531 #> Q3 0.683 0.735 1.000 0.569 0.499 0.503 0.601 0.570 #> Q4 0.537 0.503 0.569 1.000 0.799 0.839 0.751 0.676 #> Q5 0.477 0.434 0.499 0.799 1.000 0.805 0.670 0.599 #> Q6 0.484 0.436 0.503 0.839 0.805 1.000 0.709 0.633 #> Q7 0.552 0.557 0.601 0.751 0.670 0.709 1.000 0.790 #> Q8 0.521 0.531 0.570 0.676 0.599 0.633 0.790 1.000 round(result_adj$Omega, 3) # adjusted Omega #> Factor1 Factor2 Factor3  #>   0.691   0.683   0.638  round(result_adj$OmegaTotal, 3) # total Omega #> [1] 0.768 print(result_adj$Diagnostics) # communality and uniqueness per item #>    Item Communality Uniqueness PrimaryFactor #> Q1   Q1      0.5325     0.4675       Factor3 #> Q2   Q2      0.6525     0.3475       Factor3 #> Q3   Q3      0.7475     0.2525       Factor3 #> Q4   Q4      0.7850     0.2150       Factor2 #> Q5   Q5      0.7350     0.2650       Factor2 #> Q6   Q6      0.8225     0.1775       Factor2 #> Q7   Q7      0.8350     0.1650       Factor1 #> Q8   Q8      0.6600     0.3400       Factor1  # -------------------------------------------------------- # Example 3: Diagnostics assuming orthogonal factors (Per-Factor Omega) # --------------------------------------------------------  result_orth <- makeCorrLoadings(   loadings = factorLoadings,   diagnostics = TRUE ) #> Diagnostics returned with Per-Factor Omega (assuming orthogonal factors).  round(result_orth$Omega, 3) # per-factor Omega #> Factor1 Factor2 Factor3  #>   0.405   0.513   0.460  round(result_orth$OmegaTotal, 3) # total Omega #> [1] 0.721 print(result_orth$Diagnostics) #>    Item Communality Uniqueness PrimaryFactor #> Q1   Q1      0.5325     0.4675       Factor3 #> Q2   Q2      0.6525     0.3475       Factor3 #> Q3   Q3      0.7475     0.2525       Factor3 #> Q4   Q4      0.7850     0.2150       Factor2 #> Q5   Q5      0.7350     0.2650       Factor2 #> Q6   Q6      0.8225     0.1775       Factor2 #> Q7   Q7      0.8350     0.1650       Factor1 #> Q8   Q8      0.6600     0.3400       Factor1"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeItemsScale.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","title":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","text":"makeItemsScale() generates random dataframe scale items based predefined summated scale (created lfast() function), desired Cronbach's Alpha. scale, lowerbound, upperbound, items, alpha, variance","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeItemsScale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","text":"","code":"makeItemsScale(   scale,   lowerbound,   upperbound,   items,   alpha = 0.8,   variance = 0.5 )"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeItemsScale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","text":"scale (int) vector dataframe summated rating scale. range \\(lowerbound \\times items\\) \\(upperbound \\times items\\) lowerbound (int) lower bound scale item (example: '1' '1' '5' rating) upperbound (int) upper bound scale item (example: '5' '1' '5' rating) items (positive, int) k, number columns generate alpha (posiitve, real) desired Cronbach's Alpha new dataframe items. Default = '0.8'. See @details information alpha parameter variance (positive, real) quantile select items give given summated scores. Must lie '0' '1'. Default = '0.5'. See @details information variance parameter","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeItemsScale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","text":"dataframe 'items' columns 'length(scale)' rows","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeItemsScale.html","id":"alpha","dir":"Reference","previous_headings":"","what":"alpha","title":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","text":"makeItemsScale() takes value vector Likert scales produces row 'k' values average given scale value, rearranges item values within row, attempting give dataframe Likert-scale items produce predefined Cronbach's Alpha. Default value target alpha '0.8'. extreme values 'variance' parameter may reduce chances achieving desired Alpha. may need experiment little.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeItemsScale.html","id":"variance","dir":"Reference","previous_headings":"","what":"variance","title":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","text":"may many ways find combination integers sum specific value, combinations different levels variance: low-variance: '3 + 4 = 7' high-variance: '1 + 6 = 7' 'variance' parameter defines guidelines amount variance among item values new dataframe . example, consider summated value '9' apply makeItemsScale() function generate three items. zero variance (variance parameter = '0'), see items value, mean '3'. variance = '1', see items values give maximum variance among items. Similarly, mean value applied six items makeItemsScale() gives following combinations different values 'variance' parameter. mean value '3.5' gives following combinations. default value 'variance' '0.5' gives reasonable range item values. want 'responses' consistent choose lower variance value.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeItemsScale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate scale items from a summated scale, with desired Cronbach's Alpha — makeItemsScale","text":"","code":"## define parameters k <- 4 lower <- 1 upper <- 5  ## scale properties n <- 64 mean <- 3.0 sd <- 0.85  ## create scale set.seed(42) meanScale <- lfast(   n = n, mean = mean, sd = sd,   lowerbound = lower, upperbound = upper,   items = k ) #> best solution in 2841 iterations summatedScale <- meanScale * k  ## create new items newItems <- makeItemsScale(   scale = summatedScale,   lowerbound = lower, upperbound = upper,   items = k ) #> generate 64 rows #> rearrange 4 values within each of 64 rows #> Complete! #> desired Cronbach's alpha = 0.8 (achieved alpha = 0.8002)  ### test new items # str(newItems) # alpha(data = newItems) |> round(2)   ## very low variance usually gives higher Cronbach's Alpha mydat_20 <- makeItemsScale(   scale = summatedScale,   lowerbound = lower, upperbound = upper,   items = k, alpha = 0.8, variance = 0.20 ) #> generate 64 rows #> rearrange 4 values within each of 64 rows #> Complete! #> desired Cronbach's alpha = 0.8 (achieved alpha = 0.8011)  ### test new data frame # str(mydat_20)  # moments <- data.frame( #   means = apply(mydat_20, MARGIN = 2, FUN = mean) |> round(3), #   sds = apply(mydat_20, MARGIN = 2, FUN = sd) |> round(3) # ) |> t()  # moments  # cor(mydat_20) |> round(2) # alpha(data = mydat_20) |> round(2)   ## default alpha (0.8) and higher variance (0.8) mydat_80 <- makeItemsScale(   scale = summatedScale,   lowerbound = lower, upperbound = upper,   items = k, variance = 0.80 ) #> generate 64 rows #> rearrange 4 values within each of 64 rows #> Complete! #> desired Cronbach's alpha = 0.8 (achieved alpha = 0.8003)  ### test new dataframe # str(mydat_80)  # moments <- data.frame( #   means = apply(mydat_80, MARGIN = 2, FUN = mean) |> round(3), #   sds = apply(mydat_80, MARGIN = 2, FUN = sd) |> round(3) # ) |> t()  # moments  # cor(mydat_80) |> round(2) # alpha(data = mydat_80) |> round(2)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makePaired.html","id":null,"dir":"Reference","previous_headings":"","what":"Synthesise a dataset from paired-sample t-test summary statistics — makePaired","title":"Synthesise a dataset from paired-sample t-test summary statistics — makePaired","text":"function makePaired() generates dataset paired-sample t-test summary statistics. makePaired() generates correlated values data replicate rating scales taken, example, experimental design. function effectively wrapper function lfast() lcor() addition t-statistic -column correlation inferred. Paired t-tests apply observations associated . example: people treatment; people rating two different objects; ratings husband & wife; etc. paired-samples t-test defined : $$ t = \\frac{\\mathrm{mean}(D)}{\\mathrm{sd}(D) / \\sqrt{n}} $$ : \\(D\\) = differences values \\(\\mathrm{mean}(D)\\) = mean differences \\(\\mathrm{sd}(D)\\) = standard deviation differences, $$ \\mathrm{sd}(D)^2 = \\mathrm{sd}(X_{\\text{}})^2 +                           \\mathrm{sd}(X_{\\text{}})^2 -                           2\\,\\mathrm{cov}(X_{\\text{}},                           X_{\\text{}}) $$ paired-sample t-test thus requires estimate covariance two sets observations. makePaired() rearranges formulae covariance inferred t-statistic.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makePaired.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synthesise a dataset from paired-sample t-test summary statistics — makePaired","text":"","code":"makePaired(   n,   means,   sds,   t_value,   lowerbound,   upperbound,   items = 1,   precision = 0 )"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makePaired.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Synthesise a dataset from paired-sample t-test summary statistics — makePaired","text":"n (positive, integer) sample size means (real) 1:2 vector target means two /measures sds (real) 1:2 vector target standard deviations t_value (real) desired paired t-statistic lowerbound (integer) lower bound (e.g. '1' 1-5 rating scale) upperbound (integer) upper bound (e.g. '5' 1-5 rating scale) items (positive, integer) number items rating scale. Default = 1 precision (positive, real) relaxes level accuracy required. Default = 0","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makePaired.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Synthesise a dataset from paired-sample t-test summary statistics — makePaired","text":"dataframe approximating user-specified conditions.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makePaired.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Synthesise a dataset from paired-sample t-test summary statistics — makePaired","text":"Larger sample sizes usually result higher t-statistics, correspondingly small p-values. Small sample sizes relatively large standard deviations relatively high t-statistics can result impossible correlation values. Similarly, large sample sizes low t-statistics can result impossible correlations. , correlation outside -1:+1 range. happens, function fail ERROR message. user review input parameters insert realistic values.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makePaired.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Synthesise a dataset from paired-sample t-test summary statistics — makePaired","text":"","code":"n <- 20 pair_m <- c(2.5, 3.0) pair_s <- c(1.0, 1.5) lower <- 1 upper <- 5 k <- 6 t <- -2.5  pairedDat <- makePaired(   n = n, means = pair_m, sds = pair_s,   t_value = t,   lowerbound = lower, upperbound = upper, items = k ) #> Initial data vectors #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations #> Arranging values to conform with desired t-value #> Complete!  str(pairedDat) #> 'data.frame':\t20 obs. of  2 variables: #>  $ X1: num  3.67 2.5 1.83 1.33 1.33 ... #>  $ X2: num  3.67 3.83 1 1.17 2.33 ... cor(pairedDat) |> round(2) #>      X1   X2 #> X1 1.00 0.82 #> X2 0.82 1.00  t.test(pairedDat$X1, pairedDat$X2, paired = TRUE) #>  #> \tPaired t-test #>  #> data:  pairedDat$X1 and pairedDat$X2 #> t = -2.4863, df = 19, p-value = 0.02238 #> alternative hypothesis: true mean difference is not equal to 0 #> 95 percent confidence interval: #>  -0.90555937 -0.07777397 #> sample estimates: #> mean difference  #>      -0.4916667  #>"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeRepeated.html","id":null,"dir":"Reference","previous_headings":"","what":"Reproduce Repeated-Measures Data from ANOVA Summary Statistics — makeRepeated","title":"Reproduce Repeated-Measures Data from ANOVA Summary Statistics — makeRepeated","text":"Constructs synthetic dataset inter-timepoint correlation matrix repeated-measures ANOVA result, based reported means, standard deviations, F-statistic. useful summary statistics available published studies.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeRepeated.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reproduce Repeated-Measures Data from ANOVA Summary Statistics — makeRepeated","text":"","code":"makeRepeated(   n,   k,   means,   sds,   f_stat,   df_between = k - 1,   df_within = (n - 1) * (k - 1),   structure = c(\"cs\", \"ar1\", \"toeplitz\"),   names = paste0(\"time_\", 1:k),   items = 1,   lowerbound = 1,   upperbound = 5,   return_corr_only = FALSE,   diagnostics = FALSE,   ... )"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeRepeated.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reproduce Repeated-Measures Data from ANOVA Summary Statistics — makeRepeated","text":"n Integer. Sample size used original study. k Integer. Number repeated measures (timepoints). means Numeric vector length k. Mean values reported timepoint. sds Numeric vector length k. Standard deviations reported timepoint. f_stat Numeric. reported repeated-measures ANOVA F-statistic within-subjects factor. df_between Degrees freedom conditions (default: k - 1. df_within Degrees freedom within-subjects (default: (n - 1) * (k - 1)). structure Character. Correlation structure assume: \"cs\", \"ar1\", \"toeplitz\" (default = \"cs\"). names Character vector length k. Variable names timepoint (default: \"time_1\" \"time_k\"). items Integer. Number items used generate scale score (passed lfast). lowerbound, Integer. Lower bounds Likert-type response scales (default: 1). upperbound, Integer. upper bounds Likert-type response scales (default: 5). return_corr_only Logical. TRUE, return estimated correlation matrix. diagnostics Logical. TRUE, include diagnostic summaries feasible F-statistic range effect sizes. ... Reserved future use.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeRepeated.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reproduce Repeated-Measures Data from ANOVA Summary Statistics — makeRepeated","text":"named list components: data data frame simulated repeated-measures responses (unless return_corr_only = TRUE). correlation_matrix estimated inter-timepoint correlation matrix. structure correlation structure assumed. achieved_f F-statistic produced estimated rho value (diagnostics = TRUE). feasible_f_range Minimum maximum achievable F-values chosen structure (shown diagnostics requested). recommended_f Conservative, moderate, strong F-statistic suggestions similar designs. effect_size_raw Unstandardised effect size across timepoints. effect_size_standardised Effect size standardised average variance.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeRepeated.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Reproduce Repeated-Measures Data from ANOVA Summary Statistics — makeRepeated","text":"function estimates average correlation repeated measures matching reported F-statistic, one three assumed correlation structures: \"cs\" (Compound Symmetry): Default. Assumes timepoints equally correlated. Common standard RM-ANOVA settings. \"ar1\" (First-Order Autoregressive): Assumes correlations decay exponentially time lag. \"toeplitz\" (Linearly Decreasing): Assumes correlation declines linearly time lag - middle ground \"cs\" \"ar1\". function generates data frame synthetic item-scale ratings using lfast, adjusts match estimated correlation structure using lcor. Set return_corr_only = TRUE extract estimated correlation matrix.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeRepeated.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reproduce Repeated-Measures Data from ANOVA Summary Statistics — makeRepeated","text":"","code":"set.seed(42)  out1 <- makeRepeated(   n = 64,   k = 3,   means = c(3.1, 3.5, 3.9),   sds = c(1.0, 1.1, 1.0),   items = 4,   f_stat = 4.87,   structure = \"cs\",   diagnostics = FALSE ) #> best solution in 1837 iterations #> best solution in 492 iterations #> best solution in 2136 iterations  head(out1$data) #>   time_1 time_2 time_3 #> 1   3.75   4.75   1.75 #> 2   3.50   2.50   4.75 #> 3   3.25   2.75   4.00 #> 4   2.25   4.25   4.75 #> 5   2.25   4.25   3.50 #> 6   4.00   2.25   4.25 out1$correlation_matrix #>            time_1     time_2     time_3 #> time_1  1.0000000 -0.3100743 -0.3100743 #> time_2 -0.3100743  1.0000000 -0.3100743 #> time_3 -0.3100743 -0.3100743  1.0000000  out2 <- makeRepeated(   n = 32, k = 4,   means = c(2.75, 3.5, 4.0, 4.4),   sds = c(0.8, 1.0, 1.2, 1.0),   f_stat = 16,   structure = \"ar1\",   items = 5,   lowerbound = 1, upperbound = 7,   return_corr_only = FALSE,   diagnostics = TRUE ) #> best solution in 299 iterations #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations #> reached maximum of 1024 iterations  print(out2) #> $data #>    time_1 time_2 time_3 time_4 #> 1     1.8    2.4    1.8    5.8 #> 2     3.4    3.0    5.2    4.0 #> 3     3.4    5.0    5.6    6.4 #> 4     2.4    3.8    6.0    5.4 #> 5     3.0    3.0    2.0    2.8 #> 6     2.4    3.6    3.4    5.4 #> 7     2.2    2.4    4.0    3.4 #> 8     4.2    3.2    2.2    4.0 #> 9     2.2    2.6    4.2    5.2 #> 10    2.6    4.6    6.0    6.0 #> 11    2.2    5.8    3.8    2.2 #> 12    3.4    2.8    4.6    5.4 #> 13    2.0    3.6    3.6    3.8 #> 14    3.8    4.6    4.0    5.2 #> 15    3.2    3.2    4.4    4.6 #> 16    4.8    5.0    5.4    4.0 #> 17    2.0    2.8    4.8    4.6 #> 18    1.8    3.2    3.4    4.2 #> 19    2.2    2.4    3.2    3.8 #> 20    2.2    4.0    4.2    4.4 #> 21    3.4    2.4    4.6    3.4 #> 22    3.2    4.0    5.8    5.6 #> 23    3.6    2.8    1.6    3.6 #> 24    2.4    4.8    4.8    4.6 #> 25    2.8    3.2    2.6    5.0 #> 26    1.6    2.2    3.0    3.2 #> 27    2.2    3.4    3.6    5.0 #> 28    2.4    3.6    3.6    4.8 #> 29    2.0    2.2    4.8    4.0 #> 30    4.0    5.4    5.0    4.4 #> 31    2.0    2.6    4.0    3.2 #> 32    3.2    4.2    2.8    3.2 #>  #> $correlation_matrix #>            time_1    time_2    time_3     time_4 #> time_1 1.00000000 0.3910032 0.1528835 0.05977794 #> time_2 0.39100319 1.0000000 0.3910032 0.15288350 #> time_3 0.15288350 0.3910032 1.0000000 0.39100319 #> time_4 0.05977794 0.1528835 0.3910032 1.00000000 #>  #> $structure #> [1] \"ar1\" #>  #> $feasible_f_range #>       min       max  #>  9.353034 39.481390  #>  #> $recommended_f #> $recommended_f$conservative #> [1] 10.21 #>  #> $recommended_f$moderate #> [1] 11.91 #>  #> $recommended_f$strong #> [1] 30.29 #>  #>  #> $achieved_f #> [1] 15.99983 #>  #> $effect_size_raw #> [1] 0.3792188 #>  #> $effect_size_standardised #> [1] 0.3717831 #>    out3 <- makeRepeated(   n = 64, k = 4,   means = c(2.0, 2.25, 2.75, 3.0),   sds = c(0.8, 0.9, 1.0, 0.9),   items = 4,   f_stat = 24,   # structure = \"toeplitz\",   diagnostics = TRUE ) #> best solution in 3541 iterations #> best solution in 2048 iterations #> reached maximum of 4096 iterations #> best solution in 676 iterations  str(out3) #> List of 8 #>  $ data                    :'data.frame':\t64 obs. of  4 variables: #>   ..$ time_1: num [1:64] 2.75 1 1.75 1.25 1.5 1.5 2.75 1.5 2 2.5 ... #>   ..$ time_2: num [1:64] 4 1.25 3.25 1.75 1.5 2.25 3.25 2 1.5 4.5 ... #>   ..$ time_3: num [1:64] 3.75 1 1.75 3.25 2.5 3.5 4.25 4 3.25 4 ... #>   ..$ time_4: num [1:64] 4 1.25 3.75 4 3.75 4 3.25 3 3.75 3.5 ... #>  $ correlation_matrix      : num [1:4, 1:4] 1 0.489 0.489 0.489 0.489 ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:4] \"time_1\" \"time_2\" \"time_3\" \"time_4\" #>   .. ..$ : chr [1:4] \"time_1\" \"time_2\" \"time_3\" \"time_4\" #>  $ structure               : chr \"cs\" #>  $ feasible_f_range        : Named num [1:2] 9.27 61.35 #>   ..- attr(*, \"names\")= chr [1:2] \"min\" \"max\" #>  $ recommended_f           :List of 3 #>   ..$ conservative: num 11.6 #>   ..$ moderate    : num 16.1 #>   ..$ strong      : num 46.3 #>  $ achieved_f              : num 24 #>  $ effect_size_raw         : num 0.156 #>  $ effect_size_standardised: num 0.192"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScales.html","id":null,"dir":"Reference","previous_headings":"","what":"Synthesise rating-scale data with given first and second moments and a predefined correlation matrix — makeScales","title":"Synthesise rating-scale data with given first and second moments and a predefined correlation matrix — makeScales","text":"makeScales() generates dataframe random discrete values data replicate rating scale, correlated close predefined correlation matrix. makeScales() wrapper function : lfast(), generates dataframe best fits desired moments, lcor(), rearranges values column dataframe closely match desired correlation matrix.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScales.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synthesise rating-scale data with given first and second moments and a predefined correlation matrix — makeScales","text":"","code":"makeScales(n, means, sds, lowerbound = 1, upperbound = 5, items = 1, cormatrix)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScales.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Synthesise rating-scale data with given first and second moments and a predefined correlation matrix — makeScales","text":"n (positive, int) sample-size - number observations means (real) target means: vector length k mean values scale item sds (positive, real) target standard deviations: vector length k standard deviation values scale item lowerbound (positive, int) vector length k (rows & columns correlation matrix) values lower bound scale item (e.g. '1' 1-5 rating scale). Default = 1. upperbound (positive, int) vector length k (rows & columns correlation matrix) values upper bound scale item (e.g. '5' 1-5 rating scale). Default = 5. items (positive, int) vector length k number items scale. Default = 1. cormatrix (real, matrix) target correlation matrix: square symmetric positive-semi-definite matrix values ranging -1 +1, '1' diagonal.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScales.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Synthesise rating-scale data with given first and second moments and a predefined correlation matrix — makeScales","text":"dataframe rating-scale values","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScales.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Synthesise rating-scale data with given first and second moments and a predefined correlation matrix — makeScales","text":"","code":"## Example 1: four correlated items (questions)  ### define parameters  n <- 16 dfMeans <- c(2.5, 3.0, 3.0, 3.5) dfSds <- c(1.0, 1.0, 1.5, 0.75) lowerbound <- rep(1, 4) upperbound <- rep(5, 4)  corMat <- matrix(   c(     1.00, 0.30, 0.40, 0.60,     0.30, 1.00, 0.50, 0.70,     0.40, 0.50, 1.00, 0.80,     0.60, 0.70, 0.80, 1.00   ),   nrow = 4, ncol = 4 )  scale_names <- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\") rownames(corMat) <- scale_names colnames(corMat) <- scale_names  ### apply function  df1 <- makeScales(   n = n, means = dfMeans, sds = dfSds,   lowerbound = lowerbound, upperbound = upperbound, cormatrix = corMat ) #> Variable  1 :  Q1  -  #> reached maximum of 1024 iterations #> Variable  2 :  Q2  -  #> reached maximum of 1024 iterations #> Variable  3 :  Q3  -  #> reached maximum of 1024 iterations #> Variable  4 :  Q4  -  #> reached maximum of 1024 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables #>   ### test function  str(df1) #> 'data.frame':\t16 obs. of  4 variables: #>  $ Q1: num  3 3 4 4 1 2 2 4 2 2 ... #>  $ Q2: num  4 4 4 3 2 4 2 2 3 2 ... #>  $ Q3: num  3 5 5 4 2 3 1 1 2 3 ... #>  $ Q4: num  4 4 5 4 3 4 2 3 3 3 ...  #### means apply(df1, 2, mean) |> round(3) #>  Q1  Q2  Q3  Q4  #> 2.5 3.0 3.0 3.5   #### standard deviations apply(df1, 2, sd) |> round(3) #>    Q1    Q2    Q3    Q4  #> 1.033 1.033 1.506 0.730   #### correlations cor(df1) |> round(3) #>       Q1    Q2    Q3    Q4 #> Q1 1.000 0.313 0.386 0.619 #> Q2 0.313 1.000 0.514 0.707 #> Q3 0.386 0.514 1.000 0.728 #> Q4 0.619 0.707 0.728 1.000    ## Example 2: five correlated Likert scales  ### a study on employee engagement and organizational climate: # Job Satisfaction (JS) # Organizational Commitment (OC) # Perceived Supervisor Support (PSS) # Work Engagement (WE) # Turnover Intention (TI) (reverse-related to others).  ### define parameters  n <- 128 dfMeans <- c(3.8, 3.6, 3.7, 3.9, 2.2) dfSds <- c(0.7, 0.8, 0.7, 0.6, 0.9) lowerbound <- rep(1, 5) upperbound <- rep(5, 5) items <- c(4, 4, 3, 3, 3)  corMat <- matrix(   c(     1.00, 0.72, 0.58, 0.65, -0.55,     0.72, 1.00, 0.54, 0.60, -0.60,     0.58, 0.54, 1.00, 0.57, -0.45,     0.65, 0.60, 0.57, 1.00, -0.50,     -0.55, -0.60, -0.45, -0.50, 1.00   ),   nrow = 5, ncol = 5 )  scale_names <- c(\"JS\", \"OC\", \"PSS\", \"WE\", \"TI\") rownames(corMat) <- scale_names colnames(corMat) <- scale_names  ### apply function  df2 <- makeScales(   n = n, means = dfMeans, sds = dfSds,   lowerbound = lowerbound, upperbound = upperbound,   items = items, cormatrix = corMat ) #> Variable  1 :  JS  -  #> best solution in 88 iterations #> Variable  2 :  OC  -  #> best solution in 1334 iterations #> Variable  3 :  PSS  -  #> best solution in 1443 iterations #> Variable  4 :  WE  -  #> best solution in 623 iterations #> Variable  5 :  TI  -  #> best solution in 703 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables #>   ### test function  str(df2) #> 'data.frame':\t128 obs. of  5 variables: #>  $ JS : num  3.75 4 2.5 4 3.25 3.5 3.75 4.75 4.75 3.75 ... #>  $ OC : num  3 4 2 3.5 3.5 2.75 4 4.25 4.5 4 ... #>  $ PSS: num  3.67 4.33 2.67 4 2.33 ... #>  $ WE : num  3.67 4.67 3 2.67 2.33 ... #>  $ TI : num  2.33 1.33 4 1.33 2 ...  #### means apply(df2, 2, mean) |> round(3) #>    JS    OC   PSS    WE    TI  #> 3.799 3.602 3.701 3.901 2.201   #### standard deviations apply(df2, 2, sd) |> round(3) #>    JS    OC   PSS    WE    TI  #> 0.699 0.800 0.701 0.599 0.898   #### correlations cor(df2) |> round(3) #>        JS     OC    PSS     WE     TI #> JS   1.00  0.720  0.580  0.650 -0.550 #> OC   0.72  1.000  0.541  0.601 -0.600 #> PSS  0.58  0.541  1.000  0.570 -0.450 #> WE   0.65  0.601  0.570  1.000 -0.501 #> TI  -0.55 -0.600 -0.450 -0.501  1.000"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScalesRegression.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Data from Multiple-Regression Summary Statistics — makeScalesRegression","title":"Generate Data from Multiple-Regression Summary Statistics — makeScalesRegression","text":"Generates synthetic rating-scale data replicates reported regression results. function useful reproducing analyses published research summary statistics (standardised regression coefficients R-squared) reported.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScalesRegression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Data from Multiple-Regression Summary Statistics — makeScalesRegression","text":"","code":"makeScalesRegression(   n,   beta_std,   r_squared,   iv_cormatrix = NULL,   iv_cor_mean = 0.3,   iv_cor_variance = 0.01,   iv_cor_range = c(-0.7, 0.7),   iv_means,   iv_sds,   dv_mean,   dv_sd,   lowerbound_iv,   upperbound_iv,   lowerbound_dv,   upperbound_dv,   items_iv = 1,   items_dv = 1,   var_names = NULL,   tolerance = 0.005 )"},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScalesRegression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Data from Multiple-Regression Summary Statistics — makeScalesRegression","text":"n Integer. Sample size beta_std Numeric vector standardised regression coefficients (length k) r_squared Numeric. R-squared regression (-1 1) iv_cormatrix k x k correlation matrix independent variables. missing (NULL), optimised. iv_cor_mean Numeric. Mean correlation among IVs optimising (ignored iv_cormatrix provided). Default = 0.3 iv_cor_variance Numeric. Variance correlations optimising (ignored iv_cormatrix provided). Default = 0.01 iv_cor_range Numeric vector length 2. Min max constraints correlations optimising. Default = c(-0.7, 0.7) iv_means Numeric vector means IVs (length k) iv_sds Numeric vector standard deviations IVs (length k) dv_mean Numeric. Mean dependent variable dv_sd Numeric. Standard deviation dependent variable lowerbound_iv Numeric vector lower bounds IV scale (single value ) upperbound_iv Numeric vector upper bounds IV scale (single value ) lowerbound_dv Numeric. Lower bound DV scale upperbound_dv Numeric. Upper bound DV scale items_iv Integer vector number items per IV scale (single value ). Default = 1 items_dv Integer. Number items DV scale. Default = 1 var_names Character vector variable names (length k+1: IVs DV) tolerance Numeric. Acceptable deviation target R-squared (default 0.005)","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScalesRegression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Data from Multiple-Regression Summary Statistics — makeScalesRegression","text":"list containing: data Generated dataframe k IVs 1 DV target_stats List target statistics provided achieved_stats List achieved statistics generated data diagnostics Comparison target vs achieved iv_dv_cors Calculated correlations IVs DV full_cormatrix complete (k+1) x (k+1) correlation matrix used optimisation_info IV correlations optimised, details optimisation","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScalesRegression.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Data from Multiple-Regression Summary Statistics — makeScalesRegression","text":"Generate regression data summary statistics function can operate two modes: Mode 1: IV correlation matrix provided iv_cormatrix provided, function uses given correlation structure among independent variables calculates implied IV-DV correlations regression coefficients. Mode 2: optimisation (IV correlation matrix provided) iv_cormatrix = NULL, function optimises find plausible correlation structure among independent variables matches reported regression statistics. Initial correlations sampled using Fisher's z-transformation ensure proper distribution, iteratively adjusted match target R-squared. function generates Likert-scale data (individual items) using lfast() variable specified moments, correlates using lcor(). Generated data verified running regression comparing achieved statistics targets.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/reference/makeScalesRegression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Data from Multiple-Regression Summary Statistics — makeScalesRegression","text":"","code":"# Example 1: With provided IV correlation matrix set.seed(123) iv_corr <- matrix(c(1.0, 0.3, 0.3, 1.0), nrow = 2)  result1 <- makeScalesRegression(   n = 64,   beta_std = c(0.4, 0.3),   r_squared = 0.35,   iv_cormatrix = iv_corr,   iv_means = c(3.0, 3.5),   iv_sds = c(1.0, 0.9),   dv_mean = 3.8,   dv_sd = 1.1,   lowerbound_iv = 1,   upperbound_iv = 5,   lowerbound_dv = 1,   upperbound_dv = 5,   items_iv = 4,   items_dv = 4,   var_names = c(\"Attitude\", \"Intention\", \"Behaviour\") ) #> Warning: Predicted R-squared (0.3220) differs from target (0.3500) by 0.0280, #>         which exceeds tolerance (0.0050). #>          #> Input statistics may be inconsistent. #> best solution in 397 iterations #> best solution in 1507 iterations #> best solution in 162 iterations  print(result1) #> Regression Data Generation Results #> =================================== #>  #> Sample size: 64  #> Number of IVs: 2  #>  #> IV Correlation Matrix: PROVIDED #>  #> Key Statistics: #> --------------- #> Target R-squared:   0.3500 #> Achieved R-squared: 0.3223 #> Difference:         -0.0277 #>  #> Regression Coefficients (Standardised): #>   Variable Target Achieved   Diff #>   Attitude    0.4   0.3997 -3e-04 #>  Intention    0.3   0.3006  6e-04 #>  #> For full diagnostics, see $diagnostics #> For generated data, see $data head(result1$data) #>   Attitude Intention Behaviour #> 1     3.00      1.75      1.50 #> 2     4.00      5.00      2.75 #> 3     1.75      2.75      2.25 #> 4     2.75      3.00      4.50 #> 5     2.75      3.00      4.50 #> 6     3.00      2.00      3.75   # Example 2: With optimisation (no IV correlation matrix) set.seed(456) result2 <- makeScalesRegression(   n = 128,   beta_std = c(0.3, 0.25, 0.2),   r_squared = 0.40,   iv_cormatrix = NULL, # Will be optimised   iv_cor_mean = 0.3,   iv_cor_variance = 0.02,   iv_means = c(3.0, 3.2, 2.8),   iv_sds = c(1.0, 0.9, 1.1),   dv_mean = 3.5,   dv_sd = 1.0,   lowerbound_iv = 1,   upperbound_iv = 5,   lowerbound_dv = 1,   upperbound_dv = 5,   items_iv = 4,   items_dv = 5 ) #> IV correlation matrix not provided. #>              #> Optimising to find plausible structure... #> Optimisation converged after 7 iterations #>        #> (R-sq target: 0.4000, achieved in optimisation: 0.4020) #> best solution in 5083 iterations #> best solution in 669 iterations #> best solution in 1675 iterations #> best solution in 423 iterations  # View optimised correlation matrix print(result2$target_stats$iv_cormatrix) #>           [,1]      [,2]      [,3] #> [1,] 1.0000000 0.4117389 0.6615179 #> [2,] 0.4117389 1.0000000 0.6832584 #> [3,] 0.6615179 0.6832584 1.0000000 print(result2$optimisation_info) #> $converged #> [1] TRUE #>  #> $iterations #> [1] 7 #>  #> $achieved_r_squared_in_optimisation #> [1] 0.4019688 #>  #> $iv_cor_mean_used #> [1] 0.3 #>  #> $iv_cor_variance_used #> [1] 0.02 #>  #> $iv_cor_range_used #> [1] -0.7  0.7 #>"},{"path":"https://winzarh.github.io/LikertMakeR/reference/ordinal_diagnostics.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract ordinal diagnostics from a reliability() result — ordinal_diagnostics","title":"Extract ordinal diagnostics from a reliability() result — ordinal_diagnostics","text":"Extract ordinal diagnostics reliability() result","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/ordinal_diagnostics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract ordinal diagnostics from a reliability() result — ordinal_diagnostics","text":"","code":"ordinal_diagnostics(x)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/ordinal_diagnostics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract ordinal diagnostics from a reliability() result — ordinal_diagnostics","text":"x object returned reliability().","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/ordinal_diagnostics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract ordinal diagnostics from a reliability() result — ordinal_diagnostics","text":"data.frame describing observed response categories sparsity checks, NULL diagnostics available.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/print.likert_reliability.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for reliability objects — print.likert_reliability","title":"Print method for reliability objects — print.likert_reliability","text":"Print method reliability objects","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/print.likert_reliability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for reliability objects — print.likert_reliability","text":"","code":"# S3 method for class 'likert_reliability' print(x, ...)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/print.likert_reliability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for reliability objects — print.likert_reliability","text":"x object returned reliability(). ... Unused.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/print.makeScalesRegression.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for makeScalesRegression objects — print.makeScalesRegression","title":"Print method for makeScalesRegression objects — print.makeScalesRegression","text":"Print method makeScalesRegression objects","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/print.makeScalesRegression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for makeScalesRegression objects — print.makeScalesRegression","text":"","code":"# S3 method for class 'makeScalesRegression' print(x, ...)"},{"path":"https://winzarh.github.io/LikertMakeR/reference/print.makeScalesRegression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for makeScalesRegression objects — print.makeScalesRegression","text":"x object class \"makeScalesRegression\" ... Additional arguments (currently unused)","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/reliability.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate scale reliability for Likert and rating-scale data — reliability","title":"Estimate scale reliability for Likert and rating-scale data — reliability","text":"Computes internal consistency reliability estimates single-factor scale, including Cronbach’s alpha, McDonald’s omega (total), optional ordinal (polychoric-based) variants. Confidence intervals may obtained via nonparametric bootstrap.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/reliability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate scale reliability for Likert and rating-scale data — reliability","text":"","code":"reliability(   data,   include = \"none\",   ci = FALSE,   ci_level = 0.95,   n_boot = 1000,   na_method = c(\"pairwise\", \"listwise\"),   min_count = 2,   digits = 3,   verbose = TRUE )"},{"path":"https://winzarh.github.io/LikertMakeR/reference/reliability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate scale reliability for Likert and rating-scale data — reliability","text":"data data frame matrix containing item responses. column represents one item; rows represent respondents. include Character vector specifying additional estimates compute. Possible values : \"none\" (default): Pearson-based alpha omega . \"lambda6\": Include Guttman’s lambda-6 (requires package psych). \"polychoric\": Include ordinal (polychoric-based) alpha omega. Multiple options may supplied. ci Logical; TRUE, confidence intervals computed using nonparametric bootstrap. Default FALSE. ci_level Confidence level bootstrap intervals. Default 0.95. n_boot Number bootstrap resamples used ci = TRUE. Default 1000. na_method Method handling missing values. Either \"pairwise\" (default) \"listwise\". min_count Minimum observed frequency per response category required attempt polychoric correlations. Ordinal reliability estimates skipped condition violated. Default 2. digits Number decimal places used printing estimates. Default 3. verbose Logical; TRUE, warnings progress indicators displayed. Default TRUE.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/reliability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate scale reliability for Likert and rating-scale data — reliability","text":"tibble one row per reliability coefficient columns: coef_name: Name reliability coefficient. estimate: Point estimate. ci_lower, ci_upper: Confidence interval bounds (present ci = TRUE). notes: Methodological notes describing estimate obtained. returned object class \"likert_reliability\" includes additional attributes containing diagnostics bootstrap information.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/reference/reliability.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate scale reliability for Likert and rating-scale data — reliability","text":"function designed Likert-type rating-scale data prioritises transparent diagnostics ordinal reliability estimates feasible due sparse response categories. Cronbach’s alpha McDonald’s omega computed Pearson correlations. include = \"polychoric\", ordinal reliability estimates computed using polychoric correlations correspond Zumbo’s alpha ordinal omega. Ordinal reliability estimates skipped response categories sparse polychoric estimation fails. Diagnostics explaining decisions stored returned object may inspected using ordinal_diagnostics. function assumes single common factor intended multidimensional structural equation modelling contexts.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/reference/reliability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate scale reliability for Likert and rating-scale data — reliability","text":"","code":"## create dataset my_cor <- LikertMakeR::makeCorrAlpha(   items = 4,   alpha = 0.80 ) #> reached max iterations (1600) - best mean difference: 1.1e-05  my_data <- LikertMakeR::makeScales(   n = 64,   means = c(2.75, 3.00, 3.25, 3.50),   sds = c(1.25, 1.50, 1.30, 1.25),   lowerbound = rep(1, 4),   upperbound = rep(5, 4),   cormatrix = my_cor ) #> Variable  1 :  item01  -  #> reached maximum of 4096 iterations #> Variable  2 :  item02  -  #> best solution in 2657 iterations #> Variable  3 :  item03  -  #> reached maximum of 4096 iterations #> Variable  4 :  item04  -  #> reached maximum of 4096 iterations #>  #> Arranging data to match correlations #>  #> Successfully generated correlated variables #>   ## run function reliability(my_data) #>    coef_name estimate n_items n_obs                notes #>        alpha    0.801       4    64 Pearson correlations #>  omega_total    0.872       4    64 1-factor eigen omega  reliability(   my_data,   include = c(\"lambda6\", \"polychoric\") ) #>            coef_name estimate n_items n_obs #>                alpha    0.801       4    64 #>          omega_total    0.872       4    64 #>              lambda6    0.797       4    64 #>        ordinal_alpha    0.765       4    64 #>  ordinal_omega_total    0.851       4    64 #>                                                notes #>                                 Pearson correlations #>                                 1-factor eigen omega #>                                       psych::alpha() #>                              Polychoric correlations #>  Polychoric correlations | Ordinal CIs not requested  # \\donttest{ ## slower (not run on CRAN checks) reliability(   my_data,   include = \"polychoric\",   ci = TRUE,   n_boot = 200 ) #>            coef_name estimate ci_lower ci_upper n_items n_obs #>                alpha    0.801    0.672    0.871       4    64 #>          omega_total    0.872    0.805    0.912       4    64 #>        ordinal_alpha    0.765    0.607    0.818       4    64 #>  ordinal_omega_total    0.851    0.775    0.880       4    64 #>                                                notes #>                                 Pearson correlations #>                                 1-factor eigen omega #>                              Polychoric correlations #>  Polychoric correlations | Ordinal CIs via bootstrap # }"},{"path":[]},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"improvements-1-4-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LikertMakeR 1.4.0 (December 2025)","text":"New reliability() function: Calculates various reliability statistics: Cronbach’s alpha (α\\alpha) McDonald’s omega-total (ωt\\omega_t) Guttman’s lambda 6 (λ6\\lambda 6) Zumbo’s ordinal alpha (Cronbach’s alpha polychoric correlations) ordinal-omega-total (McDonald’s omega total polychoric correlations) coefficient H bootstrapped Confidence Intervals option.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"maintenance-1-4-0","dir":"Changelog","previous_headings":"","what":"Maintenance","title":"LikertMakeR 1.4.0 (December 2025)","text":"Fixed typos simplified examples. removed redundant makeItems() function reduced length README file moved content vignettes","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-130-2025-11-24","dir":"Changelog","previous_headings":"","what":"LikertMakeR 1.3.0 (2025-11-24)","title":"LikertMakeR 1.3.0 (2025-11-24)","text":"CRAN release: 2025-11-26","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"improvements-1-3-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LikertMakeR 1.3.0 (2025-11-24)","text":"New makeScalesRegression() function: Generates synthetic rating-scale data replicates reported regression results, returns data frame provides requested statistical properties correlation matrix summary moments data frame, plus diagnostic statistics, including comparison target values achieved values. makeScales() function replaces makeItems() function: finally worked turn single value vector length k. Embarrassingly straightforward. Updated version makeCorrAlpha() function produces “natural-looking” correlation matrix, plus diagnostics: previous version sorted correlations correlation matrix improve likelihood extracting positive-definite matrix. Fast, unnatural results. applied slightly faster algorithm rearranging correlations draft matrix produce one positive-definite. Additional parameter sort_cors = FALSE. TRUE, results similar earlier version makeCorrAlpha(). little faster “natural-looking”. Additional parameter diagnostics = FALSE. TRUE, returns list containing correlation matrix diagnostics list (target/achieved alpha, average inter-item correlation, eigenvalues, PD flag, key arguments). FALSE (default), returns correlation matrix . Updated version lfast() function runs slightly faster","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"maintenance-1-3-0","dir":"Changelog","previous_headings":"","what":"Maintenance","title":"LikertMakeR 1.3.0 (2025-11-24)","text":"new vignette new function makeScalesRegression(). updated examples makeScales() function. updated badges readme file.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-120-2025-10-10","dir":"Changelog","previous_headings":"","what":"LikertMakeR 1.2.0 (2025-10-10)","title":"LikertMakeR 1.2.0 (2025-10-10)","text":"CRAN release: 2025-10-09","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"improvements-1-2-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LikertMakeR 1.2.0 (2025-10-10)","text":"New makeRepeated() function : takes summary statistics reported typical repeated-measures ANOVA study, returns correlation matrix vectors repeated measures data frame based correlation matrix summary moments, plus diagnostic statistics, including possible F-statistics based information provided. #lfast_validation# vignette shows #LikertMaker# remarkably good job replicating real rating-scale data.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"maintenance-1-2-0","dir":"Changelog","previous_headings":"","what":"Maintenance","title":"LikertMakeR 1.2.0 (2025-10-10)","text":"Vignettes large many images, CRAN files include #LikertMakeR_vignette# file. Two vignettes validate lfast() makeCorrLoadings() appear package website.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-110-2025-05-26","dir":"Changelog","previous_headings":"","what":"LikertMakeR 1.1.0 (2025-05-26)","title":"LikertMakeR 1.1.0 (2025-05-26)","text":"CRAN release: 2025-05-30","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"improvements-1-1-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LikertMakeR 1.1.0 (2025-05-26)","text":"new makePaired() function: takes summary statistics paired-sample t-test produces data frame rating-scale data deliver summary statistics lcor() function rewrite: previous version used systematic swapping values column minimise difference data correlation target correlation matrix. algorithm effect causing extreme values column highly-correlated (lowly correlated applicable), leaving middle-values relatively uncorrelated. property probably noticeable cases apparent range scale values great.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"maintenance-1-1-0","dir":"Changelog","previous_headings":"","what":"Maintenance","title":"LikertMakeR 1.1.0 (2025-05-26)","text":"Vignettes minor updates.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"improvements-1-0-2","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LikertMakeR 1.0.2 (2025-04-25)","text":"test examples updated.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"maintenance-1-0-2","dir":"Changelog","previous_headings":"","what":"Maintenance","title":"LikertMakeR 1.0.2 (2025-04-25)","text":"Vignettes updated.","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"improvements-1-0-1","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LikertMakeR 1.0.1 (2025-04-07)","text":"Vignettes now properly registered included build. LikertMakeR vignette makeCorrLoadings validation Updated DESCRIPTION metadata comply CRAN requirements.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"maintenance-1-0-1","dir":"Changelog","previous_headings":"","what":"Maintenance","title":"LikertMakeR 1.0.1 (2025-04-07)","text":"Switched vignette engine knitr::rmarkdown better compatibility CRAN development tools.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-100-2025-04-03","dir":"Changelog","previous_headings":"","what":"LikertMakeR 1.0.0 (2025-04-03)","title":"LikertMakeR 1.0.0 (2025-04-03)","text":"CRAN release: 2025-04-04","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"makecorrloadings-function-added-1-0-0","dir":"Changelog","previous_headings":"","what":"makeCorrLoadings() function added","title":"LikertMakeR 1.0.0 (2025-04-03)","text":"makeCorrLoadings() generates correlation matrix inter-item correlations based item factor loadings might seen Exploratory Factor Analysis (EFA) Structural Equation Model (SEM). correlation matrix can applied  function generate synthetic data predefined factor structures.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-100-2025-01-08","dir":"Changelog","previous_headings":"","what":"LikertMakeR 1.0.0 (2025-01-08)","title":"LikertMakeR 1.0.0 (2025-01-08)","text":"CRAN release: 2025-04-04","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"update-version-number-to-correct-majorminorpatch-format-1-0-0","dir":"Changelog","previous_headings":"","what":"update version number to correct major.minor.patch format","title":"LikertMakeR 1.0.0 (2025-01-08)","text":"update V 0.4.5. new numbered submission CRAN","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"makepaired-function-added-0-4-5","dir":"Changelog","previous_headings":"","what":"makePaired() function added","title":"LikertMakeR 0.4.5 (2025-01-07)","text":"makePaired() generates dataframe two paired vectors emulate data paired-sample t-test","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-040-2024-11-17","dir":"Changelog","previous_headings":"","what":"LikertMakeR 0.4.0 (2024-11-17)","title":"LikertMakeR 0.4.0 (2024-11-17)","text":"CRAN release: 2024-11-19","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"target-cronbachs-alpha-added-to-makeitemsscale-function-0-4-0","dir":"Changelog","previous_headings":"","what":"target Cronbach’s Alpha added to makeItemsScale() function","title":"LikertMakeR 0.4.0 (2024-11-17)","text":"generated scale items now defined target Cronbach’s Alpha, well variance within scale item. latest version adds little randomness selection candidate row vectors.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-030-2024-05-18","dir":"Changelog","previous_headings":"","what":"LikertMakeR 0.3.0 (2024-05-18)","title":"LikertMakeR 0.3.0 (2024-05-18)","text":"CRAN release: 2024-05-19","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"more-randomness-in-swaps-task-to-makecorralpha-function-0-3-0","dir":"Changelog","previous_headings":"","what":"more randomness in swaps task to makeCorrAlpha() function","title":"LikertMakeR 0.3.0 (2024-05-18)","text":"correlation matrix usually values sorted lowest highest. happens less often","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"added-precision-parameter-to-makecorralpha-function-0-2-6","dir":"Changelog","previous_headings":"","what":"added ‘precision’ parameter to makeCorrAlpha() function","title":"LikertMakeR 0.2.6 (2024-05-11)","text":"‘precision’ adds random variation around target Cronbach’s Alpha. Default = ‘0’ (variation giving Alpha exact two decimal places)","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"added-correlatescales-function-0-2-5","dir":"Changelog","previous_headings":"","what":"added correlateScales() function","title":"LikertMakeR 0.2.5 (2024-04-20)","text":"Create dataframe correlated scales different dataframes scale items","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"added-makeitemsscale-function-0-2-2","dir":"Changelog","previous_headings":"","what":"added makeItemsScale() function","title":"LikertMakeR 0.2.2 (2024-03-31)","text":"Generate rating-scale items given summated scale","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-020-2024-03-02","dir":"Changelog","previous_headings":"","what":"LikertMakeR 0.2.0 (2024-03-02)","title":"LikertMakeR 0.2.0 (2024-03-02)","text":"CRAN release: 2024-03-02","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"for-submission-to-cran-0-2-0","dir":"Changelog","previous_headings":"","what":"For submission to CRAN","title":"LikertMakeR 0.2.0 (2024-03-02)","text":"Faster accurate functions: lcor() & lfast() replace old lcor() & lfast() previous lcor_C() & lfast_R()","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"added-a-new-functions-makecorralpha-makeitems-alpha-eigenvalues-0-1-9","dir":"Changelog","previous_headings":"","what":"Added a new functions: makeCorrAlpha(), makeItems(), alpha(), eigenvalues()","title":"LikertMakeR 0.1.9 (2024-02-11)","text":"makeCorrAlpha() constructs random correlation matrix given dimensions predefined Cronbach’s Alpha. makeItems() generates synthetic rating-scale data predefined first second moments predefined correlation matrix alpha() calculate Cronbach’s Alpha given correlation matrix given dataframe eigenvalues() calculates eigenvalues correlation matrix optional scree plot","code":""},{"path":[]},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"added-a-new-function-lcor_c-0-1-7","dir":"Changelog","previous_headings":"","what":"Added a new function: lcor_C()","title":"LikertMakeR 0.1.7 (2024-02-02)","text":"lcor_C() C++ implementation lcor() function. run considerably faster lcor(). ’m confident lcor_C() works well better lcor(), shall replace lcor() C++ implementation update CRAN.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-016-2024-01-18","dir":"Changelog","previous_headings":"","what":"LikertMakeR 0.1.6 (2024-01-18)","title":"LikertMakeR 0.1.6 (2024-01-18)","text":"Made code examples tidy - makes code nanoseconds faster Added -line comments. setting C++ mods make lcor() faster, introduce make_items() function.","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"likertmaker-015-2022-12-20","dir":"Changelog","previous_headings":"","what":"LikertMakeR 0.1.5 (2022-12-20)","title":"LikertMakeR 0.1.5 (2022-12-20)","text":"CRAN release: 2022-12-22","code":""},{"path":"https://winzarh.github.io/LikertMakeR/news/index.html","id":"initial-cran-release-0-1-5","dir":"Changelog","previous_headings":"","what":"Initial CRAN release","title":"LikertMakeR 0.1.5 (2022-12-20)","text":"Added references DESCRIPTION file expanded citations vignettes Reduced runtime setting target zero instead -Inf. Specified one thread instead attempting Parallel","code":""}]
