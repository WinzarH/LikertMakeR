---
title: "LikertMakeR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LikertMakeR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(LikertMakeR)
```

# *LikertMakeR*

**_LikertMakeR_** synthesises Likert scale and related rating-scale data. Such scales are constrained by upper and lower bounds and discrete increments. For example, a likelihood-of-purchase scale may be an 11-point, zero-to-10 rating scale.

The package generates a column of values that simulate the same properties as a rating scale. If multiple columns are generated, then you can use **_LikertMakeR_** to rearrange the values so that the new variables are correlated exactly in accord with a user-predefined correlation matrix.

## Purpose

The package should be useful for teaching in the Social Sciences, and for scholars who wish to "replicate" data for further analysis and visualisation when only summary statistics have been reported.

I was prompted to write the functions in the _LikertMakeR_ when I reviewed a submission for a journal, and the authors had presented their questionnaire results very poorly. By the third submission, clearly they needed an example. So I tried to generate some data with similar properties as they had reported. Contrary to the authors' conclusions, I found that the large variance in the data caused responses to pile up at the boundaries - there was a majority that agreed with a proposition, and a small, but important, minority that strongly disagreed. The simple finding caused a more nuanced discussion which, of course, could have been much earlier if the researchers had taken the trouble to look at their data.

## Rating scale properties

A Likert scale is the mean, or sum, of several ordinal rating scales. They are bipolar (usually "agree-disagree") responses to propositions that are determined to be moderately-to-highly correlated and capturing various facets of a construct.

Rating scales, such as Likert scales, are not continuous or unbounded.

For example, a 5-point Likert scale that is constructed with, say, five items (questions) will have a summed range of between 5 (all rated '1') and 25 (all rated '5') with all integers in between, and the mean range will be '1' to '5' with intervals of 1/5=0.20. A 7-point Likert scale constructed from eight items will have a summed range between 8 (all rated '1') and 56 (all rated '7') with all integers in between, and the mean range will be '1' to '7' with intervals of 1/8=0.125.

Rating-scale boundaries define minima and maxima for any scale values. If the mean is close to one boundary then data points will gather more closely to that boundary and the data will always be skewed.

## Alternative packages

This package is intended for synthesising & correlating rating-scale data with means, standard deviations, and correlations as close as possible to predefined parameters. If you don't need your data to be close to exact, then there are other options. 

For example, researchers and teachers may simulate rating scales ahead of finalising a questionnaire or demonstrating analyses. 

Different approaches are:

* sampling from a _truncated normal_ distribution. Data are sampled from a normal distribution, and then truncated to suit the rating-scale boundaries, and rounded to set discrete values as we see in rating scales. See [Heiz (2021)](https://glaswasser.github.io/simulating-correlated-likert-scale-data/) for an excellent and short example using the following packages:

    - truncnorm
    
    - faux
    
* sampling with a predetermined probability distribution

* Marginal model specification as in [Touloumis (2016)](https://journal.r-project.org/archive/2016/RJ-2016-034/index.html) and  [Gr√∏nneberg et al. (2022)](https://www.jstatsoft.org/article/view/v102i03)  using: 

    - SimCorMultRes
    
    - covsim
    


# Using _LikertMakeR_

## Generating synthetic rating scales

To synthesise a rating scale with **_LikertMakeR_**, the user must input the following parameters:

-   **_n_**: sample size

-   **_mean_**: desired mean

-   **_sd_**: desired standard deviation

-   **_lowerbound_**: desired lower bound

-   **_upperbound_**: desired upper bound

-   **_items_**: number of items making the scale - default = 1

-   **_seed_**: optional seed for reproducibility

**_LikertMakeR_** offers two different functions for synthesising a rating scale: **_lfast()_** and **_lexact()_**

### _lfast()_

-   **_lfast()_** draws a random sample from a scaled *Beta* distribution. It is very fast but gives no guarantee that the mean and standard deviation are exact. Recommended for relatively large sample sizes.

#### _lfast()_ example

##### a five-item, seven-point Likert scale

```{r }
## a five-item, seven-point Likert scale

x <- lfast(
  n = 256,
  mean = 4.0,
  sd = 1.0,
  lowerbound = 1,
  upperbound = 7,
  items = 5
)
```

distribution of generated vector:

```{r}
## distribution of x

hist(x, breaks = c(1:7), col = "sky blue", xlab = NULL, main = paste("mean=", round(mean(x), 2), ", sd=", round(sd(x), 2)))
```

##### an 11-point likelihood-of-purchase scale

```{r}
## an 11-point likelihood-of-purchase scale

x <- lfast(256, 2, 2, 0, 10, seed = 42)
```

```{r}
hist(x, breaks = c(0:10), col = "sky blue", xlab = NULL, main = paste("mean=", round(mean(x), 2), ", sd=", round(sd(x), 2)))
```

#### _lexact()_

-   **_lexact()_** attempts to produce a vector with exact first and second moments. It uses the *Differential Evolution* algorithm in the **_DEoptim_** package to find appropriate values within the desired constraints. **_lexact()_** can take some time to complete the optimisation task, but is excellent for simulating data from already-published reports where only summary statistics are reported. *If feasible*, the function should produce data with moments that are correct to two decimal places

##### _lexact()_ example

###### a five-item, seven-point Likert scale

```{r}

x <- lexact(
  n = 64,
  mean = 5.0,
  sd = 1.0,
  lowerbound = 1,
  upperbound = 7,
  items = 5
)
```

```{r}
hist(x, breaks = c(1:7), col = "sky blue", xlab = NULL, main = paste("mean=", round(mean(x), 2), ", sd=", round(sd(x), 2)))
```

###### an 11-point likelihood of purchase scale

```{r}

x <- lexact(128, 2, 1.8, 0, 10, seed = 42)
```

```{r}
hist(x, breaks = c(0:10), col = "sky blue", xlab = NULL, main = paste("mean=", round(mean(x), 2), ", sd=", round(sd(x), 2)))
```

###### a seven-point negative-to-positive scale with 6 items

```{r}

x <- lexact(
  n = 64,
  mean = 1.2,
  sd = 1.00,
  lowerbound = -3,
  upperbound = 3,
  items = 4
)
```

```{r}
hist(x, breaks = c(-3:3), col = "sky blue", xlab = NULL, main = paste("mean=", round(mean(x), 2), ", sd=", round(sd(x), 2)))
```

## Correlating vectors of synthetic rating scales

**_LikertMakeR_** offers another function, **_lcor()_**, which rearranges the values in the columns of a data set so that they are correlated at a specified level. It does not change the values - it swaps their positions in a column so that univariate statistics do not change, but their correlations with other vectors do.

**_lcor()_** systematically selects pairs of values in a column and swaps their places, and checks to see if this swap improves the correlation matrix. If so, the swap is retained. Otherwise, the values are returned to their original places. This process is iterated across each column. A large dataframe can take some time.

Value pairs are evaluated one pair at a time, so a vectorised process is infeasible at present. I am working on a faster loop solution using compiled code.

To create the desired correlations, the user must define the following data-frames:

-   **_data_**: a starter data set of rating-scales

-   **_target_**: the target correlation matrix

##### _lcor()_ example

Let's generate some data: four 5-point Likert scales, each made with five items.

###### generate uncorrelated synthetic data

```{r }

set.seed(42)
x1 <- lexact(64, 3.5, 1.00, 1, 5, 5)
x2 <- lexact(64, 2.5, 0.75, 1, 5, 5)
x3 <- lexact(64, 3.0, 1.50, 1, 5, 5)
x4 <- lexact(64, 2.0, 1.25, 1, 5, 5)

mydat4 <- cbind(x1, x2, x3, x4) |> data.frame()
```

The first ten observations from this data-frame are:

```{r}

head(mydat4, 10)
```

Mean values:

```{r}

apply(mydat4, 2, mean) |> round(3)
```

Standard deviations:

```{r echo=FALSE}

apply(mydat4, 2, sd) |> round(3)
```

We can see that the data are close to what is expected. The synthetic data have low correlations:

```{r}

cor(mydat4) |> round(2)
```

###### a target correlation matrix

```{r}
## describe a target correlation matrix
tgt4 <- matrix(
  c(
    1.00, 0.55, 0.80, 0.60,
    0.55, 1.00, 0.25, 0.70,
    0.80, 0.25, 1.00, 0.65,
    0.60, 0.70, 0.65, 1.00
  ),
  nrow = 4
)
```

So now we have a data-frame with desired first and second moments, and a target correlation matrix.

###### applying the *lcor()* function.

```{r}

## apply lcor function

new4 <- lcor(mydat4, tgt4)
```

A new data frame with correlations close to our desired correlation matrix:

```{r}

cor(new4) |> round(2)
```

```{r}

plot(new4)
```

And the means and standard deviations have not changed from the original data-frame.

Mean values:

```{r}

apply(new4, 2, mean) |> round(3)
```

Standard deviations:

```{r}

apply(new4, 2, sd) |> round(3)
```

distributions:

```{r}

hist(x1,
  breaks = seq(from = 1, to = 5, by = 0.20),
  main = paste("mean=", round(mean(x1), 2), ", sd=", round(sd(x1), 2))
)

hist(x2,
  breaks = seq(from = 1, to = 5, by = 0.20),
  main = paste("mean=", round(mean(x2), 2), ", sd=", round(sd(x2), 2))
)

hist(x3,
  breaks = seq(from = 1, to = 5, by = 0.20),
  main = paste("mean=", round(mean(x3), 2), ", sd=", round(sd(x3), 2))
)

hist(x4,
  breaks = seq(from = 1, to = 5, by = 0.20),
  main = paste("mean=", round(mean(x4), 2), ", sd=", round(sd(x4), 2))
)
```



